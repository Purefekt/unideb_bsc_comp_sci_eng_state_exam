{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"INTRODUCTION PLEASE READ ALL THE SECTIONS BEFORE STARTING THE QUESTIONS TO CORRECT TYPOS, GRAMMATICAL ERRORS SEE CONTRIBUTING SECTION The state exam is a 20-minute speech that the student must give in front of 5-6 board members. The topic is randomly chosen using a random number generator and comes from a pool of 15 pre-defined topics posted HERE , under the heading 'Final exam topics'. You must log in using your Neptun ID to access this page. This link might change in the future, in which case please let me know by contributing on GitHub or sending me an email. These are the solutions to 'who started after 2017/2018/1'. The question will be selected using a random number generator and you will not be allowed to use any notes or a computer. You can create notes from memory in the 30 minute preparation time given to you. Each question has 2 subtopics, and you have to speak on both subtopics, roughly for 10 minutes each. These are my answers which I curated using multiple resources. My approach was to write around 2100 words per question or 1050 words per subtopic. These answers are written in a speech-like flow, with no diagrams, visuals, or formulas. IMPORTANT: Memorizing all 30k+ words in this document is stupid and will cause you to feel suicidal. These topics are core computer science engineering concepts, which means there's logic to everything. For each topic, watch simple explainer youtube videos with visuals so you can store that image in your head, and then while speaking, you can re-run it in your mind and that will make speaking easy. Use these answers as a guideline.","title":"Introduction"},{"location":"#introduction","text":"PLEASE READ ALL THE SECTIONS BEFORE STARTING THE QUESTIONS TO CORRECT TYPOS, GRAMMATICAL ERRORS SEE CONTRIBUTING SECTION The state exam is a 20-minute speech that the student must give in front of 5-6 board members. The topic is randomly chosen using a random number generator and comes from a pool of 15 pre-defined topics posted HERE , under the heading 'Final exam topics'. You must log in using your Neptun ID to access this page. This link might change in the future, in which case please let me know by contributing on GitHub or sending me an email. These are the solutions to 'who started after 2017/2018/1'. The question will be selected using a random number generator and you will not be allowed to use any notes or a computer. You can create notes from memory in the 30 minute preparation time given to you. Each question has 2 subtopics, and you have to speak on both subtopics, roughly for 10 minutes each. These are my answers which I curated using multiple resources. My approach was to write around 2100 words per question or 1050 words per subtopic. These answers are written in a speech-like flow, with no diagrams, visuals, or formulas. IMPORTANT: Memorizing all 30k+ words in this document is stupid and will cause you to feel suicidal. These topics are core computer science engineering concepts, which means there's logic to everything. For each topic, watch simple explainer youtube videos with visuals so you can store that image in your head, and then while speaking, you can re-run it in your mind and that will make speaking easy. Use these answers as a guideline.","title":"INTRODUCTION"},{"location":"2_about_the_author/","text":"ABOUT THE AUTHOR This website was developed by Veer Singh. I graduated from the University of Debrecen in December 2021 with a Bachelor of Science in Computer Science Engineering. You can find me here: DM me on Instagram @veer_singh99 Email me: veersingh230799@gmail.com Personal Website LinkedIn GitHub","title":"About the Author"},{"location":"2_about_the_author/#about-the-author","text":"This website was developed by Veer Singh. I graduated from the University of Debrecen in December 2021 with a Bachelor of Science in Computer Science Engineering. You can find me here: DM me on Instagram @veer_singh99 Email me: veersingh230799@gmail.com Personal Website LinkedIn GitHub","title":"ABOUT THE AUTHOR"},{"location":"3_importance_of_state_exam/","text":"IMPORTANCE OF THE STATE EXAM This 20-minute speech will count for 33% of your final degree GPA. The other 33% comes from the average of the grade your thesis supervisor gave you and the thesis defense grade (thesis defense happens right before the state exam). And the last 33% comes from the CGPA of all your courses taken in all the 3.5 years (7 semesters), except the thesis. So basically, doing well in these 20 minutes carries the same weightage as working hard for all the semesters, that is why you can boost your final GPA by acing the thesis and state exam.","title":"Importance of the State Exam"},{"location":"3_importance_of_state_exam/#importance-of-the-state-exam","text":"This 20-minute speech will count for 33% of your final degree GPA. The other 33% comes from the average of the grade your thesis supervisor gave you and the thesis defense grade (thesis defense happens right before the state exam). And the last 33% comes from the CGPA of all your courses taken in all the 3.5 years (7 semesters), except the thesis. So basically, doing well in these 20 minutes carries the same weightage as working hard for all the semesters, that is why you can boost your final GPA by acing the thesis and state exam.","title":"IMPORTANCE OF THE STATE EXAM"},{"location":"4_my_state_exam_experience/","text":"MY STATE EXAM EXPERIENCE My time slot was 8:00 am and my assigned number was 1, which meant I was the first on that day. There were 8 other Hungarian students of BSc. Computer Science Engineering and everyone was dressed in complete formals. When I entered the room, there were 5 board members and one assistant sitting on the left. The president of the board was mediating the whole thing. He told me to copy my thesis ppt to the projector laptop and start. I had my thesis ppt on a USB drive, so I copied it, they gave me a pointer with which I could move the slides. I started speaking, but keep in mind to go fast, since I was taking too long because I had a lot of material, the president of the board nudged me to speed up. Once I was done presenting, the president of the board asked the others if they had any questions, everyone said no. Then he asked me a simple question about the dataset I used in my thesis. Once the thesis presentation was done, the president of the board ran a random number generator and I got the number 10. He gave me a copy of the topics of the state exam containing all 15 questions, empty pieces of paper and told me I had 30 minutes to prepare topic number 10. Luckily topic 10 was one of my best-prepared topics so I asked them if I can start right away. I sat down in the chair and started speaking the first subtopic. I spoke fast, almost rapping, and in great detail. Halfway into answering the first subtopic, the president of the board told me to stop and asked if anyone had any questions, everyone said no. Then I started talking about the 2nd subtopic, again speaking fast and in great detail. Again the president told me to stop, he asked if anyone had any questions, everyone said no. This time he asked me a tricky question about the 2nd subtopic, and I answered it best to my knowledge. Once I was done, I left the room and they told me that I will get the results around 1:00 pm. At this point, the next person goes in, and slowly everyone on that day gets their thesis defense and state exam done. On my day there were 9 of us, and around 12:30 pm, everyone was done. Then they told us to wait another 30 minutes. Then they called all of us inside, all the board members were now standing and the 9 of us were standing on the opposite side. The president of the board asked for our consent to say the grades out loud and he started telling the grades of the thesis defense and the state exam in the original order. Since I was the first, he said 5 in thesis defense and 5 in state exam and then moved on to the others. Once everyone's grades were told, everyone shook hands and left. A few hours after that, I got a message on Neptun that my student status has been changed to 'Graduated' and my final grade was available on Neptun -> Studies -> Training Data under the Diploma section. 10 days after this, I got my original Degree Certificate and Diploma Supplement. The degree certificate is a single-page document saying you graduated from this university in this course. The diploma supplement includes more detailed information including the transcript of all courses taken.","title":"My State Exam Experience"},{"location":"4_my_state_exam_experience/#my-state-exam-experience","text":"My time slot was 8:00 am and my assigned number was 1, which meant I was the first on that day. There were 8 other Hungarian students of BSc. Computer Science Engineering and everyone was dressed in complete formals. When I entered the room, there were 5 board members and one assistant sitting on the left. The president of the board was mediating the whole thing. He told me to copy my thesis ppt to the projector laptop and start. I had my thesis ppt on a USB drive, so I copied it, they gave me a pointer with which I could move the slides. I started speaking, but keep in mind to go fast, since I was taking too long because I had a lot of material, the president of the board nudged me to speed up. Once I was done presenting, the president of the board asked the others if they had any questions, everyone said no. Then he asked me a simple question about the dataset I used in my thesis. Once the thesis presentation was done, the president of the board ran a random number generator and I got the number 10. He gave me a copy of the topics of the state exam containing all 15 questions, empty pieces of paper and told me I had 30 minutes to prepare topic number 10. Luckily topic 10 was one of my best-prepared topics so I asked them if I can start right away. I sat down in the chair and started speaking the first subtopic. I spoke fast, almost rapping, and in great detail. Halfway into answering the first subtopic, the president of the board told me to stop and asked if anyone had any questions, everyone said no. Then I started talking about the 2nd subtopic, again speaking fast and in great detail. Again the president told me to stop, he asked if anyone had any questions, everyone said no. This time he asked me a tricky question about the 2nd subtopic, and I answered it best to my knowledge. Once I was done, I left the room and they told me that I will get the results around 1:00 pm. At this point, the next person goes in, and slowly everyone on that day gets their thesis defense and state exam done. On my day there were 9 of us, and around 12:30 pm, everyone was done. Then they told us to wait another 30 minutes. Then they called all of us inside, all the board members were now standing and the 9 of us were standing on the opposite side. The president of the board asked for our consent to say the grades out loud and he started telling the grades of the thesis defense and the state exam in the original order. Since I was the first, he said 5 in thesis defense and 5 in state exam and then moved on to the others. Once everyone's grades were told, everyone shook hands and left. A few hours after that, I got a message on Neptun that my student status has been changed to 'Graduated' and my final grade was available on Neptun -> Studies -> Training Data under the Diploma section. 10 days after this, I got my original Degree Certificate and Diploma Supplement. The degree certificate is a single-page document saying you graduated from this university in this course. The diploma supplement includes more detailed information including the transcript of all courses taken.","title":"MY STATE EXAM EXPERIENCE"},{"location":"5_other_tips/","text":"OTHER TIPS Wear complete formals including a tie. Carry your thesis ppt in a USB drive. Carry a pen to the room.","title":"Other Tips"},{"location":"5_other_tips/#other-tips","text":"Wear complete formals including a tie. Carry your thesis ppt in a USB drive. Carry a pen to the room.","title":"OTHER TIPS"},{"location":"6_contributing/","text":"CONTRIBUTE There might be grammatical errors, typos, or missing information (mostly in the electronics, control system and signals and systems based topics), that is why if you spot any such mistake, please make sure to correct it by contributing in these two ways: Contributing directly on GitHub: You can contribute to this project HERE The information is written in simple markdown files found on the main branch in the project_files/docs directory. You can open the markdown file you wish to edit and make the changes. Contributing to a GitHub project is straightforward, if you need help you can refer to THIS ARTICLE If you contribute this way, then your GitHub username will appear as a contributor which is great for your profile. Sending me an email: This method is not preferred but you can send me an email -> veersingh230799@gmail.com and let me know where the mistake is. PS: The typos or errors in the question itself are fine. It is exactly how it was printed on the pdf so it is not changed. If a student tries to copy-paste the entire question, then this website should show up, but if I corrected the typos in the questions, then this would not work.","title":"Contribute"},{"location":"6_contributing/#contribute","text":"There might be grammatical errors, typos, or missing information (mostly in the electronics, control system and signals and systems based topics), that is why if you spot any such mistake, please make sure to correct it by contributing in these two ways: Contributing directly on GitHub: You can contribute to this project HERE The information is written in simple markdown files found on the main branch in the project_files/docs directory. You can open the markdown file you wish to edit and make the changes. Contributing to a GitHub project is straightforward, if you need help you can refer to THIS ARTICLE If you contribute this way, then your GitHub username will appear as a contributor which is great for your profile. Sending me an email: This method is not preferred but you can send me an email -> veersingh230799@gmail.com and let me know where the mistake is. PS: The typos or errors in the question itself are fine. It is exactly how it was printed on the pdf so it is not changed. If a student tries to copy-paste the entire question, then this website should show up, but if I corrected the typos in the questions, then this would not work.","title":"CONTRIBUTE"},{"location":"7_show_your_appreciation/","text":"SHOW YOUR APPRECIATION If you found this helpful, you can: Give the GitHub repository a star. Share with other CSE students or even CS students since they might have some similar questions.","title":"Show Your Appreciation"},{"location":"7_show_your_appreciation/#show-your-appreciation","text":"If you found this helpful, you can: Give the GitHub repository a star. Share with other CSE students or even CS students since they might have some similar questions.","title":"SHOW YOUR APPRECIATION"},{"location":"Question_1/","text":"1. PART 1 The processor implementation options: Processor technology, implementation techniques and design technologies. Typically peripherals for embedded systems. Communication protocols. PART 2 Program units. Subprograms. Parameter evaluation. Parameter passing methods. Block. Scoping, accessibility. Abstract data type. Generic programming. I/O tools of programming languages, file handling. Exception handling. Parallel programming. Part 1 Processor \u2013 A processor or a central processing unit is the electronic circuit that executes instructions in a computer program. The processor performs basic arithmetic, logic control, and input/output operations specified by the instructions in a program. Principal components of a CPU include the arithmetic logic unit (ALU) that performs arithmetic and logic operations, processor registers that supply operands to the ALU and store the results of ALU operations, and a control unit that performs the fetch, decode, and execution cycle. All processors execute a sequence of stored instructions which is called a program. These instructions are kept in the computer memory. The instructions cycle typically comprises fetch, decode, and execute. Fetch involves retrieving the instruction from program memory, which is determined by the program counter. Next comes decode, here the instruction is converted into a signal. Finally, the instruction is executed as a single action or sequence of actions. The results are usually written to the very fast registers on the CPU but sometimes can be written to the slower but much bigger computer memory. Modern CPUs also have cache, which is super-fast memory stored on the CPU, this is much smaller than the main memory, but can be used extremely quickly and thus reduces the time and energy to access data from the main memory. Cache has a hierarchy of where the data is organized called L1, L2, L3, etc. in decreasing speeds and increasing sizes. Most CPUs are synchronous circuits which means they use a clock signal to pace sequential operations. This clock signal is produced by an external oscillator circuit that generates a consistent number of pulses each second in the form of a periodic square wave. The frequency of the clock pulses determines the rate at which a CPU executes instructions, the faster the clock, the more instructions the CPU will execute each second. Microprocessor \u2013 This is a computer processor where the data processing logic and control are included on a single integrated circuit. The microprocessor uses Very-Large-scale Integration to integrate the entire CPU onto a single IC, this greatly reduces the cost of processing power. The first commercially available microprocessor was the intel 4004. Microprocessors are very popular in almost all desktops and some laptops with modern processors like Intel\u2019s alder lake and AMD\u2019s ryzen 5000 using the x86-64 instruction set architecture. Microcontroller \u2013 These are microprocessors along with memory and programmable input/output peripherals. Microcontrollers are designed for embedded applications like single-board computers like the raspberry pi. There are more sophisticated chips called System on a chip or SoC which have a microcontroller as a part but also have graphics processing unit and Wi-Fi modules. These SoCs are often seen in smartphones like the Apple A14 chip or the Qualcomm snapdragon 888 seen in most high-end iPhones and android phones. Implementation \u2013 An instruction set architecture is an abstract model of a computer. A realization of an ISA is called an implementation. Different ISAs vary in performance, efficiency, size, etc. ISA serves as the interface between software and hardware. Software that has been written for an ISA can run on different implementations of the same ISA. This has helped lower the costs of computers and increase their applicability. There are 3 popular ISAs today, x86-64 which was developed by Intel and then AMD. These are complex instruction set computer CISC architecture, which means a single instruction can execute several low-level operations. These are most prevalent on desktops, servers, and laptops. The other very popular ISA is ARM which is developed by Arm Ltd. These are reduced instruction set computer RISC architectures where a given task requires more instructions as compared to CISC, but the individual instructions are simpler and executed faster. ARM chips are also substantially more power-efficient and thus are the primary chips used in smartphones and single-board computers like the raspberry pi. They are also slowly being used for laptops due to their battery performance. The third most popular architecture is RISC-V which in contrast to x86-64 and ARM is open source. This means anyone can use this ISA without any royalties or licensing fees. This is also a RISC-based architecture. Embedded systems are computer systems comprising of a processor, memory, and input/output peripheral devices that have a dedicated function within a larger system. Embedded systems range from simple microcontroller boards like Arduino to more complex systems which use multiple peripherals and networking equipment to communicate with other systems. Typical peripherals for embedded systems are: Serial Communication Interfaces (SCI) \u2013 These are relatively slow, asynchronous communication ports that are used to communicate with other embedded systems and devices. Inter-Integrated Circuit(I2C) \u2013 These are widely used for attaching lower-speed peripheral integrated circuits to processors and microcontrollers in short-distance communication. Universal Serial Bus (USB) \u2013 USB is an industry-standard that allows for connection, communication, and power supply. Embedded systems can use it to power, transfer data, or connect to input devices like keyboards and mice. Multimedia Cards (SD cards) \u2013 These are used to store the data, and, in most cases, they host the operating system for the embedded system. Network (Ethernet) \u2013 Ethernet ports are a family of wired computer networking technologies used to connect multiple devices physically. General Purpose Input/Output (GPIO) \u2013 GPIO is an uncommitted digital signal pin or multiple pins on embedded systems that can be used as an input or output or both and are controlled by the user at runtime. In the case of raspberry pi, we have multiple HATs (hardware attached on top) which connect to the GPIO pins. These can be temperature detection HAT, LED HATs, etc. Communication protocols in embedded systems can be of two kinds \u2013 Inter system protocol and Intra system protocol. Inter System Protocol - This is the communication between two communication devices, for example, a PC and an embedded system. Here communication is achieved through inter bus system. \u25cf USB \u2013 Universal Serial Bus, this is a two-wired serial communication protocol. USB sends and receives the data serially between the host and an external peripheral device. Data is sent as packets. \u25cf UART \u2013 Universal Asynchronous Receiver/Transmitter is a physical piece of hardware that converts parallel data into serial data. Its main purpose is to transmit and receive data serially. \u25cf USART \u2013 Universal Synchronous Asynchronous Receiver/Transmitter is identical to UART with added synchronous functionality. Intra System Protocol \u2013 This establishes communication between components within the circuit board. \u25cf I2C \u2013 Inter-Integrated Circuit is a serial communication protocol. It allows connecting peripheral chips with a microcontroller. \u25cf SPI - Serial Peripheral Interface is a serial communication protocol. It is used for short-distance communication in embedded systems. \u25cf CAN \u2013 Controller Area Network is a serial communication protocol and is based on a message-oriented communication protocol. It is primarily used for communication in embedded systems in vehicles. Part 2 The program unit is a sequence of one or more lines, organized as statements, comments, and includes directives. A program unit can be the main program, a module, a block data program unit, an external function subprogram, or an external subroutine subprogram. A subprogram is a sequence of instructions whose execution is invoked from one or more remote locations in a program, with the expectation that when the subprogram execution is complete, execution resumes at the instruction after the one that invoked the subprogram in high-level languages, subprograms are also called subroutines, procedures, and functions. In object-oriented languages, they are usually called methods or constructors. In most modern high-level languages, subprograms can have parameters, local variables, and returned values. A procedure just executes a set of instructions while a function will return a value when it has finished executing. Writing subprograms makes the code more readable and reusable as the code is broken into smaller sections. Parameter evaluation is the process of mapping formal and actual parameters when a subprogram is called. Formal parameters are defined in the specification of the subprogram, they are declared only once. Actual parameters (arguments) are specified in the calls themselves. Thus, the formal parameter list is determinative to parameter evaluation. There are three issues related to parameter evaluation: Assigning actual parameter to formal parameter \u2013 The actual parameters are assigned to formal parameters based on their relative order in the parameter list, the first argument is assigned to the first formal parameter, second to second, and so on. We can also supply the name of the formal parameter and assign it to the actual parameter, here the order is irrelevant. Number of actual parameters passed to the subprogram call \u2013 The number of formal parameters is fixed; in this case, the number of actual parameters must equal the number of formal parameters, or the number of actual parameters can be less than the number of formal parameters in which case the formal parameters did not assign an actual parameter during the subprogram call will be assigned default values which are pre-defined by the programmer. Relationship between types of formal and actual parameters \u2013 In some programming languages the type of the actual parameter must be exactly as the type of the formal parameter. In other languages, the actual parameter is converted to the type of the corresponding formal parameter. Parameter passing is the action of passing the actual parameters to the formal parameters when a function or subroutine is called. There are various methods to pass parameters: Pass by value \u2013 Here the value of the actual parameters is copied to formal parameters. These two different parameters store the values in separate memory locations. Here we are passing the value and not the variable. Pass by reference \u2013 Here both actual and formal parameters refer to the same memory location. Thus, the changes made to the formal parameters in the subroutine call will cause changes to the actual parameter. Here we are not passing the value but the memory address. Block or code block is a structure of source code that is grouped. Blocks contain one or more declarations and statements. They allow us to group statements so they can be treated as one statement and to define scopes for variables to distinguish them from the same name used elsewhere. The scope is where an item like variable, constant, function, etc that has an identifier name is recognized and be used. These examples use a variable as the item. Global scope is when a variable is defined outside a function, when the program is compiled, this variable is assigned memory space at initialization. This variable can be used for all functions in the source code and even other modules which are linked to the code. Global variables are always defined at the very top before any function definitions, so they are available to all functions. Local scope occurs when a variable is defined inside a function or a code block like an if statement. When the code is compiled, these variables are assigned to the stack in the computer memory, this exists until the function is complete. These variables are assigned at the top inside of the function or code block, making them available to everything inside the function. If a local variable has the same name identifier as a global variable, then that variable will get the local variable value inside the function call. Abstract Data Types or ADT are objects whose behavior is defined by a set of values and a set of operations. ADT only mentions what operations are to be performed but not how these operations will be implemented. It does not specify how data will be organized in memory and what algorithms will be used for implementing the operations. It is called abstract because it gives an implementation-independent view. There are many ADTs like list, stack, and queue, all of these have different implementations in different programming languages. Generic programming is a style of programming where algorithms and functions are written in terms of types so that they work on all data types and not just one. We can have a placeholder data type which is a general data type, this can be replaced with different data types and work with all of them. Computer stores data in files like plain text, image data, etc. Files can be accessed, updated, and created using programming languages. When data is written into a file, that is called input and when data is read from a file, that is called output. Files can be handled in separate ways, read-only mode allows for only getting data from the file. Write only mode allows the programming language to only write data to the file and not read it. Read and write mode allows for both getting data from the file and writing data to the file. Append mode allows for writing the file without overwriting the existing data on the file. All programming languages first initialize an object of the type of file, which contains all the information to control the stream, we also must specify the mode such as read-only, write-only, etc. Once functions associated with the file are done, the file must be closed. Once this is done, the memory allocated for handling the file is freed up. In python a file is opened using the open() function, it can be read using read(), written to using write(), and closed using close() functions of the file object. Exception handling is the process of responding to the occurrence of exceptions or errors during the execution of a program. An exception breaks the normal flow of execution and executes a pre-registered exception handler. Exception handling attempts to handle these situations so that a program does not crash. In python, there are some built-in exception handlers, we can also define custom exception handlers using the \u2018try \u2013 except\u2019 blocks. The program first attempts the try block, if it throws an exception, then we execute the except block, and the program flow is maintained instead of crashing. Parallel programming is when we use multiple processors or threads to execute parts of a program at the same time. The program is broken down into smaller steps, these are then executed simultaneously on multiple threads. The output of these steps mustn't influence each other. This is very advantageous since all modern processors are multi-core with SMT or Simultaneous multithreading enabled which means a single core can perform 2 virtual threads at the same time. This type of programming is hard to learn and to a large extent must be enforced by the programmer during programming.","title":"Question 1"},{"location":"Question_1/#1-part-1-the-processor-implementation-options-processor-technology-implementation-techniques-and-design-technologies-typically-peripherals-for-embedded-systems-communication-protocols-part-2-program-units-subprograms-parameter-evaluation-parameter-passing-methods-block-scoping-accessibility-abstract-data-type-generic-programming-io-tools-of-programming-languages-file-handling-exception-handling-parallel-programming","text":"","title":"1. PART 1 The processor implementation options: Processor technology, implementation techniques and design technologies. Typically peripherals for embedded systems. Communication protocols. PART 2 Program units. Subprograms. Parameter evaluation. Parameter passing methods. Block. Scoping, accessibility. Abstract data type. Generic programming. I/O tools of programming languages, file handling. Exception handling. Parallel programming."},{"location":"Question_1/#part-1","text":"Processor \u2013 A processor or a central processing unit is the electronic circuit that executes instructions in a computer program. The processor performs basic arithmetic, logic control, and input/output operations specified by the instructions in a program. Principal components of a CPU include the arithmetic logic unit (ALU) that performs arithmetic and logic operations, processor registers that supply operands to the ALU and store the results of ALU operations, and a control unit that performs the fetch, decode, and execution cycle. All processors execute a sequence of stored instructions which is called a program. These instructions are kept in the computer memory. The instructions cycle typically comprises fetch, decode, and execute. Fetch involves retrieving the instruction from program memory, which is determined by the program counter. Next comes decode, here the instruction is converted into a signal. Finally, the instruction is executed as a single action or sequence of actions. The results are usually written to the very fast registers on the CPU but sometimes can be written to the slower but much bigger computer memory. Modern CPUs also have cache, which is super-fast memory stored on the CPU, this is much smaller than the main memory, but can be used extremely quickly and thus reduces the time and energy to access data from the main memory. Cache has a hierarchy of where the data is organized called L1, L2, L3, etc. in decreasing speeds and increasing sizes. Most CPUs are synchronous circuits which means they use a clock signal to pace sequential operations. This clock signal is produced by an external oscillator circuit that generates a consistent number of pulses each second in the form of a periodic square wave. The frequency of the clock pulses determines the rate at which a CPU executes instructions, the faster the clock, the more instructions the CPU will execute each second. Microprocessor \u2013 This is a computer processor where the data processing logic and control are included on a single integrated circuit. The microprocessor uses Very-Large-scale Integration to integrate the entire CPU onto a single IC, this greatly reduces the cost of processing power. The first commercially available microprocessor was the intel 4004. Microprocessors are very popular in almost all desktops and some laptops with modern processors like Intel\u2019s alder lake and AMD\u2019s ryzen 5000 using the x86-64 instruction set architecture. Microcontroller \u2013 These are microprocessors along with memory and programmable input/output peripherals. Microcontrollers are designed for embedded applications like single-board computers like the raspberry pi. There are more sophisticated chips called System on a chip or SoC which have a microcontroller as a part but also have graphics processing unit and Wi-Fi modules. These SoCs are often seen in smartphones like the Apple A14 chip or the Qualcomm snapdragon 888 seen in most high-end iPhones and android phones. Implementation \u2013 An instruction set architecture is an abstract model of a computer. A realization of an ISA is called an implementation. Different ISAs vary in performance, efficiency, size, etc. ISA serves as the interface between software and hardware. Software that has been written for an ISA can run on different implementations of the same ISA. This has helped lower the costs of computers and increase their applicability. There are 3 popular ISAs today, x86-64 which was developed by Intel and then AMD. These are complex instruction set computer CISC architecture, which means a single instruction can execute several low-level operations. These are most prevalent on desktops, servers, and laptops. The other very popular ISA is ARM which is developed by Arm Ltd. These are reduced instruction set computer RISC architectures where a given task requires more instructions as compared to CISC, but the individual instructions are simpler and executed faster. ARM chips are also substantially more power-efficient and thus are the primary chips used in smartphones and single-board computers like the raspberry pi. They are also slowly being used for laptops due to their battery performance. The third most popular architecture is RISC-V which in contrast to x86-64 and ARM is open source. This means anyone can use this ISA without any royalties or licensing fees. This is also a RISC-based architecture. Embedded systems are computer systems comprising of a processor, memory, and input/output peripheral devices that have a dedicated function within a larger system. Embedded systems range from simple microcontroller boards like Arduino to more complex systems which use multiple peripherals and networking equipment to communicate with other systems. Typical peripherals for embedded systems are: Serial Communication Interfaces (SCI) \u2013 These are relatively slow, asynchronous communication ports that are used to communicate with other embedded systems and devices. Inter-Integrated Circuit(I2C) \u2013 These are widely used for attaching lower-speed peripheral integrated circuits to processors and microcontrollers in short-distance communication. Universal Serial Bus (USB) \u2013 USB is an industry-standard that allows for connection, communication, and power supply. Embedded systems can use it to power, transfer data, or connect to input devices like keyboards and mice. Multimedia Cards (SD cards) \u2013 These are used to store the data, and, in most cases, they host the operating system for the embedded system. Network (Ethernet) \u2013 Ethernet ports are a family of wired computer networking technologies used to connect multiple devices physically. General Purpose Input/Output (GPIO) \u2013 GPIO is an uncommitted digital signal pin or multiple pins on embedded systems that can be used as an input or output or both and are controlled by the user at runtime. In the case of raspberry pi, we have multiple HATs (hardware attached on top) which connect to the GPIO pins. These can be temperature detection HAT, LED HATs, etc. Communication protocols in embedded systems can be of two kinds \u2013 Inter system protocol and Intra system protocol. Inter System Protocol - This is the communication between two communication devices, for example, a PC and an embedded system. Here communication is achieved through inter bus system. \u25cf USB \u2013 Universal Serial Bus, this is a two-wired serial communication protocol. USB sends and receives the data serially between the host and an external peripheral device. Data is sent as packets. \u25cf UART \u2013 Universal Asynchronous Receiver/Transmitter is a physical piece of hardware that converts parallel data into serial data. Its main purpose is to transmit and receive data serially. \u25cf USART \u2013 Universal Synchronous Asynchronous Receiver/Transmitter is identical to UART with added synchronous functionality. Intra System Protocol \u2013 This establishes communication between components within the circuit board. \u25cf I2C \u2013 Inter-Integrated Circuit is a serial communication protocol. It allows connecting peripheral chips with a microcontroller. \u25cf SPI - Serial Peripheral Interface is a serial communication protocol. It is used for short-distance communication in embedded systems. \u25cf CAN \u2013 Controller Area Network is a serial communication protocol and is based on a message-oriented communication protocol. It is primarily used for communication in embedded systems in vehicles.","title":"Part 1"},{"location":"Question_1/#part-2","text":"The program unit is a sequence of one or more lines, organized as statements, comments, and includes directives. A program unit can be the main program, a module, a block data program unit, an external function subprogram, or an external subroutine subprogram. A subprogram is a sequence of instructions whose execution is invoked from one or more remote locations in a program, with the expectation that when the subprogram execution is complete, execution resumes at the instruction after the one that invoked the subprogram in high-level languages, subprograms are also called subroutines, procedures, and functions. In object-oriented languages, they are usually called methods or constructors. In most modern high-level languages, subprograms can have parameters, local variables, and returned values. A procedure just executes a set of instructions while a function will return a value when it has finished executing. Writing subprograms makes the code more readable and reusable as the code is broken into smaller sections. Parameter evaluation is the process of mapping formal and actual parameters when a subprogram is called. Formal parameters are defined in the specification of the subprogram, they are declared only once. Actual parameters (arguments) are specified in the calls themselves. Thus, the formal parameter list is determinative to parameter evaluation. There are three issues related to parameter evaluation: Assigning actual parameter to formal parameter \u2013 The actual parameters are assigned to formal parameters based on their relative order in the parameter list, the first argument is assigned to the first formal parameter, second to second, and so on. We can also supply the name of the formal parameter and assign it to the actual parameter, here the order is irrelevant. Number of actual parameters passed to the subprogram call \u2013 The number of formal parameters is fixed; in this case, the number of actual parameters must equal the number of formal parameters, or the number of actual parameters can be less than the number of formal parameters in which case the formal parameters did not assign an actual parameter during the subprogram call will be assigned default values which are pre-defined by the programmer. Relationship between types of formal and actual parameters \u2013 In some programming languages the type of the actual parameter must be exactly as the type of the formal parameter. In other languages, the actual parameter is converted to the type of the corresponding formal parameter. Parameter passing is the action of passing the actual parameters to the formal parameters when a function or subroutine is called. There are various methods to pass parameters: Pass by value \u2013 Here the value of the actual parameters is copied to formal parameters. These two different parameters store the values in separate memory locations. Here we are passing the value and not the variable. Pass by reference \u2013 Here both actual and formal parameters refer to the same memory location. Thus, the changes made to the formal parameters in the subroutine call will cause changes to the actual parameter. Here we are not passing the value but the memory address. Block or code block is a structure of source code that is grouped. Blocks contain one or more declarations and statements. They allow us to group statements so they can be treated as one statement and to define scopes for variables to distinguish them from the same name used elsewhere. The scope is where an item like variable, constant, function, etc that has an identifier name is recognized and be used. These examples use a variable as the item. Global scope is when a variable is defined outside a function, when the program is compiled, this variable is assigned memory space at initialization. This variable can be used for all functions in the source code and even other modules which are linked to the code. Global variables are always defined at the very top before any function definitions, so they are available to all functions. Local scope occurs when a variable is defined inside a function or a code block like an if statement. When the code is compiled, these variables are assigned to the stack in the computer memory, this exists until the function is complete. These variables are assigned at the top inside of the function or code block, making them available to everything inside the function. If a local variable has the same name identifier as a global variable, then that variable will get the local variable value inside the function call. Abstract Data Types or ADT are objects whose behavior is defined by a set of values and a set of operations. ADT only mentions what operations are to be performed but not how these operations will be implemented. It does not specify how data will be organized in memory and what algorithms will be used for implementing the operations. It is called abstract because it gives an implementation-independent view. There are many ADTs like list, stack, and queue, all of these have different implementations in different programming languages. Generic programming is a style of programming where algorithms and functions are written in terms of types so that they work on all data types and not just one. We can have a placeholder data type which is a general data type, this can be replaced with different data types and work with all of them. Computer stores data in files like plain text, image data, etc. Files can be accessed, updated, and created using programming languages. When data is written into a file, that is called input and when data is read from a file, that is called output. Files can be handled in separate ways, read-only mode allows for only getting data from the file. Write only mode allows the programming language to only write data to the file and not read it. Read and write mode allows for both getting data from the file and writing data to the file. Append mode allows for writing the file without overwriting the existing data on the file. All programming languages first initialize an object of the type of file, which contains all the information to control the stream, we also must specify the mode such as read-only, write-only, etc. Once functions associated with the file are done, the file must be closed. Once this is done, the memory allocated for handling the file is freed up. In python a file is opened using the open() function, it can be read using read(), written to using write(), and closed using close() functions of the file object. Exception handling is the process of responding to the occurrence of exceptions or errors during the execution of a program. An exception breaks the normal flow of execution and executes a pre-registered exception handler. Exception handling attempts to handle these situations so that a program does not crash. In python, there are some built-in exception handlers, we can also define custom exception handlers using the \u2018try \u2013 except\u2019 blocks. The program first attempts the try block, if it throws an exception, then we execute the except block, and the program flow is maintained instead of crashing. Parallel programming is when we use multiple processors or threads to execute parts of a program at the same time. The program is broken down into smaller steps, these are then executed simultaneously on multiple threads. The output of these steps mustn't influence each other. This is very advantageous since all modern processors are multi-core with SMT or Simultaneous multithreading enabled which means a single core can perform 2 virtual threads at the same time. This type of programming is hard to learn and to a large extent must be enforced by the programmer during programming.","title":"Part 2"},{"location":"Question_10/","text":"10. PART 1 Configuration of a web server using SSL, the OpenSSL cryptographic library: authentication, encryption. PART 2 The instruction set architecture (ISA) of Intel X86 processors (registers, addressing, instructions, memory architecture, interrupt system) Part 1 SSL is a secure sockets layer which is the standard technology for keeping an internet connection secure and safeguarding any sensitive data that is being sent between two systems, preventing criminals from reading and modifying any information transferred, including potential personal details. A handshake protocol is used between the client and server to negotiate the secret key to be used for encrypting the messages. The handshake protocol follows these steps, step 1 is client hello, here the client will send a message to the server about its SSL version, cryptographic algorithms, and data compressing methods. Step 2 is server hello, server will send the cryptographic algorithms it chose from the client\u2019s list, a session ID, its digital certificate, and its public key. In step 3, the client will verify the server\u2019s digital certificate using the certificate authority to confirm the authenticity of the server. In step 4 the client has authenticated the server and it will send a shared secret key in an encrypted format which has been encrypted using the server\u2019s public key, this key can be decrypted by the server since it has the corresponding private key, this shared secret key is stored and will be used for encryption and decryption of the data. In step 5, the client will send a finish message encrypted with the shared secret key. SSL certificate is a digital certificate that can be used for authentication of a website, we can use this for SSL. When a certificate is installed, it makes the website from HTTP to HTTPS. The SSL certificate has two keys, one is public and the other one is private. Data is encrypted with the public key and can be decrypted with a private key only. The web server with a private key can understand the data. Google made the HTTPS everywhere campaign which gave a search engine optimization boost to websites with a digital certificate and later marked all websites not using HTTPS to be insecure, this has pushed everyone to have a digital certificate. OpenSSL is the open-source implementation of SSL. OpenSSL is a cryptographic software library or toolkit. The OpenSSL program is a command-line tool for using various cryptography functions of OpenSSL\u2019s library. OpenSSL can be used to apply for a digital certificate and install the SSL files on the server. A certificate signing request or a CSR code is generated using OpenSSL. CSR is a block of encoded text with data about the website and the company. The owner must submit this CSR to the certificate authority for approval. The certificate request requires a private key from which the public key is created. To generate the private key, we need to specify a key algorithm, the key size, and an optional passphrase. The standard key algorithm is RSA, and the size is usually 2048 bits since this makes it very secure, a higher value can be used but it comes with a performance penalty. We use the OpenSSL command followed by genrsa, the -out argument and the name of the key, and the key size value. This creates the key with the chosen name in the current directory, this key is in the PEM format. The next step is to extract the public key from the private key, this can be done using the OpenSSL RSA command and using the private key as the -in argument and the public key name as the -out argument along with the -pubout argument. Since we have the private key and public key, we can create the CSR using the OpenSSL req command with the -new and -key arguments specifying the name of the private key for the -key argument and a given name for the CSE using the -out argument. OpenSSL will ask a few questions like country name, organization name, email address, etc. This CSR can be sent to the certification authority and then added to the website for a Digital certificate. SSL authentication is intended for the client. The client or the browser verifies the identity of the server, if it finds the server and its certificate are legitimate, then it goes ahead and established a connection. Encryption is a way to encode a message so that its contents are protected from unauthorized access. Secret-key or symmetric encryption is when a variable in cryptography is used what an algorithm to encrypt or decrypt code. Secret keys are only shared with the key\u2019s generator. Public-key or asymmetric encryption is a cryptographic system that uses pairs of keys, the public key is known to others who can use it to encrypt the data, this data can only be decrypted by using the private key which is only available to the owner. Part 2 Instruction set architecture or ISA is an abstract model of a computer. A device executes instructions described by that ISA; a central processing unit is called an implementation. In general, an ISA defines the supported instructions, data types, registers, the hardware support for managing main memory, fundamental features such as memory consistency addressing modes and virtual memory, and the input/output model of the ISA. Intel created the x86 ISA which was based on the intel 8086 microprocessor. This is a proprietary ISA and it is not licensed to others like ARM\u2019s ISA. x86 ISA started as 16-bit and later a 32-bit version was developed. AMD is the only other company that has access to this ISA, and AMD created the 64-bit version which is now called x86-64 or amd64. Even though both Intel and AMD use the same ISA to make their processors, they both have different products since the microarchitecture they use is different. When we talk about x86 ISA, we are referring to the 32-bit model. x86 registers are the main tools to write programs in assembly. The registers are like variables built in the processor. Registers are much faster than accessing the memory and it makes the process fast and cleaner, but there is only a small number of registers thus for many elaborate programs, we must keep storing register values to memory and take values from the memory to store in the registers. We have 4 general 32-bit registers, EAX, EBX, ECX, and EDX. We have 6 segment registers, CS, DS, ES, FS, GS, and SS. We have5 index and pointer registers ESI, EDI, EBP, EIP and ESP, and an EFLAGS register. The general registers can be broken down into 16-bit registers which are AX, BX, CX, and DX, and can be further broken down into 8 bits, AH, AL, BH, BL, CH, CL, DH, and DL. The H and L suffix on the 8-bit registers stand for high byte and low byte. These 4 general-purpose registers have some main use, A series registers are called the accumulator register and they are mainly used for I/O port access, arithmetic, and interrupt calls. The B series registers are called the base register, it is used as a base pointer for memory access and can get some interrupt return values. The C series registers are called the counter register, they are used for loop counters, and shifts, they too get some interrupt values. The D series registers are called data registers and they are used for I/O port access, arithmetic, and some interrupt calls. Segment registers hold the segment address of various items. The indexes and pointer registers have the following uses, EDI is used as destination index register ESI is used as a source index register, EBP is used as stack base pointer, ESP is used as a stack pointer, and EIP is used as an index pointer. The EFLAGS register is a 32-bit register that holds the state of the processor, and it is used for comparing parameters, conditional loops, and conditional jumps. There are various flag bits like carry flag, parity flag, zero flag, sign flag, etc. x86 systems can address up to 232 bytes of memory, which means the memory addresses are 32-bits wide. In an assembly instruction where we require two operands, the first operand is generally the destination, which contains the data in a register or memory location and the second operand is the source. Source contains either the data to be delivered or the address of the data, generally, the source data remains unaltered after the operation. There are three basic modes of addressing: Register addressing \u2013 In this addressing mode, a register contains the operand. Depending upon the instruction, the register may be the first operand, the second, or both. Since this type of processing doesn\u2019t involve memory, it is very fast. Immediate addressing \u2013 An immediate operand has a constant value or an expression. When an instruction with two operands uses immediate addressing, the first operand may be a register or memory location, and the second operand is an immediate constant. The first operand defines the length of the data. Direct memory addressing \u2013 When operands are specified in memory addressing mode, direct access to main memory is required. This way of addressing results in slower processing of data. To locate the exact location of data in memory, we need the segment start address, which is typically found in the DS register, and an offset value, this offset value is also called effective address. In direct addressing mode, the offset value is specified directly as part of the instruction, indicated by a variable name. In direct memory addressing, one of the operands refers to a memory location, and the other operand references a register. Some basic instructions in x86 ISA are: Mov \u2013 The MOV instruction copies the data item referred to by its second operand into the locations referred to by its first operand. Push \u2013 The push instruction places its operand on top of the hardware-supported stack in memory. First push decrements ESP by 4, then places its operand into the contents of the 32-bit location at the address. The stack-pointed ESP is decremented by push since the x86 stack grows down, from high addresses to lower addresses. Pop \u2013 The pop instruction removes the 4-byte data elements from the top of the hardware supported stack into the specified operand. Add \u2013 This instruction adds its two operands, storing the result in the first operand. Sub \u2013 This instruction subtracts its two operands, storing the result in the first operand. Inc, dec \u2013 The inc instruction increments the contents of its operand by one. The dec instruction decrements the contents of its operand by one. Imul \u2013 This instruction will multiply its two operands and store the result in the first operand. There is a three-operand version where it will multiply the second and third operand and store the result in the first operand. Idiv \u2013 This instruction divides the contents of the 64-bit register which is a combination of EDX:EAX registers in x86 where EDX is the most significant 32 bits and EAX is the least significant 32 bits. The quotient result is stored in EAX while the remained is placed in EDX. And, or, xor \u2013 These instructions perform the specified logical operation on their operands, storing the result in the first operand. Not \u2013 This is the logical not and flips all bit values in the operand. Shl, shr \u2013 These instructions shift the bits in the contents of the first operand left or right. Jmp \u2013 This instruction transfers program control flow to the instruction at the memory location instructed by the operand. Jcondition \u2013 This can be je, jle, jg, or other variations and these are conditional jumps based on the status of the condition. Cmp \u2013 This compares the value of the two operands. This is basically the sub instruction, except the result of the subtraction is discarded. Call, ret \u2013 These instructions implement a subroutine call and return. x86 ISA is a 32-bit model, which means the memory addresses are of the width 32. The memory is called a stack, the stack has a width of 32 bits and it grows downwards. When we add something to the top of the stack, the ESP, or stack pointer gets decremented. When we start a program, the ESP stack pointer is at the top of the stack, when we issue a push command which will push data to the top of the memory stack, this will cause the ESP to be decremented. An interrupt is an alert to the processor and serves as a request for the processor to interrupt the currently executing code. There are three types of interrupts: Hardware interrupts \u2013 These are triggered by hardware devices like a special key being pressed on the keyboard. Hardware interrupts are typically asynchronous, their occurrence is unrelated to the instructions being executed at the time they are raised. Software interrupts \u2013 There are a series of software interrupts that are used to transfer control to a function in the operating system kernel. Software interrupts are triggered by the instruction INT. For example, INT 14H triggers interrupt 0x14, the processor stops the current program and jumps to the code to handle interrupt 14. Exceptions \u2013 Exceptions are caused by exceptional conditions in the code which is executing. For example, an attempt to divide by zero or accessing a protected memory area. The processor will detect this problem and transfer control to a handler to handle this exception.","title":"Question 10"},{"location":"Question_10/#10-part-1-configuration-of-a-web-server-using-ssl-the-openssl-cryptographic-library-authentication-encryption-part-2-the-instruction-set-architecture-isa-of-intel-x86-processors-registers-addressing-instructions-memory-architecture-interrupt-system","text":"","title":"10. PART 1 Configuration of a web server using SSL, the OpenSSL cryptographic library: authentication, encryption. PART 2 The instruction set architecture (ISA) of Intel X86 processors (registers, addressing, instructions, memory architecture, interrupt system)"},{"location":"Question_10/#part-1","text":"SSL is a secure sockets layer which is the standard technology for keeping an internet connection secure and safeguarding any sensitive data that is being sent between two systems, preventing criminals from reading and modifying any information transferred, including potential personal details. A handshake protocol is used between the client and server to negotiate the secret key to be used for encrypting the messages. The handshake protocol follows these steps, step 1 is client hello, here the client will send a message to the server about its SSL version, cryptographic algorithms, and data compressing methods. Step 2 is server hello, server will send the cryptographic algorithms it chose from the client\u2019s list, a session ID, its digital certificate, and its public key. In step 3, the client will verify the server\u2019s digital certificate using the certificate authority to confirm the authenticity of the server. In step 4 the client has authenticated the server and it will send a shared secret key in an encrypted format which has been encrypted using the server\u2019s public key, this key can be decrypted by the server since it has the corresponding private key, this shared secret key is stored and will be used for encryption and decryption of the data. In step 5, the client will send a finish message encrypted with the shared secret key. SSL certificate is a digital certificate that can be used for authentication of a website, we can use this for SSL. When a certificate is installed, it makes the website from HTTP to HTTPS. The SSL certificate has two keys, one is public and the other one is private. Data is encrypted with the public key and can be decrypted with a private key only. The web server with a private key can understand the data. Google made the HTTPS everywhere campaign which gave a search engine optimization boost to websites with a digital certificate and later marked all websites not using HTTPS to be insecure, this has pushed everyone to have a digital certificate. OpenSSL is the open-source implementation of SSL. OpenSSL is a cryptographic software library or toolkit. The OpenSSL program is a command-line tool for using various cryptography functions of OpenSSL\u2019s library. OpenSSL can be used to apply for a digital certificate and install the SSL files on the server. A certificate signing request or a CSR code is generated using OpenSSL. CSR is a block of encoded text with data about the website and the company. The owner must submit this CSR to the certificate authority for approval. The certificate request requires a private key from which the public key is created. To generate the private key, we need to specify a key algorithm, the key size, and an optional passphrase. The standard key algorithm is RSA, and the size is usually 2048 bits since this makes it very secure, a higher value can be used but it comes with a performance penalty. We use the OpenSSL command followed by genrsa, the -out argument and the name of the key, and the key size value. This creates the key with the chosen name in the current directory, this key is in the PEM format. The next step is to extract the public key from the private key, this can be done using the OpenSSL RSA command and using the private key as the -in argument and the public key name as the -out argument along with the -pubout argument. Since we have the private key and public key, we can create the CSR using the OpenSSL req command with the -new and -key arguments specifying the name of the private key for the -key argument and a given name for the CSE using the -out argument. OpenSSL will ask a few questions like country name, organization name, email address, etc. This CSR can be sent to the certification authority and then added to the website for a Digital certificate. SSL authentication is intended for the client. The client or the browser verifies the identity of the server, if it finds the server and its certificate are legitimate, then it goes ahead and established a connection. Encryption is a way to encode a message so that its contents are protected from unauthorized access. Secret-key or symmetric encryption is when a variable in cryptography is used what an algorithm to encrypt or decrypt code. Secret keys are only shared with the key\u2019s generator. Public-key or asymmetric encryption is a cryptographic system that uses pairs of keys, the public key is known to others who can use it to encrypt the data, this data can only be decrypted by using the private key which is only available to the owner.","title":"Part 1"},{"location":"Question_10/#part-2","text":"Instruction set architecture or ISA is an abstract model of a computer. A device executes instructions described by that ISA; a central processing unit is called an implementation. In general, an ISA defines the supported instructions, data types, registers, the hardware support for managing main memory, fundamental features such as memory consistency addressing modes and virtual memory, and the input/output model of the ISA. Intel created the x86 ISA which was based on the intel 8086 microprocessor. This is a proprietary ISA and it is not licensed to others like ARM\u2019s ISA. x86 ISA started as 16-bit and later a 32-bit version was developed. AMD is the only other company that has access to this ISA, and AMD created the 64-bit version which is now called x86-64 or amd64. Even though both Intel and AMD use the same ISA to make their processors, they both have different products since the microarchitecture they use is different. When we talk about x86 ISA, we are referring to the 32-bit model. x86 registers are the main tools to write programs in assembly. The registers are like variables built in the processor. Registers are much faster than accessing the memory and it makes the process fast and cleaner, but there is only a small number of registers thus for many elaborate programs, we must keep storing register values to memory and take values from the memory to store in the registers. We have 4 general 32-bit registers, EAX, EBX, ECX, and EDX. We have 6 segment registers, CS, DS, ES, FS, GS, and SS. We have5 index and pointer registers ESI, EDI, EBP, EIP and ESP, and an EFLAGS register. The general registers can be broken down into 16-bit registers which are AX, BX, CX, and DX, and can be further broken down into 8 bits, AH, AL, BH, BL, CH, CL, DH, and DL. The H and L suffix on the 8-bit registers stand for high byte and low byte. These 4 general-purpose registers have some main use, A series registers are called the accumulator register and they are mainly used for I/O port access, arithmetic, and interrupt calls. The B series registers are called the base register, it is used as a base pointer for memory access and can get some interrupt return values. The C series registers are called the counter register, they are used for loop counters, and shifts, they too get some interrupt values. The D series registers are called data registers and they are used for I/O port access, arithmetic, and some interrupt calls. Segment registers hold the segment address of various items. The indexes and pointer registers have the following uses, EDI is used as destination index register ESI is used as a source index register, EBP is used as stack base pointer, ESP is used as a stack pointer, and EIP is used as an index pointer. The EFLAGS register is a 32-bit register that holds the state of the processor, and it is used for comparing parameters, conditional loops, and conditional jumps. There are various flag bits like carry flag, parity flag, zero flag, sign flag, etc. x86 systems can address up to 232 bytes of memory, which means the memory addresses are 32-bits wide. In an assembly instruction where we require two operands, the first operand is generally the destination, which contains the data in a register or memory location and the second operand is the source. Source contains either the data to be delivered or the address of the data, generally, the source data remains unaltered after the operation. There are three basic modes of addressing: Register addressing \u2013 In this addressing mode, a register contains the operand. Depending upon the instruction, the register may be the first operand, the second, or both. Since this type of processing doesn\u2019t involve memory, it is very fast. Immediate addressing \u2013 An immediate operand has a constant value or an expression. When an instruction with two operands uses immediate addressing, the first operand may be a register or memory location, and the second operand is an immediate constant. The first operand defines the length of the data. Direct memory addressing \u2013 When operands are specified in memory addressing mode, direct access to main memory is required. This way of addressing results in slower processing of data. To locate the exact location of data in memory, we need the segment start address, which is typically found in the DS register, and an offset value, this offset value is also called effective address. In direct addressing mode, the offset value is specified directly as part of the instruction, indicated by a variable name. In direct memory addressing, one of the operands refers to a memory location, and the other operand references a register. Some basic instructions in x86 ISA are: Mov \u2013 The MOV instruction copies the data item referred to by its second operand into the locations referred to by its first operand. Push \u2013 The push instruction places its operand on top of the hardware-supported stack in memory. First push decrements ESP by 4, then places its operand into the contents of the 32-bit location at the address. The stack-pointed ESP is decremented by push since the x86 stack grows down, from high addresses to lower addresses. Pop \u2013 The pop instruction removes the 4-byte data elements from the top of the hardware supported stack into the specified operand. Add \u2013 This instruction adds its two operands, storing the result in the first operand. Sub \u2013 This instruction subtracts its two operands, storing the result in the first operand. Inc, dec \u2013 The inc instruction increments the contents of its operand by one. The dec instruction decrements the contents of its operand by one. Imul \u2013 This instruction will multiply its two operands and store the result in the first operand. There is a three-operand version where it will multiply the second and third operand and store the result in the first operand. Idiv \u2013 This instruction divides the contents of the 64-bit register which is a combination of EDX:EAX registers in x86 where EDX is the most significant 32 bits and EAX is the least significant 32 bits. The quotient result is stored in EAX while the remained is placed in EDX. And, or, xor \u2013 These instructions perform the specified logical operation on their operands, storing the result in the first operand. Not \u2013 This is the logical not and flips all bit values in the operand. Shl, shr \u2013 These instructions shift the bits in the contents of the first operand left or right. Jmp \u2013 This instruction transfers program control flow to the instruction at the memory location instructed by the operand. Jcondition \u2013 This can be je, jle, jg, or other variations and these are conditional jumps based on the status of the condition. Cmp \u2013 This compares the value of the two operands. This is basically the sub instruction, except the result of the subtraction is discarded. Call, ret \u2013 These instructions implement a subroutine call and return. x86 ISA is a 32-bit model, which means the memory addresses are of the width 32. The memory is called a stack, the stack has a width of 32 bits and it grows downwards. When we add something to the top of the stack, the ESP, or stack pointer gets decremented. When we start a program, the ESP stack pointer is at the top of the stack, when we issue a push command which will push data to the top of the memory stack, this will cause the ESP to be decremented. An interrupt is an alert to the processor and serves as a request for the processor to interrupt the currently executing code. There are three types of interrupts: Hardware interrupts \u2013 These are triggered by hardware devices like a special key being pressed on the keyboard. Hardware interrupts are typically asynchronous, their occurrence is unrelated to the instructions being executed at the time they are raised. Software interrupts \u2013 There are a series of software interrupts that are used to transfer control to a function in the operating system kernel. Software interrupts are triggered by the instruction INT. For example, INT 14H triggers interrupt 0x14, the processor stops the current program and jumps to the code to handle interrupt 14. Exceptions \u2013 Exceptions are caused by exceptional conditions in the code which is executing. For example, an attempt to divide by zero or accessing a protected memory area. The processor will detect this problem and transfer control to a handler to handle this exception.","title":"Part 2"},{"location":"Question_11/","text":"11. PART 1 Interprocess communication (file, signal, pipe, socket) PART 2 Time complexity of algorithms: insertion sort, merge sort, searching in linear and logarithmic time. Quick sort, the minimal number of necessary comparisons. Sorting in linear time: radix sort, bucket sort. Part 1 Interprocess communication or IPC is the mechanism an operating system provides to allow the processes to manage shared data. These processes are instances of a computer program that is being executed by one or many threads. There are two kinds of processes, independent processes which are not affected by the execution of other processes thus do not use IPC, and cooperating processes that can be and may affect other executing processes, here we use IPC to increase modularity. Typically, applications can use IPC as clients and servers, where the client requests data and the server responds to client requests. Many applications are both clients and servers. This communication could involve a process letting another process know that some event has occurred, or data can be transferred from one process to another. A file is a record stored on a disk or a record created on demand by a file server, this can be accessed by multiple processes. All operating systems use files for data storage. Shared files are the most basic IPC mechanism. A simple example could be when one process creates and writes to a file and another process reads this same file. A special race condition might arise when the production process and the reader process try to access the file at the same time, here we can lock the file through the produces process, no other process can access the file till the lock phase. A signal is a system message which is sent from one process to another, not usually used to transfer data but instead used to remotely command the partnered process. This is available in most operating systems. These messages trigger a specific behavior such as quitting or error handling. A signal is an asynchronous notification sent to a process to notify it of an event. When a signal is sent, the operating system interrupts the target process\u2019 normal flow of execution to deliver the signal, the target process might have a registered signal handler that executes its defined routine, otherwise, the default signal handler is executed. Some examples of system signals are SIGSTOP and SIGKILL. A pipe is a communication channel that maybe be used for one-way IPC. An implementation of pipe is often integrated with the operating system\u2019s file IO subsystem. Pipe is a mechanism by which the output of one process is directed into the input of another process, a parent process can create several new processes and arrange them in a pipeline. A pipe could be accessed as a file, but the operating system treats it as a first in first out queue. A pipe file is created using the pipe system call, which has an input end and an output end. One can write into a pipe from the input end and read it from the output end. If two processes need to communicate, then the process which writes needs to close its read end of the pipe and the process which reads needs to close its write end of the pipe. Full-duplex or two-way communication requires two pipes. Sockets provide point-to-point and two-way communication between two processes. IPC sockets enable channel-based communication for processes on the same physical device, whereas network sockets enable this kind of IPC for processes running on different devices by using networking. Network sockets need support from protocols like TCP to work, they listen on a given TCP/IP socket. IPC sockets communicate using a local file as a socket address. Sockets are bidirectional and use a server-client model, where the client requests information from the server, and the data is transferred using these sockets. Part 2 Time complexity is the amount of time taken by an algorithm to run, as a function of the length of the input. It measures the time taken to execute each statement of code in an algorithm. Time complexity can define the effectiveness of an algorithm, there may be more than one way to solve a problem in programming, knowing how the algorithm works efficiently can help programmers write better programs. Insertion sort is a simple sorting algorithm that works like when we sort playing cards. In computers, we can sort an array of numbers in an ascending order using insertion sort. Suppose we have an array of integers, and we want to sort it in ascending order, we start by splitting the array into a sorted and an unsorted part. The sorted array at first contains a single element which can be the first element in the original array, this is considered sorted since an array with a single element is sorted. Next, we look at the unsorted part, we take the first element of this part and compare it with the only element in our sorted part, if this new element is larger than our original element, then we place it at the end of the sorted part, otherwise, we place it at the beginning. Next, we will again look at the first element of the unsorted array and compare it to the last element of the sorted part, if it is larger then we place it at the end of the sorted part, otherwise, we compare it to the second last element of the sorted part, if it is larger than it goes to the right of this element otherwise it goes to its left. We repeat this process by taking the first element of the unsorted part, comparing it to all elements in the sorted part starting from the last element to the first, and placing the new element. This way we can sort an array. If n is the number of elements to be sorted, then the time complexity of insertion sort is O(n2) for the average and worst-case and O(n) for the best case. Merge sort is a sorting algorithm that works on the principle of divide and conquers. Merge sort takes an array and divides it into two halves, it then further divides those subarrays into halves until we are let with single element subarrays. Then we compare the adjacent arrays, which contain a single element, so we are comparing two adjacent elements. Here we sort them in an ascending or descending order, based on the objective. Once we have sorted adjacent pairs of elements, now we have multiple subarrays of length two which are relatively sorted. Then we can keep merging the adjacent pairs of subarrays by appointing a pointer to the left-most subarray element and comparing it to the first element of the other subarray, this way we can merge all the subarrays and finally we get a sorted array. Merge sort has the same time complexity of O(n log(n)) for best, worst, and average cases. Linear algorithms have the big O notation of O(n), while logarithmic algorithms have the big O notation of O(log n). Linear or O(n) is a brute force technique, the performance is dependent on the input size, thus as the data size scales up, the performance becomes less efficient, but for small data sizes, it can be very fast. An example could be to search for a number in an array, using a linear search algorithm, we can read each item in the array and search for the number, when we hit the number we will stop, thus in the worst case our number would be the last element of the array, in this case, if we have N elements in the array, then we must go through N number of elements. Logarithmic or O(log n) narrows down the search by repeatedly halving the dataset until we find the target value. An example of this would be binary search, a form of a logarithmic algorithm. The algorithm will traverse either upwards or downwards depending on the target value being higher, lower than, or equal to the median. The most of work this algorithm does is at the beginning, but it slowly flattens out as we discard the irrelevant range from the array and continue halving until we find the target value. We do assume that this array is sorted. As the data increases or in this example as the number of elements in the array increases, the logarithmic algorithm still performs good, whereas the linear algorithm will take way more time since it scales linearly with the data size. Quicksort is a recursive algorithm like merge sort and is used for sorting. In quick sort we use pivots. Suppose we have an array of integers, and we need to sort it in ascending order. We will first pick a random element to be the pivot, this is usually the first or the last element of the array. We place it at the root and place all elements less than it on the left side and all the elements larger than it on the right. Now we will have 2 subarrays, the one on the left and the one on the right, and the pivot element in the middle. We will apply a quick sort to both subarrays by choosing a random pivot and placing the elements to the left or right. We repeat this process for each subarray till we have single elements left at the end. Now we can simply build the sorted array by keeping the elements on the left of the pivot on the left and the ones on the right of the pivot to the right and go up. We usually use the first or last element of the array as the pivot but in some cases where we might have some information about the range of numbers present, for example, if the range of numbers in our unsorted array is between 0 and 999, then it is wiser to choose a pivot around the median or a number close to 500 since this will make the algorithm run faster. The best and average-case time complexity is O(n log(n)) and the worst-case time complexity is O(n2). The minimal number of comparisons needed is O(n log(n)) comparisons. Radix sort is a sorting algorithm where we do not compare the numbers in the array, this is in contrast to comparison-based algorithms like insertion sort and quicksort. The first step is finding the largest number in the array and identifying the number of digits it contains. Suppose we have an unsorted array of integers, and our largest number is 812, then the larger number of digits an element can have in this array is 3. Now we will convert all numbers in the array to 3 digits, for example, if we have the number 20, then it becomes 020, and the number 5 becomes 005. When dealing with numbers we will start from the least significant digit and move towards the most significant digit. Here we have decimal numbers, thus we can have 10 different numbers at a given digit place, 0-9. We will first group all the ones place digits in all numbers or the least significant number in all numbers into one of 10 buckets. We now write all the elements in an array starting from the 0th bucket and moving to the 9th bucket. This was past 1. In pass 2, we will look at the tens digit position, and again group all the elements into 10 buckets, once we do that, we will form another array of these elements based on the order starting from the 0th bucket to the 9th bucket. In pass 3, we will look at the hundreds digit or the most significant digit, and group the elements in these 10 buckets. Once this is done, we can form another array placing the elements in order from the 0th bucket to the 9th bucket. We will notice that after passing 3, the array is sorted. Thus, the number of passes required is equal to the length of the largest element in the array, and the number of buckets formed on each pass is equal to the number of different values each digit can take, 10 in this case. We can also apply radix sort to strings, for example, we have to sort an array with names using English language, here we will have 26 different buckets since each letter in an element can take one of 26 values from a-z, the number of passes required will be equal to the length of the longest name in the array. When sorting strings we will go from the most significant letter to the least significant letter. The time complexity for this sorting algorithm is O(d*(n+b)). Here d is the length of the largest element or the number passes, n is the number of elements in the array and b is the number of values an individual number of the letter can take on, in the case of integers it is 10. Bucket sort is the sister sorting algorithm of radix sort, it too does not use comparison to sort the elements. In the initial pass, we will put the elements in separate buckets, but in the case of integers, we will use the most significant number. In the case of integers, we will have 10 buckets and some elements in them. Here we will use another stable sorting algorithm to sort the elements in each bucket. Once each bucket is sorted, we can place them back in an array starting from the 0th group to the 9th group in the case of sorting integers in ascending order. The average time complexity of bucket sort is O(n+k), where n is the number of elements and k is the number of buckets and the worst time complexity is O(n2). Radix sort is good for spread out or sparse data, whereas bucket sort is good for dense data.","title":"Question 11"},{"location":"Question_11/#11-part-1-interprocess-communication-file-signal-pipe-socket-part-2-time-complexity-of-algorithms-insertion-sort-merge-sort-searching-in-linear-and-logarithmic-time-quick-sort-the-minimal-number-of-necessary-comparisons-sorting-in-linear-time-radix-sort-bucket-sort","text":"","title":"11. PART 1 Interprocess communication (file, signal, pipe, socket) PART 2 Time complexity of algorithms: insertion sort, merge sort, searching in linear and logarithmic time. Quick sort, the minimal number of necessary comparisons. Sorting in linear time: radix sort, bucket sort."},{"location":"Question_11/#part-1","text":"Interprocess communication or IPC is the mechanism an operating system provides to allow the processes to manage shared data. These processes are instances of a computer program that is being executed by one or many threads. There are two kinds of processes, independent processes which are not affected by the execution of other processes thus do not use IPC, and cooperating processes that can be and may affect other executing processes, here we use IPC to increase modularity. Typically, applications can use IPC as clients and servers, where the client requests data and the server responds to client requests. Many applications are both clients and servers. This communication could involve a process letting another process know that some event has occurred, or data can be transferred from one process to another. A file is a record stored on a disk or a record created on demand by a file server, this can be accessed by multiple processes. All operating systems use files for data storage. Shared files are the most basic IPC mechanism. A simple example could be when one process creates and writes to a file and another process reads this same file. A special race condition might arise when the production process and the reader process try to access the file at the same time, here we can lock the file through the produces process, no other process can access the file till the lock phase. A signal is a system message which is sent from one process to another, not usually used to transfer data but instead used to remotely command the partnered process. This is available in most operating systems. These messages trigger a specific behavior such as quitting or error handling. A signal is an asynchronous notification sent to a process to notify it of an event. When a signal is sent, the operating system interrupts the target process\u2019 normal flow of execution to deliver the signal, the target process might have a registered signal handler that executes its defined routine, otherwise, the default signal handler is executed. Some examples of system signals are SIGSTOP and SIGKILL. A pipe is a communication channel that maybe be used for one-way IPC. An implementation of pipe is often integrated with the operating system\u2019s file IO subsystem. Pipe is a mechanism by which the output of one process is directed into the input of another process, a parent process can create several new processes and arrange them in a pipeline. A pipe could be accessed as a file, but the operating system treats it as a first in first out queue. A pipe file is created using the pipe system call, which has an input end and an output end. One can write into a pipe from the input end and read it from the output end. If two processes need to communicate, then the process which writes needs to close its read end of the pipe and the process which reads needs to close its write end of the pipe. Full-duplex or two-way communication requires two pipes. Sockets provide point-to-point and two-way communication between two processes. IPC sockets enable channel-based communication for processes on the same physical device, whereas network sockets enable this kind of IPC for processes running on different devices by using networking. Network sockets need support from protocols like TCP to work, they listen on a given TCP/IP socket. IPC sockets communicate using a local file as a socket address. Sockets are bidirectional and use a server-client model, where the client requests information from the server, and the data is transferred using these sockets.","title":"Part 1"},{"location":"Question_11/#part-2","text":"Time complexity is the amount of time taken by an algorithm to run, as a function of the length of the input. It measures the time taken to execute each statement of code in an algorithm. Time complexity can define the effectiveness of an algorithm, there may be more than one way to solve a problem in programming, knowing how the algorithm works efficiently can help programmers write better programs. Insertion sort is a simple sorting algorithm that works like when we sort playing cards. In computers, we can sort an array of numbers in an ascending order using insertion sort. Suppose we have an array of integers, and we want to sort it in ascending order, we start by splitting the array into a sorted and an unsorted part. The sorted array at first contains a single element which can be the first element in the original array, this is considered sorted since an array with a single element is sorted. Next, we look at the unsorted part, we take the first element of this part and compare it with the only element in our sorted part, if this new element is larger than our original element, then we place it at the end of the sorted part, otherwise, we place it at the beginning. Next, we will again look at the first element of the unsorted array and compare it to the last element of the sorted part, if it is larger then we place it at the end of the sorted part, otherwise, we compare it to the second last element of the sorted part, if it is larger than it goes to the right of this element otherwise it goes to its left. We repeat this process by taking the first element of the unsorted part, comparing it to all elements in the sorted part starting from the last element to the first, and placing the new element. This way we can sort an array. If n is the number of elements to be sorted, then the time complexity of insertion sort is O(n2) for the average and worst-case and O(n) for the best case. Merge sort is a sorting algorithm that works on the principle of divide and conquers. Merge sort takes an array and divides it into two halves, it then further divides those subarrays into halves until we are let with single element subarrays. Then we compare the adjacent arrays, which contain a single element, so we are comparing two adjacent elements. Here we sort them in an ascending or descending order, based on the objective. Once we have sorted adjacent pairs of elements, now we have multiple subarrays of length two which are relatively sorted. Then we can keep merging the adjacent pairs of subarrays by appointing a pointer to the left-most subarray element and comparing it to the first element of the other subarray, this way we can merge all the subarrays and finally we get a sorted array. Merge sort has the same time complexity of O(n log(n)) for best, worst, and average cases. Linear algorithms have the big O notation of O(n), while logarithmic algorithms have the big O notation of O(log n). Linear or O(n) is a brute force technique, the performance is dependent on the input size, thus as the data size scales up, the performance becomes less efficient, but for small data sizes, it can be very fast. An example could be to search for a number in an array, using a linear search algorithm, we can read each item in the array and search for the number, when we hit the number we will stop, thus in the worst case our number would be the last element of the array, in this case, if we have N elements in the array, then we must go through N number of elements. Logarithmic or O(log n) narrows down the search by repeatedly halving the dataset until we find the target value. An example of this would be binary search, a form of a logarithmic algorithm. The algorithm will traverse either upwards or downwards depending on the target value being higher, lower than, or equal to the median. The most of work this algorithm does is at the beginning, but it slowly flattens out as we discard the irrelevant range from the array and continue halving until we find the target value. We do assume that this array is sorted. As the data increases or in this example as the number of elements in the array increases, the logarithmic algorithm still performs good, whereas the linear algorithm will take way more time since it scales linearly with the data size. Quicksort is a recursive algorithm like merge sort and is used for sorting. In quick sort we use pivots. Suppose we have an array of integers, and we need to sort it in ascending order. We will first pick a random element to be the pivot, this is usually the first or the last element of the array. We place it at the root and place all elements less than it on the left side and all the elements larger than it on the right. Now we will have 2 subarrays, the one on the left and the one on the right, and the pivot element in the middle. We will apply a quick sort to both subarrays by choosing a random pivot and placing the elements to the left or right. We repeat this process for each subarray till we have single elements left at the end. Now we can simply build the sorted array by keeping the elements on the left of the pivot on the left and the ones on the right of the pivot to the right and go up. We usually use the first or last element of the array as the pivot but in some cases where we might have some information about the range of numbers present, for example, if the range of numbers in our unsorted array is between 0 and 999, then it is wiser to choose a pivot around the median or a number close to 500 since this will make the algorithm run faster. The best and average-case time complexity is O(n log(n)) and the worst-case time complexity is O(n2). The minimal number of comparisons needed is O(n log(n)) comparisons. Radix sort is a sorting algorithm where we do not compare the numbers in the array, this is in contrast to comparison-based algorithms like insertion sort and quicksort. The first step is finding the largest number in the array and identifying the number of digits it contains. Suppose we have an unsorted array of integers, and our largest number is 812, then the larger number of digits an element can have in this array is 3. Now we will convert all numbers in the array to 3 digits, for example, if we have the number 20, then it becomes 020, and the number 5 becomes 005. When dealing with numbers we will start from the least significant digit and move towards the most significant digit. Here we have decimal numbers, thus we can have 10 different numbers at a given digit place, 0-9. We will first group all the ones place digits in all numbers or the least significant number in all numbers into one of 10 buckets. We now write all the elements in an array starting from the 0th bucket and moving to the 9th bucket. This was past 1. In pass 2, we will look at the tens digit position, and again group all the elements into 10 buckets, once we do that, we will form another array of these elements based on the order starting from the 0th bucket to the 9th bucket. In pass 3, we will look at the hundreds digit or the most significant digit, and group the elements in these 10 buckets. Once this is done, we can form another array placing the elements in order from the 0th bucket to the 9th bucket. We will notice that after passing 3, the array is sorted. Thus, the number of passes required is equal to the length of the largest element in the array, and the number of buckets formed on each pass is equal to the number of different values each digit can take, 10 in this case. We can also apply radix sort to strings, for example, we have to sort an array with names using English language, here we will have 26 different buckets since each letter in an element can take one of 26 values from a-z, the number of passes required will be equal to the length of the longest name in the array. When sorting strings we will go from the most significant letter to the least significant letter. The time complexity for this sorting algorithm is O(d*(n+b)). Here d is the length of the largest element or the number passes, n is the number of elements in the array and b is the number of values an individual number of the letter can take on, in the case of integers it is 10. Bucket sort is the sister sorting algorithm of radix sort, it too does not use comparison to sort the elements. In the initial pass, we will put the elements in separate buckets, but in the case of integers, we will use the most significant number. In the case of integers, we will have 10 buckets and some elements in them. Here we will use another stable sorting algorithm to sort the elements in each bucket. Once each bucket is sorted, we can place them back in an array starting from the 0th group to the 9th group in the case of sorting integers in ascending order. The average time complexity of bucket sort is O(n+k), where n is the number of elements and k is the number of buckets and the worst time complexity is O(n2). Radix sort is good for spread out or sparse data, whereas bucket sort is good for dense data.","title":"Part 2"},{"location":"Question_12/","text":"12. PART 1 Entity-relationship (ER) model, design with ER diagrams. Relational data model, relation, scheme, attribute. Building up a relational scheme from an ER-diagram. PART 2 Diodes. Rectifiers. DC to DC converters. Voltage regulators. Current regulators. Part 1 The entity-relationship model or ER model describes interrelated things of interest in a specific domain. A basic ER model is composed of entity types that classify the things of interest and specify relationships that can exist between entities or instances of those entity types. ER models are commonly formed to represent things a business needs to remember to perform business processes. The ER model becomes an abstract data model, that defines a data or information structure that can be implemented in a database, typically a relational database. An entity-relationship model is also called an entity-relationship diagram or ERD. ERD is a type of structural diagram for use in database design, it uses different symbols and connectors to visualize two important information, the major entities in the system and the inter-relationships among these entities. When we are talking about entities in an ERD, we are often referring to business objects such as people and roles, tangible business objects like products, and intangible business objects like logs. The relationship is how these entities relate to each other within the system. Entity-relationship diagrams are composed of entities, relationships, and attributes. They also depict cardinality which defines the relationship in terms of numbers: Entity \u2013 This is a definable thing like a person or object or event that can have data stored about it. These are typically shown as a rectangle. Entities can be a strong entity that is defined solely by their attributes and weak entity which cannot be defined solely by their own attributes. A weak entity is related to its corresponding strong entity which defines it, this kind of relationship is known as an identifying relation. A strong entity is a rectangle whereas a weak entity is a double-lined rectangle. Attribute \u2013 These are properties or characteristics of an entity. These are drawn as ovals. A super key is a set of attributes, one or more that together define an entity. The candidate key is the minimal super key which has the least number of attributes that can still be a super key. A primary key is a candidate key chosen by the database designer; this will be used to uniquely identify the entity set. A foreign key identifies the relationships between entities. A primary key will have an underline. Attributes can be simple atomic attributes, compositive attributes which can be broken down into separate attributes like a full name attribute can be broken down into a first name and last name attribute, and a derived attribute is derived from another attribute such as age can be derived from a date of birth attribute. A derived attribute is shown as an oval with a dotted outline. A multi-valued attribute can have multiple values, like a phone number attribute, this is shown as an oval with a double outline. Cardinality \u2013 This is a numerical representation of the relationship between two entities. There are three main cardinalities, one-to-one is when one attribute is related to one attribute, an example would be one student being associated with one mailing address. A one-to-many relationship means one attribute is related to many attributes, an example would be one student registers for multiple courses, but all courses have a single student registered to it. A many-to-many relationship is when multiple attributes on both entities can be related to one another, an example could be multiple students can be associated with multiple faculty members, and in turn, multiple faculty members are related to multiple students. A relationship is shown as a diamond between two entities which describes the relationship as a verb, on both sides of the diamond it connects the entities with a line, on top of this line If there is a 1 then it means it\u2019s one relationship from that side, or it can be N which means many relationships. Thus we can have different variations like 1:1, 1:N, N:1, and M:N where M:N is many to many and we use M here to differentiate it from N. An identifying relation is represented as a double-lined diamond. A relational schema is a blueprint used in database design to represent the data to be entered into the database and describes how that data is structured into tables which are called relations in relational schemas. The schema describes how those tables relate to each other. In a relational schema, the table or relation consists of a set of columns called attributes and a set of rows called tuples. Each row is unique, but the rows can be moved around, changed in order, modified, or deleted. A database designer usually starts by designing the visual ERD, once this is done, we can convert this conceptual model into a logical model which breaks down entities, attributes, and relationships into tables, columns, fields, and keys. To convert an ERD to a relational schema, we will use these steps: Convert the strong entity types into a table. Identify the primary key attribute and mention it. The attributes will be the columns. Convert the weak entity types into their table, but here the primary key will be the primary key of the strong entity type as a foreign key. The attributes will be the columns. We can now convert the relationships. There are three degrees of relationships; Binary relationships are those where two different entity types are participating. Like a student enrolled in a course. A unary relationship describes when the same entity type is the only participant. For example, a record of citizens where citizens in the same entity are married to each other, is a unary relationship. A ternary relationship is one where three different entity types are participating, for example, an online store might have tables representing customer, product, and supplier. With the degrees and cardinality, we can create the relations in an order using primary keys and foreign keys. \u25cf Binary 1:1 \u2013 Primary key of either one entity can be used as a foreign key for the other entity. \u25cf Binary 1:N \u2013 Primary key of the 1 side will be the foreign key for the N side. \u25cf Binary M:N \u2013 Here a new relation is created, in which the primary key is a composite key of two foreign keys. \u25cf Unary 1:1 \u2013 Primary key of one can be the foreign key of the other in any way. \u25cf Unary 1:N \u2013 We can use a recursive foreign key. The foreign key will reference a primary key in the same relation. \u25cf Ternary \u2013 Here we need a third table. The primary key of the new table takes foreign keys from each of the participants. Part 2 A diode is a semiconductor device that essentially acts as a one-way switch for current. It allows current to flow easily in one direction but severely restricts current from flowing in the opposite direction. Diodes are also known as rectifiers because they change alternating current AC into pulsating direct current DC. Diodes are rated according to their type, voltage, and current capacity. The polarity of diodes is determined by the anode, which is the positive lead, and the cathode, which is the negative lead. A diode symbol uses a triangle with a pipe at one end. The base of the triangle end is the anode and the pipe end is the cathode, current flows from anode to cathode. When a diode allows current, it is forward-biased. When a diode is reverse-biased, it acts as an insulator and does not permit current to flow. Rectifiers are a type of diode; rectification is an application of diodes. Rectification is the conversion of alternating current AC to direct current DC. This involves a device that only allows the one-way flow of electric charge which is the diode. The simplest kind of rectifier circuit is the half-wave rectifier. It only allows one-half of an AC waveform to pass through the load. Half-wave rectification is insufficient for more applications. The AC power source is only able to supply power to the load one-half every cycle, meaning half of its capacity is unused which makes it inefficient. Half-wave rectification is the simplest way to reduce power to a resistive load, this can be used in for example a two-position lamp dimmer switches that apply full AC power to the lamp filament for full brightness level and then half-wave rectify for less or dimmer brightness. The other kind of rectification is full-wave rectification, this is used when we need the full power of the input AC where we use both the positive and negative half-cycles of the AC wave. We can perform full-wave rectification with a full-wave center-tapped rectifier and a bridge full-wave rectifier. A center-tapped rectifier, in this rectifier we have an input circuit where the AC input is connected with a transformer in the middle connected to another circuit which is the output circuit, where we want to receive the full-wave DC output. Here the transformer side in the output circuit has a wire in the center of the transformer, thus the name, center-tapped. We connect two diodes, one to each end of the sides of the center-tapped wire. When the polarity is positive, one of the diodes is forward biased, and the other is reversed biased and the forward-biased diode and the current flows towards the load, when the polarity switches to negative, now the diode which was the previous forward-biased becomes reverse biased and the diode which was reversed biased becomes forward biased. This causes the current to flow through the second diode and to the load. This way we can use the full potential of the incoming AC, but this time our entire output is DC. The bridge rectifier has an input circuit where the AC input lines and a transformer are connected to an output circuit, this transformer does not have a center-tapped wire, this circuit is connected to 4 diodes that have one output that goes to the load. The diodes are arranged in a diamond pattern and opposite side diodes work together. Let's say we have 4 diodes, D1, D2, D3, and D4. D1 and D3 are opposite diodes and D2 and D4 are opposite diodes. When the input AC has positive polarity, the diodes D1 and D3 will be forward biased and the other two will be reverse biased, the current travels to the load through D1 and D3 when the polarity flips, now D2 and D4 will be forward biased and the other diodes will be reverse biased and the current flows to the load through D2 and D4. This way we convert all of the input AC to DC. The output of a full-wave rectifier is twice that of a half-wave rectifier. A DC-DC converter converts one DC voltage to another. The operating voltage of different electronic devices such as ICs can vary over a wide range, this makes it necessary to provide a different voltage for each element. DC-DC converters are based on an electronic circuit that uses electronic switching technology. A DC-DC converter can support both very low voltage and high voltage applications. DC-DC converters are used when the voltage must be regulated and consistent, to avoid any fluctuations. These converters use high-frequency switching circuits, together with conductors and capacitors to reduce the noise and maintain a DC voltage. Before DC-DC converters were used, electronics used rectifiers and transformers, this was inefficient and wasted a lot of energy as heat. Since DC-DC converters are extremely efficient, they are ideal for use in small devices like mobile phones and laptops. Buck converter is a type of DC-DC converter that produces a voltage that has been stepped down from the input voltage. This can be used to power lower voltage devices from a higher voltage source. A boost converter on the other hand steps up the voltage and produces a higher output voltage as opposed to the lower input voltage. A buck-boost converter is a dual-purpose DC-DC converter that can step up or step down the voltage to produce an output that may be higher or lower than the input. A voltage regulator is a system designed to automatically maintain a constant voltage. A voltage regulator may use a simple feed-forward design or may include negative feedback These can be used to regulate AC and DC voltages. Electronic voltage regulators are found in devices such as computer power supply units or PSUs where they stabilize the DC voltages used by the processor and the other elements. Just like the need to regulate the voltage, there are scenarios where we need to regulate the current which is being supplied to a particular part of a circuit. Unlike voltage regulation where we step up or step down the voltage to meet different needs, current regulation focuses on supplying a constant current, irrespective of variations in load resistance or input voltage. Current regulators involve variations in voltage or resistance to achieve the stable current output, this is based on the ohms law. An example could be if we have a variable resistor in a circuit, this variable resistor will change its resistance based on feedback from the circuit to maintain the given current value.","title":"Question 12"},{"location":"Question_12/#12-part-1-entity-relationship-er-model-design-with-er-diagrams-relational-data-model-relation-scheme-attribute-building-up-a-relational-scheme-from-an-er-diagram-part-2-diodes-rectifiers-dc-to-dc-converters-voltage-regulators-current-regulators","text":"","title":"12. PART 1 Entity-relationship (ER) model, design with ER diagrams. Relational data model, relation, scheme, attribute. Building up a relational scheme from an ER-diagram. PART 2 Diodes. Rectifiers. DC to DC converters. Voltage regulators. Current regulators."},{"location":"Question_12/#part-1","text":"The entity-relationship model or ER model describes interrelated things of interest in a specific domain. A basic ER model is composed of entity types that classify the things of interest and specify relationships that can exist between entities or instances of those entity types. ER models are commonly formed to represent things a business needs to remember to perform business processes. The ER model becomes an abstract data model, that defines a data or information structure that can be implemented in a database, typically a relational database. An entity-relationship model is also called an entity-relationship diagram or ERD. ERD is a type of structural diagram for use in database design, it uses different symbols and connectors to visualize two important information, the major entities in the system and the inter-relationships among these entities. When we are talking about entities in an ERD, we are often referring to business objects such as people and roles, tangible business objects like products, and intangible business objects like logs. The relationship is how these entities relate to each other within the system. Entity-relationship diagrams are composed of entities, relationships, and attributes. They also depict cardinality which defines the relationship in terms of numbers: Entity \u2013 This is a definable thing like a person or object or event that can have data stored about it. These are typically shown as a rectangle. Entities can be a strong entity that is defined solely by their attributes and weak entity which cannot be defined solely by their own attributes. A weak entity is related to its corresponding strong entity which defines it, this kind of relationship is known as an identifying relation. A strong entity is a rectangle whereas a weak entity is a double-lined rectangle. Attribute \u2013 These are properties or characteristics of an entity. These are drawn as ovals. A super key is a set of attributes, one or more that together define an entity. The candidate key is the minimal super key which has the least number of attributes that can still be a super key. A primary key is a candidate key chosen by the database designer; this will be used to uniquely identify the entity set. A foreign key identifies the relationships between entities. A primary key will have an underline. Attributes can be simple atomic attributes, compositive attributes which can be broken down into separate attributes like a full name attribute can be broken down into a first name and last name attribute, and a derived attribute is derived from another attribute such as age can be derived from a date of birth attribute. A derived attribute is shown as an oval with a dotted outline. A multi-valued attribute can have multiple values, like a phone number attribute, this is shown as an oval with a double outline. Cardinality \u2013 This is a numerical representation of the relationship between two entities. There are three main cardinalities, one-to-one is when one attribute is related to one attribute, an example would be one student being associated with one mailing address. A one-to-many relationship means one attribute is related to many attributes, an example would be one student registers for multiple courses, but all courses have a single student registered to it. A many-to-many relationship is when multiple attributes on both entities can be related to one another, an example could be multiple students can be associated with multiple faculty members, and in turn, multiple faculty members are related to multiple students. A relationship is shown as a diamond between two entities which describes the relationship as a verb, on both sides of the diamond it connects the entities with a line, on top of this line If there is a 1 then it means it\u2019s one relationship from that side, or it can be N which means many relationships. Thus we can have different variations like 1:1, 1:N, N:1, and M:N where M:N is many to many and we use M here to differentiate it from N. An identifying relation is represented as a double-lined diamond. A relational schema is a blueprint used in database design to represent the data to be entered into the database and describes how that data is structured into tables which are called relations in relational schemas. The schema describes how those tables relate to each other. In a relational schema, the table or relation consists of a set of columns called attributes and a set of rows called tuples. Each row is unique, but the rows can be moved around, changed in order, modified, or deleted. A database designer usually starts by designing the visual ERD, once this is done, we can convert this conceptual model into a logical model which breaks down entities, attributes, and relationships into tables, columns, fields, and keys. To convert an ERD to a relational schema, we will use these steps: Convert the strong entity types into a table. Identify the primary key attribute and mention it. The attributes will be the columns. Convert the weak entity types into their table, but here the primary key will be the primary key of the strong entity type as a foreign key. The attributes will be the columns. We can now convert the relationships. There are three degrees of relationships; Binary relationships are those where two different entity types are participating. Like a student enrolled in a course. A unary relationship describes when the same entity type is the only participant. For example, a record of citizens where citizens in the same entity are married to each other, is a unary relationship. A ternary relationship is one where three different entity types are participating, for example, an online store might have tables representing customer, product, and supplier. With the degrees and cardinality, we can create the relations in an order using primary keys and foreign keys. \u25cf Binary 1:1 \u2013 Primary key of either one entity can be used as a foreign key for the other entity. \u25cf Binary 1:N \u2013 Primary key of the 1 side will be the foreign key for the N side. \u25cf Binary M:N \u2013 Here a new relation is created, in which the primary key is a composite key of two foreign keys. \u25cf Unary 1:1 \u2013 Primary key of one can be the foreign key of the other in any way. \u25cf Unary 1:N \u2013 We can use a recursive foreign key. The foreign key will reference a primary key in the same relation. \u25cf Ternary \u2013 Here we need a third table. The primary key of the new table takes foreign keys from each of the participants.","title":"Part 1"},{"location":"Question_12/#part-2","text":"A diode is a semiconductor device that essentially acts as a one-way switch for current. It allows current to flow easily in one direction but severely restricts current from flowing in the opposite direction. Diodes are also known as rectifiers because they change alternating current AC into pulsating direct current DC. Diodes are rated according to their type, voltage, and current capacity. The polarity of diodes is determined by the anode, which is the positive lead, and the cathode, which is the negative lead. A diode symbol uses a triangle with a pipe at one end. The base of the triangle end is the anode and the pipe end is the cathode, current flows from anode to cathode. When a diode allows current, it is forward-biased. When a diode is reverse-biased, it acts as an insulator and does not permit current to flow. Rectifiers are a type of diode; rectification is an application of diodes. Rectification is the conversion of alternating current AC to direct current DC. This involves a device that only allows the one-way flow of electric charge which is the diode. The simplest kind of rectifier circuit is the half-wave rectifier. It only allows one-half of an AC waveform to pass through the load. Half-wave rectification is insufficient for more applications. The AC power source is only able to supply power to the load one-half every cycle, meaning half of its capacity is unused which makes it inefficient. Half-wave rectification is the simplest way to reduce power to a resistive load, this can be used in for example a two-position lamp dimmer switches that apply full AC power to the lamp filament for full brightness level and then half-wave rectify for less or dimmer brightness. The other kind of rectification is full-wave rectification, this is used when we need the full power of the input AC where we use both the positive and negative half-cycles of the AC wave. We can perform full-wave rectification with a full-wave center-tapped rectifier and a bridge full-wave rectifier. A center-tapped rectifier, in this rectifier we have an input circuit where the AC input is connected with a transformer in the middle connected to another circuit which is the output circuit, where we want to receive the full-wave DC output. Here the transformer side in the output circuit has a wire in the center of the transformer, thus the name, center-tapped. We connect two diodes, one to each end of the sides of the center-tapped wire. When the polarity is positive, one of the diodes is forward biased, and the other is reversed biased and the forward-biased diode and the current flows towards the load, when the polarity switches to negative, now the diode which was the previous forward-biased becomes reverse biased and the diode which was reversed biased becomes forward biased. This causes the current to flow through the second diode and to the load. This way we can use the full potential of the incoming AC, but this time our entire output is DC. The bridge rectifier has an input circuit where the AC input lines and a transformer are connected to an output circuit, this transformer does not have a center-tapped wire, this circuit is connected to 4 diodes that have one output that goes to the load. The diodes are arranged in a diamond pattern and opposite side diodes work together. Let's say we have 4 diodes, D1, D2, D3, and D4. D1 and D3 are opposite diodes and D2 and D4 are opposite diodes. When the input AC has positive polarity, the diodes D1 and D3 will be forward biased and the other two will be reverse biased, the current travels to the load through D1 and D3 when the polarity flips, now D2 and D4 will be forward biased and the other diodes will be reverse biased and the current flows to the load through D2 and D4. This way we convert all of the input AC to DC. The output of a full-wave rectifier is twice that of a half-wave rectifier. A DC-DC converter converts one DC voltage to another. The operating voltage of different electronic devices such as ICs can vary over a wide range, this makes it necessary to provide a different voltage for each element. DC-DC converters are based on an electronic circuit that uses electronic switching technology. A DC-DC converter can support both very low voltage and high voltage applications. DC-DC converters are used when the voltage must be regulated and consistent, to avoid any fluctuations. These converters use high-frequency switching circuits, together with conductors and capacitors to reduce the noise and maintain a DC voltage. Before DC-DC converters were used, electronics used rectifiers and transformers, this was inefficient and wasted a lot of energy as heat. Since DC-DC converters are extremely efficient, they are ideal for use in small devices like mobile phones and laptops. Buck converter is a type of DC-DC converter that produces a voltage that has been stepped down from the input voltage. This can be used to power lower voltage devices from a higher voltage source. A boost converter on the other hand steps up the voltage and produces a higher output voltage as opposed to the lower input voltage. A buck-boost converter is a dual-purpose DC-DC converter that can step up or step down the voltage to produce an output that may be higher or lower than the input. A voltage regulator is a system designed to automatically maintain a constant voltage. A voltage regulator may use a simple feed-forward design or may include negative feedback These can be used to regulate AC and DC voltages. Electronic voltage regulators are found in devices such as computer power supply units or PSUs where they stabilize the DC voltages used by the processor and the other elements. Just like the need to regulate the voltage, there are scenarios where we need to regulate the current which is being supplied to a particular part of a circuit. Unlike voltage regulation where we step up or step down the voltage to meet different needs, current regulation focuses on supplying a constant current, irrespective of variations in load resistance or input voltage. Current regulators involve variations in voltage or resistance to achieve the stable current output, this is based on the ohms law. An example could be if we have a variable resistor in a circuit, this variable resistor will change its resistance based on feedback from the circuit to maintain the given current value.","title":"Part 2"},{"location":"Question_13/","text":"13. PART 1 Modern processor solutions (pipeline, hazard, out-of-order execution, speculative execution, superscalar-, VLIW- and vector processors) PART 2 Optimization and evaluation of relational queries. Tree-based optimization in relational algebra. Cost-based optimization. Part 1 A CPU has a fetch, decode, execute cycle or a fetch, decode, execute, store cycle where store is a part of execute. In the fetch phase, the instruction at the program counter is read into the instruction register and the program counter is incremented by one. This instruction has been fetched and is sent to the decode phase where it is decoded. That instruction then flows to the arithmetic logic unit ALU to be executed and the value is finally stored in the memory. This cycle keeps on going on, but here we can create a pipeline where as soon as the first instruction has moved from the fetch phase to the decode phase, we can fetch the next instruction. Next, the first instruction will move to the execute phase, the second instruction can now move to the decode phase and thus we can fetch the third instruction. When all phases are full, this is called a full pipeline and we are utilizing all the CPU. Here we are executing one instruction on each cycle, whereas it would take 4 cycles to complete one instruction in our basic CPU. This is an implementation of instruction-level parallelism within a single processor. The instructions follow a sequential pipe based on the instructions fetched by the program counter. This can be analogous to a modern-day car assembly line, where an unfinished car goes through an assembly line and stops at different stations where robotic arms add different elements to the car, at all times all robotic stations are busy working on an unfinished car, this way we can produce lots of cars in less amount of time. Hazard or pipeline hazards are problems that arise in the pipeline which prevent the next instruction from executing during its designated clock cycle, this can potentially lead to incorrect computation results. There are three types of pipeline hazards: Data hazards \u2013 This is a condition in which either the source or the destination operands of an instruction are not available at the time expected in the pipeline. As a result of this some operation has to be delayed and the pipeline stalls. This can happen when there are two instructions and one of them depends on the data obtained from the other. For the two given instructions, we have A = 3 + 2 and B = A * 4, here we cannot execute the second instruction till we complete the first instruction since the second instruction needs the output of the first. In a pipelines processor, these two instructions can likely lead to incorrect results due to the data dependency between these two instructions. Structural hazards \u2013 This condition arises when two instructions require a given hardware resource at the same time and hence for one of the instructions the pipeline needs to be stalled. The most common case is when memory is accessed at the same time by two instructions. One instruction may need to access the memory as part of the execute or write back phase while the other instruction is being fetched. In this case, if both the instructions reside in the same memory, both instructions can\u2019t proceed together and one of them needs to be stalled. This can be solved by using sufficient hardware resources like more memory. Control hazard \u2013 The instruction fetch unit of the CPU is responsible for providing a stream of instruction to the execution unit, the instructions fetched by the fetch unit are in consecutive memory locations until they are executed. However, the problem arises when one of the instructions is a branching instruction to some other memory location, this makes all the other fetched instructions in the pipeline from the consecutive memory to be invalid and need to be removed, this is called flushing of the pipeline. This causes a stall till new instructions are again fetched from the memory address specified by the branch instruction. A processor that executes instructions one after the other, may use resources inefficiently which leads to poor performance. To improve the performance of the processor this can be done by executing different sub-steps of sequential instructions simultaneously. Out-of-order execution is an approach that is used in high-performance microprocessors. This approach efficiently uses instruction cycles and reduces costly delays. A processor will execute the instructions in an order of availability of data or operands instead of the original order of the instructions in the program. By doing so, the processor will avoid being idle while data is retrieved for the next instruction in the program. In other words, a processor that uses multiple execution units completes the processing of instructions in the wrong order. Suppose there are two instructions I1 and I2, in that order, in an out-of-order system the processor can execute I2 before I1 has completed. Speculative execution is an optimization technique where a computer system performs some tasks that may not be needed. Work is done before it is known whether it is needed, to prevent a delay that would have to be incurred by doing the work after it is known that it is needed. If it turns out the work was not needed, after all, most changes made by the work are reverted and the results are ignored. The objective of this is to use the extra resources if they are available or idle. This approach is used by things like branch prediction and pipelined processors. A superscalar processor is a CPU that implements a form of parallelism called instruction-level parallelism within a single processor. In contrast to a scalar processor that can execute at most one single instruction per clock cycle, a superscalar processor can execute more than one instruction during a clock cycle simultaneously by sending multiple instructions to different execution units on the processor. This allows for more throughput or the number of instructions that can be executed in a unit of time than would be otherwise possible at a fixed clock rate. Each execution unit is not a separate processor or a separate core, but it is an execution resource within a single CPU. Superscalar CPU can also be pipelined, here superscalar and pipelining are considered different performance enhancement techniques. Superscalar executes multiple instructions in parallel by using multiple execution units, whereas pipelining executes multiple instructions in the same execution unit in parallel by dividing the execution unit into different phases. VLIW or very long instruction word processors are processors in which a language compiler or pre-processor breaks program instructions down into basic operations that can be performed by the processor in parallel, this takes advantage of instruction-level parallelism. These operations are put into a very long instruction word which the processor can then take apart for further analysis, handling each operation to an appropriate functional unit. VLIW is seen as the next step past RISC processors which also work with a limited set of basic instructions and can usually execute more than one instruction at a time, being superscalar. The advantage of VLIW processors is that the complexity is moved from the hardware to software, which means that the hardware can be smaller, cheaper, and require less power to operate. The challenge is in designing a great compiler that is intelligent enough to build very long instruction words. The compiler here bundles the instructions which do not conflict with each other and make all the low-level decisions that make it very complex, the compiler also searches for all instructions which can be executed in parallel. Other than the compiler complexity, the program sizes for VLIW programs are much larger. Vector processors are processors which can execute the complete vector input in a single instruction. A vector input is dimensional a set of scalars, which are of the same type arranged in an ordered format. Thus, in other words, a vector processor is a complete unit that executes a sequential set of similar data items in the memory using a single instruction. Vector processors can greatly improve the performance on certain workloads like numerical simulation, deep learning models, and video game rendering. Part 2 Query is a request for information from a database, a SQL query needs to be processed first before it can be executed, and the required data can be received or modified. Query processing can be thought of as a block box that takes the SQL query as input and gives the result as output, this result can be returned tuples. Query processing involves three steps, parsing and translation, optimization, and finally evaluation. The first step is parsing and translation, the SQL queries cannot be understood by the system, it is a high-level implementation of the request, which is understood by humans. Thus, in this step, the aim is to convert the high-level statement into low-level relational algebra, which better represents the internal or low-level features of a query. The parser checks the syntax of the query and verifies the names of the relation, tuple, and attribute values. Then it translates it into relational algebra. For example, we need to find the salary of all employees which is more than $100k, here our SQL statement can look like SELECT EMP_NAME FROM EMPLOYEE WHERE SALARY > 100,000. This SQL statement can be converted to lots of variations of relational algebra of which are all true, two such examples could be \u03c3salary>100000 (\u03c0emp_name (Employee)) and \u03c0emp_name (\u03c3salary>100000 (Employee)). Once this is done the relational algebra expressions are sent to the optimizer. The optimizer calculates the cost of the query. Both these statements are valid relational algebra for the given SQL statement, but based on the CPU usage, disk usage, and network usage on each step, the optimizer will assign cost values to them. This information is sent to the evaluation plan which is a sequence of primitive or basic operations used to evaluate a query. The query evaluation engine takes the query evaluation plan and executes the plan and returns the answer to the query. Tree-based optimization is an optimization technique where a query tree is generated. A query tree is a tree data structure representing a relational algebra expression. The tables of the query become the leaf nodes and the relational algebra operations become the internal nodes. In this, we move from the bottom of the tree or the leaf nodes to the top to the root. We start at the deepest node which will be a table and move up creating temporary relations, we keep going up and creating these temporary relations based on the operators at the nodes. These relations are called temporary since they are stored in the memory and will be removed at the end. This way we will eventually evaluate the operation at the root of the tree which will be the result of this expression. We can implement an exhaustive search where we generate all possible query trees and then select the best one. This will give us the optimal solution but has a large time and space complexity. We can also apply rule-based heuristics which have a lower time and space complexity, but they do not necessarily give the best solution. Here a rule could be to perform select and project operations before join operations. Cost-based optimization is when the optimizer allocates a cost in numerical form which is related to each step of the possible plan and then finds the values to get the cost estimate for the plan or a possible strategy. After calculating the costs of all possible plans, the optimizer tries to choose a plan which will have the lowest cost estimate. For this reason, the optimizer is referred to as a cost-based optimizer. The cost of an operation can depend on the cardinality of the number of rows that are returned by performing the operations specified by the query execution plan, this affects the cost the most. The cost is also dependent on selectivity, which refers to the number of rows that are selected. The selectivity of any row from the table or any table from the database depends upon this condition. The cost also depends on the amount of money spent on the system to optimize the system. The different kinds of costs can be \u2013 access cost to secondary storage which is the cost of searching, reading, or writing data blocks on the secondary storage; memory usage cost which is the number of memory buffers that are needed for the execution of the query; storage cost is the cost of storing any intermediate files that are generated by the execution strategy for the query; computational cost the cost of performing the memory operations that are available on the record within the data buffers, this can also be called the CPU cost; and the communication cost which is the cost that is associated with sending or communication the query and its results from one place to another.","title":"Question 13"},{"location":"Question_13/#13-part-1-modern-processor-solutions-pipeline-hazard-out-of-order-execution-speculative-execution-superscalar-vliw-and-vector-processors-part-2-optimization-and-evaluation-of-relational-queries-tree-based-optimization-in-relational-algebra-cost-based-optimization","text":"","title":"13. PART 1 Modern processor solutions (pipeline, hazard, out-of-order execution, speculative execution, superscalar-, VLIW- and vector processors) PART 2 Optimization and evaluation of relational queries. Tree-based optimization in relational algebra. Cost-based optimization."},{"location":"Question_13/#part-1","text":"A CPU has a fetch, decode, execute cycle or a fetch, decode, execute, store cycle where store is a part of execute. In the fetch phase, the instruction at the program counter is read into the instruction register and the program counter is incremented by one. This instruction has been fetched and is sent to the decode phase where it is decoded. That instruction then flows to the arithmetic logic unit ALU to be executed and the value is finally stored in the memory. This cycle keeps on going on, but here we can create a pipeline where as soon as the first instruction has moved from the fetch phase to the decode phase, we can fetch the next instruction. Next, the first instruction will move to the execute phase, the second instruction can now move to the decode phase and thus we can fetch the third instruction. When all phases are full, this is called a full pipeline and we are utilizing all the CPU. Here we are executing one instruction on each cycle, whereas it would take 4 cycles to complete one instruction in our basic CPU. This is an implementation of instruction-level parallelism within a single processor. The instructions follow a sequential pipe based on the instructions fetched by the program counter. This can be analogous to a modern-day car assembly line, where an unfinished car goes through an assembly line and stops at different stations where robotic arms add different elements to the car, at all times all robotic stations are busy working on an unfinished car, this way we can produce lots of cars in less amount of time. Hazard or pipeline hazards are problems that arise in the pipeline which prevent the next instruction from executing during its designated clock cycle, this can potentially lead to incorrect computation results. There are three types of pipeline hazards: Data hazards \u2013 This is a condition in which either the source or the destination operands of an instruction are not available at the time expected in the pipeline. As a result of this some operation has to be delayed and the pipeline stalls. This can happen when there are two instructions and one of them depends on the data obtained from the other. For the two given instructions, we have A = 3 + 2 and B = A * 4, here we cannot execute the second instruction till we complete the first instruction since the second instruction needs the output of the first. In a pipelines processor, these two instructions can likely lead to incorrect results due to the data dependency between these two instructions. Structural hazards \u2013 This condition arises when two instructions require a given hardware resource at the same time and hence for one of the instructions the pipeline needs to be stalled. The most common case is when memory is accessed at the same time by two instructions. One instruction may need to access the memory as part of the execute or write back phase while the other instruction is being fetched. In this case, if both the instructions reside in the same memory, both instructions can\u2019t proceed together and one of them needs to be stalled. This can be solved by using sufficient hardware resources like more memory. Control hazard \u2013 The instruction fetch unit of the CPU is responsible for providing a stream of instruction to the execution unit, the instructions fetched by the fetch unit are in consecutive memory locations until they are executed. However, the problem arises when one of the instructions is a branching instruction to some other memory location, this makes all the other fetched instructions in the pipeline from the consecutive memory to be invalid and need to be removed, this is called flushing of the pipeline. This causes a stall till new instructions are again fetched from the memory address specified by the branch instruction. A processor that executes instructions one after the other, may use resources inefficiently which leads to poor performance. To improve the performance of the processor this can be done by executing different sub-steps of sequential instructions simultaneously. Out-of-order execution is an approach that is used in high-performance microprocessors. This approach efficiently uses instruction cycles and reduces costly delays. A processor will execute the instructions in an order of availability of data or operands instead of the original order of the instructions in the program. By doing so, the processor will avoid being idle while data is retrieved for the next instruction in the program. In other words, a processor that uses multiple execution units completes the processing of instructions in the wrong order. Suppose there are two instructions I1 and I2, in that order, in an out-of-order system the processor can execute I2 before I1 has completed. Speculative execution is an optimization technique where a computer system performs some tasks that may not be needed. Work is done before it is known whether it is needed, to prevent a delay that would have to be incurred by doing the work after it is known that it is needed. If it turns out the work was not needed, after all, most changes made by the work are reverted and the results are ignored. The objective of this is to use the extra resources if they are available or idle. This approach is used by things like branch prediction and pipelined processors. A superscalar processor is a CPU that implements a form of parallelism called instruction-level parallelism within a single processor. In contrast to a scalar processor that can execute at most one single instruction per clock cycle, a superscalar processor can execute more than one instruction during a clock cycle simultaneously by sending multiple instructions to different execution units on the processor. This allows for more throughput or the number of instructions that can be executed in a unit of time than would be otherwise possible at a fixed clock rate. Each execution unit is not a separate processor or a separate core, but it is an execution resource within a single CPU. Superscalar CPU can also be pipelined, here superscalar and pipelining are considered different performance enhancement techniques. Superscalar executes multiple instructions in parallel by using multiple execution units, whereas pipelining executes multiple instructions in the same execution unit in parallel by dividing the execution unit into different phases. VLIW or very long instruction word processors are processors in which a language compiler or pre-processor breaks program instructions down into basic operations that can be performed by the processor in parallel, this takes advantage of instruction-level parallelism. These operations are put into a very long instruction word which the processor can then take apart for further analysis, handling each operation to an appropriate functional unit. VLIW is seen as the next step past RISC processors which also work with a limited set of basic instructions and can usually execute more than one instruction at a time, being superscalar. The advantage of VLIW processors is that the complexity is moved from the hardware to software, which means that the hardware can be smaller, cheaper, and require less power to operate. The challenge is in designing a great compiler that is intelligent enough to build very long instruction words. The compiler here bundles the instructions which do not conflict with each other and make all the low-level decisions that make it very complex, the compiler also searches for all instructions which can be executed in parallel. Other than the compiler complexity, the program sizes for VLIW programs are much larger. Vector processors are processors which can execute the complete vector input in a single instruction. A vector input is dimensional a set of scalars, which are of the same type arranged in an ordered format. Thus, in other words, a vector processor is a complete unit that executes a sequential set of similar data items in the memory using a single instruction. Vector processors can greatly improve the performance on certain workloads like numerical simulation, deep learning models, and video game rendering.","title":"Part 1"},{"location":"Question_13/#part-2","text":"Query is a request for information from a database, a SQL query needs to be processed first before it can be executed, and the required data can be received or modified. Query processing can be thought of as a block box that takes the SQL query as input and gives the result as output, this result can be returned tuples. Query processing involves three steps, parsing and translation, optimization, and finally evaluation. The first step is parsing and translation, the SQL queries cannot be understood by the system, it is a high-level implementation of the request, which is understood by humans. Thus, in this step, the aim is to convert the high-level statement into low-level relational algebra, which better represents the internal or low-level features of a query. The parser checks the syntax of the query and verifies the names of the relation, tuple, and attribute values. Then it translates it into relational algebra. For example, we need to find the salary of all employees which is more than $100k, here our SQL statement can look like SELECT EMP_NAME FROM EMPLOYEE WHERE SALARY > 100,000. This SQL statement can be converted to lots of variations of relational algebra of which are all true, two such examples could be \u03c3salary>100000 (\u03c0emp_name (Employee)) and \u03c0emp_name (\u03c3salary>100000 (Employee)). Once this is done the relational algebra expressions are sent to the optimizer. The optimizer calculates the cost of the query. Both these statements are valid relational algebra for the given SQL statement, but based on the CPU usage, disk usage, and network usage on each step, the optimizer will assign cost values to them. This information is sent to the evaluation plan which is a sequence of primitive or basic operations used to evaluate a query. The query evaluation engine takes the query evaluation plan and executes the plan and returns the answer to the query. Tree-based optimization is an optimization technique where a query tree is generated. A query tree is a tree data structure representing a relational algebra expression. The tables of the query become the leaf nodes and the relational algebra operations become the internal nodes. In this, we move from the bottom of the tree or the leaf nodes to the top to the root. We start at the deepest node which will be a table and move up creating temporary relations, we keep going up and creating these temporary relations based on the operators at the nodes. These relations are called temporary since they are stored in the memory and will be removed at the end. This way we will eventually evaluate the operation at the root of the tree which will be the result of this expression. We can implement an exhaustive search where we generate all possible query trees and then select the best one. This will give us the optimal solution but has a large time and space complexity. We can also apply rule-based heuristics which have a lower time and space complexity, but they do not necessarily give the best solution. Here a rule could be to perform select and project operations before join operations. Cost-based optimization is when the optimizer allocates a cost in numerical form which is related to each step of the possible plan and then finds the values to get the cost estimate for the plan or a possible strategy. After calculating the costs of all possible plans, the optimizer tries to choose a plan which will have the lowest cost estimate. For this reason, the optimizer is referred to as a cost-based optimizer. The cost of an operation can depend on the cardinality of the number of rows that are returned by performing the operations specified by the query execution plan, this affects the cost the most. The cost is also dependent on selectivity, which refers to the number of rows that are selected. The selectivity of any row from the table or any table from the database depends upon this condition. The cost also depends on the amount of money spent on the system to optimize the system. The different kinds of costs can be \u2013 access cost to secondary storage which is the cost of searching, reading, or writing data blocks on the secondary storage; memory usage cost which is the number of memory buffers that are needed for the execution of the query; storage cost is the cost of storing any intermediate files that are generated by the execution strategy for the query; computational cost the cost of performing the memory operations that are available on the record within the data buffers, this can also be called the CPU cost; and the communication cost which is the cost that is associated with sending or communication the query and its results from one place to another.","title":"Part 2"},{"location":"Question_14/","text":"14. PART 1 Explain the NAT/PAT address translation mechanisms. PART 2 Basic notions concerning data structures: modelling, abstraction, abstract data types. Elementary data structures: lists, stacks, queues. Sets, multisets, arrays. The representation of trees, tree traversal, deletion and insertion. Part 1 NAT or network address translation is a technology that runs on the edge devices between a wide area network or the internet and local area networks. These edge devices are usually routers. The role of NAT is to convert public IP addresses to private IP addresses or map public IP addresses to private IP addresses. Public IP addresses are registered IP addresses that can be found on the internet, on the other hand, private IP addresses are bound to their local network and cannot connect to the internet on their own. Organizations and houses having multiple devices choose from a set of defined ranges of private IP addresses, one such range is from 192.168.0.1 to 192.168.255.255, this allows for 216 private addresses, which is a huge number. Thus, two separate organizations can have devices with the same private IP address. Now to connect a device having a private IP address to the internet, the edge device or the router will translate the private address to a public address, and this information is stored in a NAT table. The NAT table keeps a record of these mappings, this way the packets are routed to and from their correct endpoints. We have two basic types of NAT which are: SNAT or static NAT \u2013 Static NAT is when the edge device will do a one-to-one mapping between the public address and the private address. These mappings are written to the NAT table and do not change; thus they are static. This type of implementation includes lots of manual mapping when the network is set up, it is also wasteful since the organization has to buy as many public addresses as private addresses or end devices. DNAT or dynamic NAT \u2013 Dynamic NAT is when the edge device will have a pool of public addresses assigned to it. When a device on the local network requests to connect to the internet, the edge device will assign a public address to that private address dynamically from the pool of free public addresses. Once the device ends the connection or times out due to inactivity, the corresponding private address is freed up and put back into the pool of public addresses, ready to be reassigned to a new private address. This implementation is very costly since the organization has to purchase a pool of public addresses upfront, this is also wasteful. The IPv4 system uses a 32-bit address for the public IP addresses, which means we can have at most 232 IP addresses or around 4.3 billion public addresses. This was fine at the early times of the internet when we had a very small number of users, using the dial-up connection. Under the dial-up connection, a user would establish the connection and be assigned a public address, once the connection was over, that public address was freed up. But due to the explosion of the number of users connecting to the internet, especially the number of devices connecting to the internet per user, one single user might have a smart TV, laptop, mobile phone, smartwatch, a smart speaker connected to the internet. This caused us to run out of the 4.3 billion addresses on the IPv4 system. This is where PAT comes into play. PAT is port address translation, it is an upgrade to the NAT system where the edge device can have just one public IP address, which it can map to multiple local private addresses. In PAT each device on the network is assigned a port number, when a device on the private network connects to the internet, the private IP address is translated to the public IP address with the port number appended to it giving it a unique IP address. Returning packets are swapped back using the NAT table. Using PAT we can perform many to one mapping, which means multiple internal hosts can connect to the internet using a single public address. This makes PAT very cheap since the router has to have just one public IP address and it is not wasteful as well. The usage of PAT might become the thing of the past since the introduction of IPv6 which uses 64-bit addresses, which is more than enough to allow a separate public address to each device. Part 2 A data structure is a data organization, management, and storage format that enables efficient access and modification. A data structure is a collection of data values, the relationships among them, and the functions or operations that can be applied to the data. Data structures serve as the basis for abstract data types or ADT. ADT defines the logical form of the data type; the data structure implements the physical form of the data type. Different types of data structures are suited for different kinds of applications, some are highly specialized to specific tasks like a compiler implementation usually uses hash tables to look up identifiers. Data modeling is the process of creating a visual representation of either a whole information system or parts of it to communicate connections between data points and structures. The goal is to illustrate the types of data used and stored within the system, the relationships among these data types, the ways the data can be grouped and organized, and its formats and attributes. Data models are built around business needs. Rules and requirements are defined upfront through feedback from business stakeholders so they can be incorporated into the design of a new system or adapted to the existing one. Data can be modeled at various levels of abstraction. The process begins by getting the business requirements, these are translated into data structures to formulate a concrete database design. Data modeling uses standardized schemas and formal techniques which makes it consistent and easy to fix later by someone who did not design the model in the first place. Data models can be categorized into three categories with decreasing levels of abstraction. These models are: Conceptual data model \u2013 They offer a bigger picture view of what the system will contain and how it will be organized. Conceptual models are created as a part of the initial requirement gathering process. Typically, they include entity classes, their characteristics and constraints, and relationships between them. Logical data model \u2013 They are less abstract and provide more detail about the concepts and relationships in the domain. One of several formal data modeling notations is followed. These indicate data attributes, such as data types and their lengths, and show relationships among the entities. Logical data models don\u2019t specify any technical system requirement. Physical data model \u2013 These provide a schema for how the data will be physically stored within a database, they are the least abstract of all three models. They offer a final design that can be implemented as a relational database, including tables that show the relationships among entities as well as the primary keys and the foreign keys that will be used to maintain the relationships. Physical data models can include database management system-specific properties. Abstract data types are an abstraction of a data structure that provides only the interface to which the data structure must adhere. The interface does not give any specific details about how the data structure should be implemented or in what programming language. The reason we choose to abstract the implementation of the data structure is that different programming languages have a different strategies to implement the same data structure, for example in C a data structure is implemented using a structure or a struct, but in C++ a data structure is implemented using objects and classes. An example of an abstract data type is a stack that can be created with both linked list and array, but we do not need to know the implementation, just the interface or the operations this stack can perform. A list or a linked list is a linear data structure. A linked list is like a chain of nodes, where each node contains information like data and a pointer to the succeeding node in the chain. There\u2019s a head pointer, which points to the first element of the linked list, and if the list is empty then it simply points to null or nothing. Linked lists are used to implement file systems, hash tables, etc. A linked list can be a singly linked list or unidirectional and double linked list or bidirectional. We can insert an element at the end of a linked list, delete a given element, search for elements or check if the linked list is empty. Stack is an elementary data structure where we place the latest data in the memory in an order that the last one appears first. A stack is like piling books which is the data in this analogy, one on top of the other, if we have to remove the last element then we can remove the element at the top of the stack or remove the book at the top, but if we want to remove a book somewhere in the middle, then we have to remove all books on top of the one which we need to remove first. This is the LIFO or last in first out method. Some basic operations we can use on stacks are push when we insert an element to the top, pop when we remove the top element and return it, and top which returns the top element of the stack without removing it. We can also check if the stack is empty or not. Queues are linear data structures that store elements sequentially. They are like a stack but instead of using the last in first out method LIFO, it uses the first in first out or FIFO method. As the name implies, it is like a queue, it\u2019s like people standing in a ticket line, where people are the data, the person at the very front will get the ticket first, more people will be added at the back of the line and the person who came early will get the ticket before a person who came later. Elements are inserted at the back and accessed at the front of the queue. Some operations we can perform on queues are inserting an element to the end of the queue, removing an element at the start of the queue, returning the first element of the queue, and checking if the queue is empty. Set is a data structure that can store any number of unique values in any order. Sets do not allow non-repeated values. Sets can use the mathematical set operations like union, intersection, and difference. We can create hash tables using sets, has tables are data structures where we have key-value pairs, we can add a unique key and its value, we can also remove a key that will remove its value, and we can change the value of a key by using the key name as the index. In the Python programming language, dictionaries are hash tables. A multiset is a data structure that stores and manipulates an unordered collection of elements that may be repeated. An array the is most widely used data structure. Each data element is assigned a positive numerical value called the index, which corresponds to the position of that item in the array. Most languages define the starting index of the array as 0 except for MATLAB which starts at 1. One dimensional array contains a single chain of elements or vector data, multi-dimensional arrays contain arrays within arrays like tensors. We can insert an element at a given index of the array, we can get an element from any index position, delete an element at any index and get the size of the array. A tree is a non-linear data structure, this is in contrast with arrays, linked lists, stacks of a queue all of which are linear data structures. A tree data structure is organized hierarchically, there is one single top-level node called the root node, this root node had child nodes. Each node that has more child nodes of its own is called a parent node and nodes that do not have any child nodes are called a terminal node or a leaf node. A given branch of the tree terminates at a lead node. We can create subtrees from a tree that is not the entire tree but a part of the original tree starting at some parent node and including all its descendent nodes. The depth of a node is the length from the root to the given node. A common type of tree is the binary search tree. In a binary search tree, each node has two child nodes, except the lead nodes which have no child nodes. The value of a child node can also be null. This tree is arranged in a way so that every left node is less than its parent node which is less than all nodes on the right side. For example, if we have three elements, 4,5,6, we can place 5 at the root with 4 being the left child node and 6 being the right child node. Finding an element in such an arrangement is very fast since we already know if the left subtree would be less than or more than the given element to find, this way on each step we half the number of elements to search for. Unlike linear data structures like arrays and linked lists which have only one logical way to traverse them, trees can be traversed in different ways. These ways are: In-order \u2013 In this method, the left subtree is visited first, then root, and then the right subtree. Pre-order \u2013 In this method, the root node is visited first, then the left subtree, and finally the right subtree. Post-order \u2013 In this method, the root is visited last, first, we traverse the left subtree, then the right, and finally the root. Insertion works in a similar way to searching. We get the element and see if the root node is less than or more than it, based on that we go either left or right and repeat this on every node till we find the spot for it. If the inserted node is not a leaf node, then we have to copy the values of nodes affected by this addition and make them child nodes or parents\u2019 nodes of the added node. Deletion for leaf nodes is simple since we search for it and simply delete it. To delete a node with children, we need to find the inorder successor of that node, then copy it and subsequently change other nodes in the tree.","title":"Question 14"},{"location":"Question_14/#14-part-1-explain-the-natpat-address-translation-mechanisms-part-2-basic-notions-concerning-data-structures-modelling-abstraction-abstract-data-types-elementary-data-structures-lists-stacks-queues-sets-multisets-arrays-the-representation-of-trees-tree-traversal-deletion-and-insertion","text":"","title":"14. PART 1 Explain the NAT/PAT address translation mechanisms. PART 2 Basic notions concerning data structures: modelling, abstraction, abstract data types. Elementary data structures: lists, stacks, queues. Sets, multisets, arrays. The representation of trees, tree traversal, deletion and insertion."},{"location":"Question_14/#part-1","text":"NAT or network address translation is a technology that runs on the edge devices between a wide area network or the internet and local area networks. These edge devices are usually routers. The role of NAT is to convert public IP addresses to private IP addresses or map public IP addresses to private IP addresses. Public IP addresses are registered IP addresses that can be found on the internet, on the other hand, private IP addresses are bound to their local network and cannot connect to the internet on their own. Organizations and houses having multiple devices choose from a set of defined ranges of private IP addresses, one such range is from 192.168.0.1 to 192.168.255.255, this allows for 216 private addresses, which is a huge number. Thus, two separate organizations can have devices with the same private IP address. Now to connect a device having a private IP address to the internet, the edge device or the router will translate the private address to a public address, and this information is stored in a NAT table. The NAT table keeps a record of these mappings, this way the packets are routed to and from their correct endpoints. We have two basic types of NAT which are: SNAT or static NAT \u2013 Static NAT is when the edge device will do a one-to-one mapping between the public address and the private address. These mappings are written to the NAT table and do not change; thus they are static. This type of implementation includes lots of manual mapping when the network is set up, it is also wasteful since the organization has to buy as many public addresses as private addresses or end devices. DNAT or dynamic NAT \u2013 Dynamic NAT is when the edge device will have a pool of public addresses assigned to it. When a device on the local network requests to connect to the internet, the edge device will assign a public address to that private address dynamically from the pool of free public addresses. Once the device ends the connection or times out due to inactivity, the corresponding private address is freed up and put back into the pool of public addresses, ready to be reassigned to a new private address. This implementation is very costly since the organization has to purchase a pool of public addresses upfront, this is also wasteful. The IPv4 system uses a 32-bit address for the public IP addresses, which means we can have at most 232 IP addresses or around 4.3 billion public addresses. This was fine at the early times of the internet when we had a very small number of users, using the dial-up connection. Under the dial-up connection, a user would establish the connection and be assigned a public address, once the connection was over, that public address was freed up. But due to the explosion of the number of users connecting to the internet, especially the number of devices connecting to the internet per user, one single user might have a smart TV, laptop, mobile phone, smartwatch, a smart speaker connected to the internet. This caused us to run out of the 4.3 billion addresses on the IPv4 system. This is where PAT comes into play. PAT is port address translation, it is an upgrade to the NAT system where the edge device can have just one public IP address, which it can map to multiple local private addresses. In PAT each device on the network is assigned a port number, when a device on the private network connects to the internet, the private IP address is translated to the public IP address with the port number appended to it giving it a unique IP address. Returning packets are swapped back using the NAT table. Using PAT we can perform many to one mapping, which means multiple internal hosts can connect to the internet using a single public address. This makes PAT very cheap since the router has to have just one public IP address and it is not wasteful as well. The usage of PAT might become the thing of the past since the introduction of IPv6 which uses 64-bit addresses, which is more than enough to allow a separate public address to each device.","title":"Part 1"},{"location":"Question_14/#part-2","text":"A data structure is a data organization, management, and storage format that enables efficient access and modification. A data structure is a collection of data values, the relationships among them, and the functions or operations that can be applied to the data. Data structures serve as the basis for abstract data types or ADT. ADT defines the logical form of the data type; the data structure implements the physical form of the data type. Different types of data structures are suited for different kinds of applications, some are highly specialized to specific tasks like a compiler implementation usually uses hash tables to look up identifiers. Data modeling is the process of creating a visual representation of either a whole information system or parts of it to communicate connections between data points and structures. The goal is to illustrate the types of data used and stored within the system, the relationships among these data types, the ways the data can be grouped and organized, and its formats and attributes. Data models are built around business needs. Rules and requirements are defined upfront through feedback from business stakeholders so they can be incorporated into the design of a new system or adapted to the existing one. Data can be modeled at various levels of abstraction. The process begins by getting the business requirements, these are translated into data structures to formulate a concrete database design. Data modeling uses standardized schemas and formal techniques which makes it consistent and easy to fix later by someone who did not design the model in the first place. Data models can be categorized into three categories with decreasing levels of abstraction. These models are: Conceptual data model \u2013 They offer a bigger picture view of what the system will contain and how it will be organized. Conceptual models are created as a part of the initial requirement gathering process. Typically, they include entity classes, their characteristics and constraints, and relationships between them. Logical data model \u2013 They are less abstract and provide more detail about the concepts and relationships in the domain. One of several formal data modeling notations is followed. These indicate data attributes, such as data types and their lengths, and show relationships among the entities. Logical data models don\u2019t specify any technical system requirement. Physical data model \u2013 These provide a schema for how the data will be physically stored within a database, they are the least abstract of all three models. They offer a final design that can be implemented as a relational database, including tables that show the relationships among entities as well as the primary keys and the foreign keys that will be used to maintain the relationships. Physical data models can include database management system-specific properties. Abstract data types are an abstraction of a data structure that provides only the interface to which the data structure must adhere. The interface does not give any specific details about how the data structure should be implemented or in what programming language. The reason we choose to abstract the implementation of the data structure is that different programming languages have a different strategies to implement the same data structure, for example in C a data structure is implemented using a structure or a struct, but in C++ a data structure is implemented using objects and classes. An example of an abstract data type is a stack that can be created with both linked list and array, but we do not need to know the implementation, just the interface or the operations this stack can perform. A list or a linked list is a linear data structure. A linked list is like a chain of nodes, where each node contains information like data and a pointer to the succeeding node in the chain. There\u2019s a head pointer, which points to the first element of the linked list, and if the list is empty then it simply points to null or nothing. Linked lists are used to implement file systems, hash tables, etc. A linked list can be a singly linked list or unidirectional and double linked list or bidirectional. We can insert an element at the end of a linked list, delete a given element, search for elements or check if the linked list is empty. Stack is an elementary data structure where we place the latest data in the memory in an order that the last one appears first. A stack is like piling books which is the data in this analogy, one on top of the other, if we have to remove the last element then we can remove the element at the top of the stack or remove the book at the top, but if we want to remove a book somewhere in the middle, then we have to remove all books on top of the one which we need to remove first. This is the LIFO or last in first out method. Some basic operations we can use on stacks are push when we insert an element to the top, pop when we remove the top element and return it, and top which returns the top element of the stack without removing it. We can also check if the stack is empty or not. Queues are linear data structures that store elements sequentially. They are like a stack but instead of using the last in first out method LIFO, it uses the first in first out or FIFO method. As the name implies, it is like a queue, it\u2019s like people standing in a ticket line, where people are the data, the person at the very front will get the ticket first, more people will be added at the back of the line and the person who came early will get the ticket before a person who came later. Elements are inserted at the back and accessed at the front of the queue. Some operations we can perform on queues are inserting an element to the end of the queue, removing an element at the start of the queue, returning the first element of the queue, and checking if the queue is empty. Set is a data structure that can store any number of unique values in any order. Sets do not allow non-repeated values. Sets can use the mathematical set operations like union, intersection, and difference. We can create hash tables using sets, has tables are data structures where we have key-value pairs, we can add a unique key and its value, we can also remove a key that will remove its value, and we can change the value of a key by using the key name as the index. In the Python programming language, dictionaries are hash tables. A multiset is a data structure that stores and manipulates an unordered collection of elements that may be repeated. An array the is most widely used data structure. Each data element is assigned a positive numerical value called the index, which corresponds to the position of that item in the array. Most languages define the starting index of the array as 0 except for MATLAB which starts at 1. One dimensional array contains a single chain of elements or vector data, multi-dimensional arrays contain arrays within arrays like tensors. We can insert an element at a given index of the array, we can get an element from any index position, delete an element at any index and get the size of the array. A tree is a non-linear data structure, this is in contrast with arrays, linked lists, stacks of a queue all of which are linear data structures. A tree data structure is organized hierarchically, there is one single top-level node called the root node, this root node had child nodes. Each node that has more child nodes of its own is called a parent node and nodes that do not have any child nodes are called a terminal node or a leaf node. A given branch of the tree terminates at a lead node. We can create subtrees from a tree that is not the entire tree but a part of the original tree starting at some parent node and including all its descendent nodes. The depth of a node is the length from the root to the given node. A common type of tree is the binary search tree. In a binary search tree, each node has two child nodes, except the lead nodes which have no child nodes. The value of a child node can also be null. This tree is arranged in a way so that every left node is less than its parent node which is less than all nodes on the right side. For example, if we have three elements, 4,5,6, we can place 5 at the root with 4 being the left child node and 6 being the right child node. Finding an element in such an arrangement is very fast since we already know if the left subtree would be less than or more than the given element to find, this way on each step we half the number of elements to search for. Unlike linear data structures like arrays and linked lists which have only one logical way to traverse them, trees can be traversed in different ways. These ways are: In-order \u2013 In this method, the left subtree is visited first, then root, and then the right subtree. Pre-order \u2013 In this method, the root node is visited first, then the left subtree, and finally the right subtree. Post-order \u2013 In this method, the root is visited last, first, we traverse the left subtree, then the right, and finally the root. Insertion works in a similar way to searching. We get the element and see if the root node is less than or more than it, based on that we go either left or right and repeat this on every node till we find the spot for it. If the inserted node is not a leaf node, then we have to copy the values of nodes affected by this addition and make them child nodes or parents\u2019 nodes of the added node. Deletion for leaf nodes is simple since we search for it and simply delete it. To delete a node with children, we need to find the inorder successor of that node, then copy it and subsequently change other nodes in the tree.","title":"Part 2"},{"location":"Question_15/","text":"15. PART 1 Basic concepts of object-oriented paradigm. Class, object, instantiation. Inheritance, class hierarchy. Polymorphism, method overloading. Scoping, information hiding, accessibility levels. Abstract classes and interfaces. Class diagram of UML. PART 2 Compare the SNMP and RMON network management systems. Part 1 Object-oriented programming is a programming model that emphasizes objects. The object-oriented programming model considers data important rather than actions or functions. OOP is not a programming language but a model that other programming languages follow, it was brought into the spotlight because of the popularity of Java and is now ubiquitous everywhere like C++ which is an OOP implementation of C, C#, and Python. There are four pillars of OOP: Abstraction \u2013 Abstraction means to hide the implementation details. When we call a function, we do not need to understand what it is exactly doing. This allows us to abstract a lot of functions in a big codebase and focuses on building better applications. This also helps us create reusable and easy-to-understand and easy-to-change code by abstracting away certain details. The function could be defined in a separate class and can be called, we can simply pass the parameters and get the output. Polymorphism \u2013 This means the condition of occurring in several different forms. This is the method in an OOP language that performs different things as per the object\u2019s class which calls it. With polymorphism, a message is sent to multiple class objects, and every object responds appropriately according to the properties of its class. There are typical kinds of polymorphism, static polymorphism like method overloading and dynamic polymorphism like method overriding. Inheritance \u2013 This lets one class acquire the attributes and methods of another class. The main benefit here is reusability. We might have multiple things that do mostly the same thing with small changes, here we use inheritance. The parent and the child will be closely related for it to make sense. A child class will inherit all its parents\u2019 attributes and methods, then it can extend to it and have its attributes and methods on top of the ones from the parent. For example, we might have a main class Car, we can then create two new child classes from it called BMW and AUDI and they can have their additions based on their features. Single inheritance is when a class has only one parent class, multiple inheritances are when a child class has more than one parent class. Multi-level inheritance is when for example class 3 inherits from class 2 which also inherits from class 1 and hierarchal inheritance is when a single parent\u2019s class has multiple child classes like the car example. If a child class has a method that is inherited from its parent, but requires a different implementation, here we can use method overriding, this is changing the implementation of an already present method. Encapsulation \u2013 This refers to the bundling of data with methods into a single unit. OOPs, languages use encapsulation in the form of a class which is a program code template that allows developers to create an object that has both the data and the functions/methods. This implementation is used to hide the internal representation or state of an object from the outside. The idea is to hide the attribute of the object from the outside and bundle it with methods that provide read or write access to it, this way we can hide specific information and control access to the internal state of the object. In java, we use getters and setter methods. The getter methods retrieve an attribute and the setter method changes it. Class is a blueprint or a set of instructions to build a specific type of object. It revolves around real-life entities or objects, like a car can be a class and have data like the color and manufacture date and methods like returning the mileage based on the car\u2019s life. A class is a logical entity and in Java, it is declared using the class keyboard for example class Student{}. An object is an instance of a class, it is a real-world entity such as a laptop, mobile, etc, it is a physical entity. An object in Java is mainly created through the new keyword, for example, Student s1 = new Student(), here we define an object with the name s1 and it is of the class Student. A class is declared once since it is the blueprint, but we can have as many objects of that class, every time we create a new object, we allocate memory to it. Instantiation is the creation of an object which is an instance of a given class. When we instantiate an object, we define its attributes which are requested by the class, this allocates memory for this object. The class hierarchy is also called the inheritance tree. Due to inheritance a child class can extend or inherit from its parent class, this can form a hierarchy. A child class can inherit from one parent class or multiple parent classes, a parent class can have multiple child classes. This hierarchy can be as deep as needed and can branch out in any direction. Method overloading is a form of polymorphism. Overloading happens when we have two methods of the same name but different arguments. These methods are differentiated based on the number and type of parameters passed as arguments. An example could be, we have two methods of the same name sum, the first method is defined as having 2 integer inputs and the second method is defined as having 3 floating-point inputs. Now if we call this sum(1,2), then the program will call the first method, but if we call sum(1.0,2.5,4.5), then we will call the second method. The advantage of method overloading is that it increases code readability and maintainability. The appropriate method is called for which the arguments are provided during runtime. The scope of an entity such as a variable defines in what part of a computer program that name can be used to refer to the entity. In other parts of the program, the same name may refer to a different entity or it might not exist at all. We have three types of scope in an OOP language like Java: Public attributes and functions are available to every type of object that attempts to access them. Protected attributes and functions can be accessed within their package, or by a subclass of its class in another package. Private attributes and functions can only be accessed by their class. Information hiding is about protecting data or information from changing them by mistake throughout the program. It protects data from modification by other parts of the program. This hidden data can be accessed indirectly using safe mechanisms or methods. Private, protected and the public are the accessibility levels. An abstract class is declared with the abstract keyword. It can have abstract and nonabstract methods. This class cannot be instantiated, but it can be subclassed. When an abstract class is subclassed, the subclass provides implementations for all the abstract methods in the parent class. An abstract method is declared without an implementation. An interface is just the declaration of methods of an object, it is not the implementation. In an interface, we define the kind of operations an object can perform. These operations are defined by the class that implements the interface. A unified modeling language UML class diagram is a structure diagram that describes the structure of a system by showing the classes, their attributes, their methods, and the relationships among objects. It is like a relational schema for databases. The classes are represented as rectangles having two parts, the attributes or the characteristics of that object, and the methods or the functions of that object. Each attribute will have its datatype defined, like string or integer. The methods will have open and close parenthesis to show that they are the methods. For all attributes and methods, we show the visibility or accessibility with different symbols: - sign means this attribute or method is private and can only be accessed by its class, + means this attribute or method is public and can be accessed by anything in the source code, # means this attribute or method is protected and can be accessed by its class or any subclasses, and ~ means this attribute or method is packaged and can be accessed by anything in its package. Attributes are generally private, and the methods are public or protected. Once we have defined separate classes, we create the relationships between them. Inheritance is represented by an arrow, using inheritance we can define multiple child classes which are like the parent class but require some changes, for example, an animal abstract class can be used to create a lion, zebra, and a gorilla without having to repeat the same attributes and methods. An abstract class cannot be instantiated on its own, but it is instantiated through a subclass, these are written with italics in the UML diagram. Another type of relationship is association shown with a straight line, this is a simple relationship without any dependency between two classes, for example, the class cow eats class grass. The next type of relationship is aggregation, which is shown with a line and an empty diamond, in this relationship a class can be a part of the other class, but it does not have to be, there are no strict rules. The last type of relationship is composition, this is shown as a straight line with a filled-in diamond. In this relationship, the child object will not exist without the parent object, for example, a house class and a bedroom class. The bedroom will not exist if the house class is destroyed. This way we can model an application by representing its classes and the relationships between them. Part 2 SNMP or simple network management protocol is a way for different devices on a network to share information. It allows devices to communicate even if the devices are different hardware and run different software. It is a vital tool for effective network management. SNMP is a simple architecture based on a client-server model. The servers are called managers, they collect and process information about devices on the network. The clients are called agents, these are any device connected to the network. They can include computers, network switches, phones, printers, and so on. To provide flexibility, SNMP doesn\u2019t require network devices to exchange data in a rigid format of fixed size. Instead, it uses a tree-like format, under which data is always available for managers to collect, thus we use the UDP protocol and SNMP works on the UDP port number 161 by default. The data tree consists of multiple tables or branches which are called management information bases or MIBs. MIBs group together particular types of devices, each MIB has a unique identifying number and an identifying string. These numbers and strings can be used interchangeably. Each MIB consists of one or more nodes, which represent individual devices on the network and each node has a unique object identifier or OID. The OID for a given node is determined by the identifier of the MIB on which it exists combined with the node\u2019s identifier within its MIB. Using OID a manager can query an agent to find information about a device on the network, for example, if the manager wants to know whether an interface is up, it will first query the interface MIB, then check the OID value which tells the operational status to determine if the interface is up. When the admin asks for the status of a device, it's called polling, and when a device replies, it\u2019s called a trap or a notification. RMON or remote monitoring is an extension to SNMP. RMON is a method of monitoring network traffic on a remote ethernet to find network issues. These issues can be dropped packets, network collisions, or traffic congestion. RMON was developed to address the weak points of a standard MIB, which could not provide statistics on the data link and physical layer parameters. With RMON, a network admin can set up performance thresholds that create alerts when these thresholds are crossed, this allows us to maintain a proactive network management strategy. RMON probes are hardware or software elements of a network device or a software embedded device like a router or a switch. The RMON probe is set on the device on a TCP/IP subnet, this software runs on the network device and gets the information about the network and traffic activity. This information can be passed back to the SNMP manager console for analysis and reporting. The differences between SNMP and RMON are, SNMP is a device-oriented protocol and focuses on how devices are functioning over the network while RMON is more network-oriented. While SNMP is used for monitoring devices over the network, RMON monitors the network itself and provides information to the network administrator which can be used to troubleshoot and make changes according to the needs of the network. Another difference between SNMP and RMON is that SNMP runs on preconfigured software to ensure that the network runs optimal, but RMON gives the ability to the network administrator to use the information and make the network run at peak performance. RMON is usually used to do an in-depth analysis of the network to a point that it can be used to find out how many bytes and packets were transferred and if any were dropped to bandwidth and internet access. An SNMP device, such as a router usually needs additional software installed on it to provide RMON functionality to turn it into a probe. SNMP is always the first line of defense when retrieving information about the network and what is happening in the network, RMON is an extension and makes it easier and gives us more information.","title":"Question 15"},{"location":"Question_15/#15-part-1-basic-concepts-of-object-oriented-paradigm-class-object-instantiation-inheritance-class-hierarchy-polymorphism-method-overloading-scoping-information-hiding-accessibility-levels-abstract-classes-and-interfaces-class-diagram-of-uml-part-2-compare-the-snmp-and-rmon-network-management-systems","text":"","title":"15. PART 1 Basic concepts of object-oriented paradigm. Class, object, instantiation. Inheritance, class hierarchy. Polymorphism, method overloading. Scoping, information hiding, accessibility levels. Abstract classes and interfaces. Class diagram of UML. PART 2 Compare the SNMP and RMON network management systems."},{"location":"Question_15/#part-1","text":"Object-oriented programming is a programming model that emphasizes objects. The object-oriented programming model considers data important rather than actions or functions. OOP is not a programming language but a model that other programming languages follow, it was brought into the spotlight because of the popularity of Java and is now ubiquitous everywhere like C++ which is an OOP implementation of C, C#, and Python. There are four pillars of OOP: Abstraction \u2013 Abstraction means to hide the implementation details. When we call a function, we do not need to understand what it is exactly doing. This allows us to abstract a lot of functions in a big codebase and focuses on building better applications. This also helps us create reusable and easy-to-understand and easy-to-change code by abstracting away certain details. The function could be defined in a separate class and can be called, we can simply pass the parameters and get the output. Polymorphism \u2013 This means the condition of occurring in several different forms. This is the method in an OOP language that performs different things as per the object\u2019s class which calls it. With polymorphism, a message is sent to multiple class objects, and every object responds appropriately according to the properties of its class. There are typical kinds of polymorphism, static polymorphism like method overloading and dynamic polymorphism like method overriding. Inheritance \u2013 This lets one class acquire the attributes and methods of another class. The main benefit here is reusability. We might have multiple things that do mostly the same thing with small changes, here we use inheritance. The parent and the child will be closely related for it to make sense. A child class will inherit all its parents\u2019 attributes and methods, then it can extend to it and have its attributes and methods on top of the ones from the parent. For example, we might have a main class Car, we can then create two new child classes from it called BMW and AUDI and they can have their additions based on their features. Single inheritance is when a class has only one parent class, multiple inheritances are when a child class has more than one parent class. Multi-level inheritance is when for example class 3 inherits from class 2 which also inherits from class 1 and hierarchal inheritance is when a single parent\u2019s class has multiple child classes like the car example. If a child class has a method that is inherited from its parent, but requires a different implementation, here we can use method overriding, this is changing the implementation of an already present method. Encapsulation \u2013 This refers to the bundling of data with methods into a single unit. OOPs, languages use encapsulation in the form of a class which is a program code template that allows developers to create an object that has both the data and the functions/methods. This implementation is used to hide the internal representation or state of an object from the outside. The idea is to hide the attribute of the object from the outside and bundle it with methods that provide read or write access to it, this way we can hide specific information and control access to the internal state of the object. In java, we use getters and setter methods. The getter methods retrieve an attribute and the setter method changes it. Class is a blueprint or a set of instructions to build a specific type of object. It revolves around real-life entities or objects, like a car can be a class and have data like the color and manufacture date and methods like returning the mileage based on the car\u2019s life. A class is a logical entity and in Java, it is declared using the class keyboard for example class Student{}. An object is an instance of a class, it is a real-world entity such as a laptop, mobile, etc, it is a physical entity. An object in Java is mainly created through the new keyword, for example, Student s1 = new Student(), here we define an object with the name s1 and it is of the class Student. A class is declared once since it is the blueprint, but we can have as many objects of that class, every time we create a new object, we allocate memory to it. Instantiation is the creation of an object which is an instance of a given class. When we instantiate an object, we define its attributes which are requested by the class, this allocates memory for this object. The class hierarchy is also called the inheritance tree. Due to inheritance a child class can extend or inherit from its parent class, this can form a hierarchy. A child class can inherit from one parent class or multiple parent classes, a parent class can have multiple child classes. This hierarchy can be as deep as needed and can branch out in any direction. Method overloading is a form of polymorphism. Overloading happens when we have two methods of the same name but different arguments. These methods are differentiated based on the number and type of parameters passed as arguments. An example could be, we have two methods of the same name sum, the first method is defined as having 2 integer inputs and the second method is defined as having 3 floating-point inputs. Now if we call this sum(1,2), then the program will call the first method, but if we call sum(1.0,2.5,4.5), then we will call the second method. The advantage of method overloading is that it increases code readability and maintainability. The appropriate method is called for which the arguments are provided during runtime. The scope of an entity such as a variable defines in what part of a computer program that name can be used to refer to the entity. In other parts of the program, the same name may refer to a different entity or it might not exist at all. We have three types of scope in an OOP language like Java: Public attributes and functions are available to every type of object that attempts to access them. Protected attributes and functions can be accessed within their package, or by a subclass of its class in another package. Private attributes and functions can only be accessed by their class. Information hiding is about protecting data or information from changing them by mistake throughout the program. It protects data from modification by other parts of the program. This hidden data can be accessed indirectly using safe mechanisms or methods. Private, protected and the public are the accessibility levels. An abstract class is declared with the abstract keyword. It can have abstract and nonabstract methods. This class cannot be instantiated, but it can be subclassed. When an abstract class is subclassed, the subclass provides implementations for all the abstract methods in the parent class. An abstract method is declared without an implementation. An interface is just the declaration of methods of an object, it is not the implementation. In an interface, we define the kind of operations an object can perform. These operations are defined by the class that implements the interface. A unified modeling language UML class diagram is a structure diagram that describes the structure of a system by showing the classes, their attributes, their methods, and the relationships among objects. It is like a relational schema for databases. The classes are represented as rectangles having two parts, the attributes or the characteristics of that object, and the methods or the functions of that object. Each attribute will have its datatype defined, like string or integer. The methods will have open and close parenthesis to show that they are the methods. For all attributes and methods, we show the visibility or accessibility with different symbols: - sign means this attribute or method is private and can only be accessed by its class, + means this attribute or method is public and can be accessed by anything in the source code, # means this attribute or method is protected and can be accessed by its class or any subclasses, and ~ means this attribute or method is packaged and can be accessed by anything in its package. Attributes are generally private, and the methods are public or protected. Once we have defined separate classes, we create the relationships between them. Inheritance is represented by an arrow, using inheritance we can define multiple child classes which are like the parent class but require some changes, for example, an animal abstract class can be used to create a lion, zebra, and a gorilla without having to repeat the same attributes and methods. An abstract class cannot be instantiated on its own, but it is instantiated through a subclass, these are written with italics in the UML diagram. Another type of relationship is association shown with a straight line, this is a simple relationship without any dependency between two classes, for example, the class cow eats class grass. The next type of relationship is aggregation, which is shown with a line and an empty diamond, in this relationship a class can be a part of the other class, but it does not have to be, there are no strict rules. The last type of relationship is composition, this is shown as a straight line with a filled-in diamond. In this relationship, the child object will not exist without the parent object, for example, a house class and a bedroom class. The bedroom will not exist if the house class is destroyed. This way we can model an application by representing its classes and the relationships between them.","title":"Part 1"},{"location":"Question_15/#part-2","text":"SNMP or simple network management protocol is a way for different devices on a network to share information. It allows devices to communicate even if the devices are different hardware and run different software. It is a vital tool for effective network management. SNMP is a simple architecture based on a client-server model. The servers are called managers, they collect and process information about devices on the network. The clients are called agents, these are any device connected to the network. They can include computers, network switches, phones, printers, and so on. To provide flexibility, SNMP doesn\u2019t require network devices to exchange data in a rigid format of fixed size. Instead, it uses a tree-like format, under which data is always available for managers to collect, thus we use the UDP protocol and SNMP works on the UDP port number 161 by default. The data tree consists of multiple tables or branches which are called management information bases or MIBs. MIBs group together particular types of devices, each MIB has a unique identifying number and an identifying string. These numbers and strings can be used interchangeably. Each MIB consists of one or more nodes, which represent individual devices on the network and each node has a unique object identifier or OID. The OID for a given node is determined by the identifier of the MIB on which it exists combined with the node\u2019s identifier within its MIB. Using OID a manager can query an agent to find information about a device on the network, for example, if the manager wants to know whether an interface is up, it will first query the interface MIB, then check the OID value which tells the operational status to determine if the interface is up. When the admin asks for the status of a device, it's called polling, and when a device replies, it\u2019s called a trap or a notification. RMON or remote monitoring is an extension to SNMP. RMON is a method of monitoring network traffic on a remote ethernet to find network issues. These issues can be dropped packets, network collisions, or traffic congestion. RMON was developed to address the weak points of a standard MIB, which could not provide statistics on the data link and physical layer parameters. With RMON, a network admin can set up performance thresholds that create alerts when these thresholds are crossed, this allows us to maintain a proactive network management strategy. RMON probes are hardware or software elements of a network device or a software embedded device like a router or a switch. The RMON probe is set on the device on a TCP/IP subnet, this software runs on the network device and gets the information about the network and traffic activity. This information can be passed back to the SNMP manager console for analysis and reporting. The differences between SNMP and RMON are, SNMP is a device-oriented protocol and focuses on how devices are functioning over the network while RMON is more network-oriented. While SNMP is used for monitoring devices over the network, RMON monitors the network itself and provides information to the network administrator which can be used to troubleshoot and make changes according to the needs of the network. Another difference between SNMP and RMON is that SNMP runs on preconfigured software to ensure that the network runs optimal, but RMON gives the ability to the network administrator to use the information and make the network run at peak performance. RMON is usually used to do an in-depth analysis of the network to a point that it can be used to find out how many bytes and packets were transferred and if any were dropped to bandwidth and internet access. An SNMP device, such as a router usually needs additional software installed on it to provide RMON functionality to turn it into a probe. SNMP is always the first line of defense when retrieving information about the network and what is happening in the network, RMON is an extension and makes it easier and gives us more information.","title":"Part 2"},{"location":"Question_2/","text":"2. PART 1 Synthesis of continuous time control systems. The gain and phase margin. Linear systems and their description in time- and frequency domains. Signal transfer in control systems. PART 2 Explain the data elements of TCP and UDP transport layer protocols, and the differences between their mechanisms. Part 1 A control system is a system that provides the desired response by controlling the output. The input is varied by some mechanism. The traffic lights control system is an example of a control system. A sequence of input signals is applied, and the output is one of the three lights that stay on for some duration of time. Based on the traffic data at a particular junction, the on and off times of the lights can be determined. The input signal controls the output, and it operates on a time basis. Control systems can be classified on the type of signal. A system that deals with continuous-time signals is called a continuous-time system; its opposite is the discrete-time system which uses discrete-time signals. Continuous-time systems view variables as having a particular value for potentially only an infinitesimally short amount of time. The variable time ranges over the entire real number line. Its domain is a continuum, that is the function's domain is an uncountable set. Gain and Phase margin can be derived from a Bode plot. A bode plot is a graph that is used to determine the stability of a control system. It maps the frequency response of the system through two graphs \u2013 the bode magnitude plot (expressing the magnitude in decibels) and the bode phase plot (expressing the phase shift in degrees). The Gain Margin (GM) is the amount of gain which can be increased or decreased without making the system unstable. It is expressed as a magnitude in decibels. The greater the gain margin, the greater the stability of a system. GM can be read directly from a bode plot, this is done by calculating the vertical distance between the magnitude curve and the x-axis at the frequency, this point is known as the phase crossover frequency. The gain margin is the negative of gain. If the gain is 20 then the gain margin is -20 decibels. Phase Margin (PM) is the amount of phase which can be increased or decreased without making the system unstable. It is expressed as the phase in degrees. The greater the phase margin, the greater the stability of the system. We can read the phase margin directly from a bode plot. This is done by calculating the vertical distance between the phase curve and the x-axis at the frequency. This point is known as the gain crossover frequency. Phase margin is the phase lag plus 180 degrees. If the phase lag is -189 degrees, then the phase margin is -9 degrees. Linear systems are systems whose outputs for a linear combination of inputs are the same as a linear combination of individual responses to those inputs. Linear systems can be time-invariant which are linear systems where the output does not depend on when input was applied. So, if we apply an input to a system now or T seconds from now, the output will be identical except for a time delay of T seconds. Linear systems in frequency domains refer to the analysis of these systems with respect to frequency. A frequency-domain graph shows how much of the signal lies within each given frequency band over a range of frequencies. To change a linear system from a time domain to a frequency domain, we apply a Laplace transform function of the impulse response. In the frequency domain, the output is the product of the transfer function with the transformed input. Transfer function represents the relationship between the output signal of a control system and the input signal for all possible input values. For any control system, there is a reference input known as excitation which operates through a transfer function to produce a controlled output. The transfer function of a control system is defined as the ratio of the Laplace transform of the output variable to the Laplace transform of the input variable assuming all initial conditions to be zero. It is not necessary that the output and input of a control system are of the same category. For example, in electric motors, the input is an electrical signal whereas the output is a mechanical signal since electrical energy is required to rotate the motors. Similarly in an electric generator, the input is a mechanical signal, and the output is an electrical signal since mechanical energy is required to produce electricity in a generator. But for mathematical analysis of a system, all kinds of signals should be represented in a similar form. This is done by transforming all kinds of signals to their Laplace form. Transfer functions can be obtained using a Block diagram method or a signal flow graph. The Block diagram method is when the transfer function of each element of a control system is represented by a block diagram. A modified form of a block diagram is a signal flow graph which further shortens the representation of a control system. Poles of a transfer function are defined as those values in the ratio whose substitution in the denominator makes the transfer function infinite. Zeros of transfer function have to do with the numerator. Those values of the ratio that when substituted make the transfer function zero is the zeros of the transfer function. The steps to calculate the transfer function are as follows: The time-domain equations of the system must be written after considering different required variables in the system. Consider the initial conditions as 0, write the Laplace transform of the time domain equations of the system. Determine the input as well as the output variables from the frequency domain equations. Remove the initially considered variables and write the resultant equations in the form of input and output variables. The ratio of Laplace transform of output and input must be determined to have the transfer function of the overall system. Part 2 TCP stands for Transmission Control Protocol, which is a communications standard that enables applications, programs, and computing devices to exchange messages over a network. It is designed to send packets across the internet and ensure the successful delivery of data and messages over networks. TCP guarantees the integrity of the data being communicated over a network. High-level protocols like File Transfer Protocol (FTP), Secure Shell (SSH), Telnet, Internet Message Access Protocol (IMAP), and Simple Mail Transfer Protocol (SMTP) all use TCP. TCP is an expensive protocol since a connection is established between the server and the client. Three-way handshake and error detection add to reliability but increase latency. TCP accepts data from a data stream, divides it into chunks, and adds a TCP header creating a TCP segment. The TCP segment is then encapsulated into an Internet Protocol datagram and exchanged with peers. A TCP segment consists of a segment header and a data section. The segment header contains 10 mandatory fields and an optional extension field. The data sections follow the header and are the actual data carried for the application. The sections are: Source port (16 bits) \u2013 The client\u2019s port number. Destination port (16 bits) \u2013 The server\u2019s port number. Sequence number (32 bits) \u2013 A sequence number used for guaranteeing packet order. Acknowledgment Number (32 bits) \u2013 An acknowledge number notifying senders of the receipt of TCP segments. Data offset (4 bits) \u2013 The size of the TCP header. Reserved (3 bits) \u2013 Set to zero, reserved for future use. Flags (9 bits) \u2013 Flags set TCP control options used to alter the connection. Window (16 bits) \u2013 The receive window. The number of bytes that the sender of this segment is willing to receive. Checksum (16 bits) \u2013 A 16-bit checksum used for error-checking. Urgent pointer (16 bits) \u2013 If the sender sets the URG flag, then this 16-bit field is an offset from the sequence number indicating the last urgent data byte. Options (0-320 bits [divisible by 32]) Padding (0-320 bits [divisible by 32]) - Zeros, ensures that the TCP header ends, and data begins on a 32-bit boundary. Data (variable) \u2013 The payload data for this TCP segment. TCP uses a three-way handshake to establish the connection. First, the client will generate a random number and set it as the sequence number and set the SYN or synchronization bit on, and send this to the server. The server will send a message back to the client, the SYN bit will be on and the ACK or acknowledge bit will be set to the sequence number the client sent plus one. The server will generate its sequence number and send it as well. In the last step, the client sends a message to the server, this time the SYN bit is 0, the acknowledgment number is the sequence number of the server plus one and the sequence number is the acknowledgment number sent by the server in the last step which is the sequence number of the client in the first step plus one. After the handshake is complete, a client can start sending data packets immediately. TCP segments are exchanged between the client and the server. To guard against the unreliable network, TCP uses sequence numbers to verify the correct delivery and ordering of TCP segments. The sequence number is included on each transmitted packet and acknowledged by the opposite host as an acknowledgment number to inform the sending host that the transmitted data was received successfully. Flow control can be managed by the window field, if a sender is under heavy load, then it sets the window to a low value to decrease pressure and vice versa. UDP is User Datagram Protocol is a communications protocol that is primarily used to establish low-latency and loss tolerating connections between applications on the internet. UDP speeds up transmissions by enabling the transfer of data before an agreement is provided by the receiver. As a result, UDP is good for time-sensitive communication like voice over IP (VoIP) and domain name system DNS lookup. UDP sends messages as datagrams and doesn\u2019t provide any guarantee that the data will be delivered or checked for dropped data. UDP allows for packets to be dropped and received in a different order than they were transmitted, making it have low latency. It can be used where many clients are connected, and real-time error correction isn\u2019t necessary such as gaming. UDP header has 4 fields, each of 2 bytes: Source port number \u2013 the port number of the sender. Destination port number \u2013 the destination port number. Length \u2013 the length in bytes of the UDP header and any encapsulated data. Checksum \u2013 used for error checking, it is required in IPv6 and optional in IPv4. Differences between UDP and TCP: UDP is a connectionless protocol whereas TCP is a connection-oriented protocol. UDP is used for VoIP, gaming, live broadcasts whereas TCP is used for most data protocols on the internet. UDP is faster and uses fewer resources whereas TCP has higher latency and is resource-intensive. The packets in UDP don\u2019t necessarily arrive in order, whereas the TCP ensures packet order so they can be stitched back together. UDP allows for missing packets, the sender is unable to know whether a packet was successfully received or not, whereas TCP guarantees no missing packets, and all sent data makes it to the intended recipient. There is basic error checking mechanisms in UDP whereas there is extensive error checking and acknowledgment of data. UDP supports broadcasting whereas TCP does not. UDP is better suited for time-sensitive applications whereas TCP is better suited for applications that need data reliability and are not as time sensitive. UDP has a smaller header than TCP.","title":"Question 2"},{"location":"Question_2/#2-part-1-synthesis-of-continuous-time-control-systems-the-gain-and-phase-margin-linear-systems-and-their-description-in-time-and-frequency-domains-signal-transfer-in-control-systemspart-2-explain-the-data-elements-of-tcp-and-udp-transport-layer-protocols-and-the-differences-between-their-mechanisms","text":"","title":"2. PART 1 Synthesis of continuous time control systems. The gain and phase margin. Linear systems and their description in time- and frequency domains. Signal transfer in control systems.PART 2 Explain the data elements of TCP and UDP transport layer protocols, and the differences between their mechanisms."},{"location":"Question_2/#part-1","text":"A control system is a system that provides the desired response by controlling the output. The input is varied by some mechanism. The traffic lights control system is an example of a control system. A sequence of input signals is applied, and the output is one of the three lights that stay on for some duration of time. Based on the traffic data at a particular junction, the on and off times of the lights can be determined. The input signal controls the output, and it operates on a time basis. Control systems can be classified on the type of signal. A system that deals with continuous-time signals is called a continuous-time system; its opposite is the discrete-time system which uses discrete-time signals. Continuous-time systems view variables as having a particular value for potentially only an infinitesimally short amount of time. The variable time ranges over the entire real number line. Its domain is a continuum, that is the function's domain is an uncountable set. Gain and Phase margin can be derived from a Bode plot. A bode plot is a graph that is used to determine the stability of a control system. It maps the frequency response of the system through two graphs \u2013 the bode magnitude plot (expressing the magnitude in decibels) and the bode phase plot (expressing the phase shift in degrees). The Gain Margin (GM) is the amount of gain which can be increased or decreased without making the system unstable. It is expressed as a magnitude in decibels. The greater the gain margin, the greater the stability of a system. GM can be read directly from a bode plot, this is done by calculating the vertical distance between the magnitude curve and the x-axis at the frequency, this point is known as the phase crossover frequency. The gain margin is the negative of gain. If the gain is 20 then the gain margin is -20 decibels. Phase Margin (PM) is the amount of phase which can be increased or decreased without making the system unstable. It is expressed as the phase in degrees. The greater the phase margin, the greater the stability of the system. We can read the phase margin directly from a bode plot. This is done by calculating the vertical distance between the phase curve and the x-axis at the frequency. This point is known as the gain crossover frequency. Phase margin is the phase lag plus 180 degrees. If the phase lag is -189 degrees, then the phase margin is -9 degrees. Linear systems are systems whose outputs for a linear combination of inputs are the same as a linear combination of individual responses to those inputs. Linear systems can be time-invariant which are linear systems where the output does not depend on when input was applied. So, if we apply an input to a system now or T seconds from now, the output will be identical except for a time delay of T seconds. Linear systems in frequency domains refer to the analysis of these systems with respect to frequency. A frequency-domain graph shows how much of the signal lies within each given frequency band over a range of frequencies. To change a linear system from a time domain to a frequency domain, we apply a Laplace transform function of the impulse response. In the frequency domain, the output is the product of the transfer function with the transformed input. Transfer function represents the relationship between the output signal of a control system and the input signal for all possible input values. For any control system, there is a reference input known as excitation which operates through a transfer function to produce a controlled output. The transfer function of a control system is defined as the ratio of the Laplace transform of the output variable to the Laplace transform of the input variable assuming all initial conditions to be zero. It is not necessary that the output and input of a control system are of the same category. For example, in electric motors, the input is an electrical signal whereas the output is a mechanical signal since electrical energy is required to rotate the motors. Similarly in an electric generator, the input is a mechanical signal, and the output is an electrical signal since mechanical energy is required to produce electricity in a generator. But for mathematical analysis of a system, all kinds of signals should be represented in a similar form. This is done by transforming all kinds of signals to their Laplace form. Transfer functions can be obtained using a Block diagram method or a signal flow graph. The Block diagram method is when the transfer function of each element of a control system is represented by a block diagram. A modified form of a block diagram is a signal flow graph which further shortens the representation of a control system. Poles of a transfer function are defined as those values in the ratio whose substitution in the denominator makes the transfer function infinite. Zeros of transfer function have to do with the numerator. Those values of the ratio that when substituted make the transfer function zero is the zeros of the transfer function. The steps to calculate the transfer function are as follows: The time-domain equations of the system must be written after considering different required variables in the system. Consider the initial conditions as 0, write the Laplace transform of the time domain equations of the system. Determine the input as well as the output variables from the frequency domain equations. Remove the initially considered variables and write the resultant equations in the form of input and output variables. The ratio of Laplace transform of output and input must be determined to have the transfer function of the overall system.","title":"Part 1"},{"location":"Question_2/#part-2","text":"TCP stands for Transmission Control Protocol, which is a communications standard that enables applications, programs, and computing devices to exchange messages over a network. It is designed to send packets across the internet and ensure the successful delivery of data and messages over networks. TCP guarantees the integrity of the data being communicated over a network. High-level protocols like File Transfer Protocol (FTP), Secure Shell (SSH), Telnet, Internet Message Access Protocol (IMAP), and Simple Mail Transfer Protocol (SMTP) all use TCP. TCP is an expensive protocol since a connection is established between the server and the client. Three-way handshake and error detection add to reliability but increase latency. TCP accepts data from a data stream, divides it into chunks, and adds a TCP header creating a TCP segment. The TCP segment is then encapsulated into an Internet Protocol datagram and exchanged with peers. A TCP segment consists of a segment header and a data section. The segment header contains 10 mandatory fields and an optional extension field. The data sections follow the header and are the actual data carried for the application. The sections are: Source port (16 bits) \u2013 The client\u2019s port number. Destination port (16 bits) \u2013 The server\u2019s port number. Sequence number (32 bits) \u2013 A sequence number used for guaranteeing packet order. Acknowledgment Number (32 bits) \u2013 An acknowledge number notifying senders of the receipt of TCP segments. Data offset (4 bits) \u2013 The size of the TCP header. Reserved (3 bits) \u2013 Set to zero, reserved for future use. Flags (9 bits) \u2013 Flags set TCP control options used to alter the connection. Window (16 bits) \u2013 The receive window. The number of bytes that the sender of this segment is willing to receive. Checksum (16 bits) \u2013 A 16-bit checksum used for error-checking. Urgent pointer (16 bits) \u2013 If the sender sets the URG flag, then this 16-bit field is an offset from the sequence number indicating the last urgent data byte. Options (0-320 bits [divisible by 32]) Padding (0-320 bits [divisible by 32]) - Zeros, ensures that the TCP header ends, and data begins on a 32-bit boundary. Data (variable) \u2013 The payload data for this TCP segment. TCP uses a three-way handshake to establish the connection. First, the client will generate a random number and set it as the sequence number and set the SYN or synchronization bit on, and send this to the server. The server will send a message back to the client, the SYN bit will be on and the ACK or acknowledge bit will be set to the sequence number the client sent plus one. The server will generate its sequence number and send it as well. In the last step, the client sends a message to the server, this time the SYN bit is 0, the acknowledgment number is the sequence number of the server plus one and the sequence number is the acknowledgment number sent by the server in the last step which is the sequence number of the client in the first step plus one. After the handshake is complete, a client can start sending data packets immediately. TCP segments are exchanged between the client and the server. To guard against the unreliable network, TCP uses sequence numbers to verify the correct delivery and ordering of TCP segments. The sequence number is included on each transmitted packet and acknowledged by the opposite host as an acknowledgment number to inform the sending host that the transmitted data was received successfully. Flow control can be managed by the window field, if a sender is under heavy load, then it sets the window to a low value to decrease pressure and vice versa. UDP is User Datagram Protocol is a communications protocol that is primarily used to establish low-latency and loss tolerating connections between applications on the internet. UDP speeds up transmissions by enabling the transfer of data before an agreement is provided by the receiver. As a result, UDP is good for time-sensitive communication like voice over IP (VoIP) and domain name system DNS lookup. UDP sends messages as datagrams and doesn\u2019t provide any guarantee that the data will be delivered or checked for dropped data. UDP allows for packets to be dropped and received in a different order than they were transmitted, making it have low latency. It can be used where many clients are connected, and real-time error correction isn\u2019t necessary such as gaming. UDP header has 4 fields, each of 2 bytes: Source port number \u2013 the port number of the sender. Destination port number \u2013 the destination port number. Length \u2013 the length in bytes of the UDP header and any encapsulated data. Checksum \u2013 used for error checking, it is required in IPv6 and optional in IPv4. Differences between UDP and TCP: UDP is a connectionless protocol whereas TCP is a connection-oriented protocol. UDP is used for VoIP, gaming, live broadcasts whereas TCP is used for most data protocols on the internet. UDP is faster and uses fewer resources whereas TCP has higher latency and is resource-intensive. The packets in UDP don\u2019t necessarily arrive in order, whereas the TCP ensures packet order so they can be stitched back together. UDP allows for missing packets, the sender is unable to know whether a packet was successfully received or not, whereas TCP guarantees no missing packets, and all sent data makes it to the intended recipient. There is basic error checking mechanisms in UDP whereas there is extensive error checking and acknowledgment of data. UDP supports broadcasting whereas TCP does not. UDP is better suited for time-sensitive applications whereas TCP is better suited for applications that need data reliability and are not as time sensitive. UDP has a smaller header than TCP.","title":"Part 2"},{"location":"Question_3/","text":"3. PART 1 Combinational logic design. Multiplexers/Demultiplexers. Encoders/Decoders. Comparators. Parity generators/checkers. Arithmetical logical units. PART 2 Present the general problem solving methods and compare them with the methods for solving constraint satisfaction problems. Part 1 Combinational Logic is a type of digital logic that is implemented by Boolean circuits, where the output is a pure function of the present input only. This contrasts with sequential logic, in which the output depends not only on the present input but also on the history of the input, thus sequential logic has memory while combinational logic does not. Combinational logic is used in computer circuits to perform Boolean algebra on input signals and stored data. Computers contain lots of elements that are made using combinational logic like multiplexers, demultiplexers, encoders, decoders, comparators, arithmetic logic units, etc. These circuits are made up of basic NAND, NOR, or NOT gates that are combined or connected to produce more complicated circuits. We have three main ways of specifying the function of a combinational logic circuit: Boolean Algebra \u2013 This is the algebraic expression showing the operation of the logic circuit for each input variable either True or False. Truth Table \u2013 A truth table defines all outputs of a logic gate for given input combinations. Logic Diagram \u2013 This is a graphical representation of the logic circuit that shows the wiring and connections of each logic gate. Multiplexers or MUX are combinational logic circuits that take in several analogs or digital input signals and output only one. The selection is managed by a separate set of digital inputs known as select lines. It can have a maximum of 2n data inputs, n selection lines, and a single output line. One of these data inputs is connected to the output line. In a 4x1 multiplexer, we have 4 data inputs, 2 selection lines, and 1 output. We can construct a truth table of the selection line values, we will have 4 such combinations for both values of each selection line, these 4 combinations will each refer to one of the 4 data inputs. MUX is used in communication systems and computer memory. Demultiplexers or demux are combinational logic circuits that can distribute multiple outputs from a single input. A demux has one input, n number of output lines, m number of control lines and it should conform to n = 2m. In a 1 to 4 demux, we have 1 input, 4 outputs, and 2 control bits. As we have 4 different Boolean combinations for the control bits, these will dictate which of the 4 outputs is used. Demux is used in the arithmetic logic unit and serial to parallel converters. Encoders are combinational logic circuits that convert binary information in the form of 2n input lines into n output lines. For example, we have octal to binary encoder. This takes in 8 input lines and generates 3 output lines, where n = 3. We have a special encoder called the priority encoder which compresses the multiple binary input into a small number of outputs. Encoders are used to translate decimal values to binary for operations like addition, subtraction. Priority encoders are used to detect interrupts in microprocessors applications. Decoders are combinational logic circuits that do the opposite job of encoders. It converts n lines of input into 2n lines of output. For example, we have a 3 to 8 decoder where n = 3. Decoders are used in code conversions, in high-performance memory systems, and for data distribution. Digital comparators are electronic devices that take two numbers as input in binary form and determine whether one number is greater than, less than, or equal to the other number. Comparators are used in microprocessors and microcontrollers. These are made up of AND, NOR, and NOT gates. A 1-bit digital comparator is the simplest digital comparator, it can compare 2 inputs of 1 bit each. We can formulate a truth table that compares these 2 bits. Parity generators are used for error detection during data transmission. When data is transmitted, there may be noise that can change a 1 to 0 or a 0 to a 1. The parity bit is added to the data to make the 1s either even or odd. At the receiving end, the number of 1s is counted and if it doesn\u2019t match with the transmitted data, then it means there is an error in the data. A parity generator is a combinational logic circuit that generates the parity bit in the transmitter. On the receiving end, we have a parity checker which checks the parity. A combination of parity generators and parity checkers is used in digital systems to detect single-bit errors in transmitted data. In even parity, the parity bit will make the total number of 1s an even number, and for odd parity, it will do the same, but the sum will be odd. These errors can be detected and corrected using exclusive OR gates. In a combinational circuit that accepts n-1 bit data, it generates a parity bit which is added to the bitstream. In an even parity bit scheme, the parity bit is 0, if there are an even number of 1s and 1 if there are odd number of 1s. For example: if we want to transmit a 3-bit message with an even parity bit, we have 4 bits in total, A, B, C, and P is the parity bit. We can formulate a truth table with 8 different combinations of A, B, C, for half of the P will be 0 and for the other half, it will be 1. Arithmetic Logic Unit or ALU is a combinational logic circuit that performs arithmetic and bitwise operations on integer binary numbers. This contrasts with the floating-point unit which operates on floating-point numbers or decimals. The inputs to an ALU are the data to be operated on called the operands and a code indicating the operation to be performed. The output is the result of the performed operation on the operands. Many ALU designs, also have status inputs or outputs which convey information about a previous operation or the current operation between ALU and an external status register. Basic ALU has three parallel data buses consisting of two input operands and result output. The opcode is a parallel bus that conveys the ALU an operation selection code. Part 2 Problem-solving is the method to reach the desired goal or find a solution to a given situation. In computer science, problem-solving refers to artificial intelligence techniques using efficient algorithms, heuristics, etc. to find solutions. Some general problem-solving methods are: Heuristics \u2013 This type of method understands the problem and finds a solution based on experimental and trial and error methods. However, these heuristics do not often find the best optimal solution to a specific problem, instead, these offer quick and efficient solutions to attain immediate goals. Since this method is fast and efficient but has lower inaccuracy, it can be combined with optimization algorithms to increase the accuracy of this method. An example of such a problem is the traveling salesman problem where we have a list of cities and their distances, the user must find the optimal route for the salesman to visit every city and return to the starting city. Using a greedy algorithm, we pick the next best step on every current city. Searching Algorithms \u2013 Searching is a very common method of solving a problem. Rational agents use these searching algorithms to find optimal solutions. These agents are goal-based and use atomic representation. These algorithms base the quality of the solution by keeping in mind optimality, time complexity, and space complexity. There are two main types of search algorithms: a. Informed Search \u2013 These algorithms use basic domain knowledge and understand the available information. They use this as a guideline for optimal solutions. These are more efficient solutions than uninformed searches. Some examples are Greedy search and A* search. A greedy algorithm for example finds the optimal choice at each stage, intending to find the global optimal eventually, if we are at a node with 3 possible paths, this algorithm will choose the node with the lowest cost without caring about the overall system. An example is Dijkstra\u2019s algorithm which is a pathfinding algorithm. b. Uninformed Search \u2013 These algorithms do not have the basic domain knowledge. It contains information regarding traversing a tree and identifying lead and goal nodes. The search goes through every node till reaching the desired destination. Some examples are Breadth-first search, Depth-first search, Uniform cost search, etc. In depth-first search, we traverse a given node till its deepest node, then we go back and on to the next node, we do not go to the next node until we have traversed all child nodes of the node we are on. In breadth-first search, we traverse all nodes on a given level before moving to a deeper level. Constraint Satisfaction Problems or CSP can be used to solve a variety of problems more efficiently. A CSP consists of three components, which are variables that are denoted with a V, domains of those variables which are denoted with a D, and constraints that are denoted with a C. In general, problems we try to represent them as states, we move ahead to different states and try to solve those problems, in a CSP we use these 3 components, V, D, and C. V is the set of variables, this is a finite set, for example, we might have variables like V1, V2, all the way till Vn. D is the set of domains; these are also finite and there is one domain for each variable. An example of a set of domains can be D1, D2, all the way till Dn. The domain of a variable is all set of values that the variable can take, this can be a real number, imaginary number, a color, or anything. C is the set of constraints that specify the allowable combination of values. C is represented as Ci which is a tuple of two elements, scope, and relation. Ci = (scope, relation). The scope is the variables we are currently dealing with, and the relationship defines the relationship between the variables in the scope. For this example, let us assume the score is binary, which means the scope contains two variables, let\u2019s say V1 and V2. Now the relationship can be any relationship between these two variables in the scope, let\u2019s say V1 cannot be equal to V2. This relationship is our constraints applied to these two values, so in this example, V1 and V2 can take values from their domain where they do not equal each other, if the domain is a set of all integers from and including 1 to 3, then one valid case could be that V1=1 and V2=2, and an invalid case could be that V1=1 and V2=1. In a constraints satisfaction problem, we have multiple such relationships between many combinations of variables. A CSP is considered solved when all variables have a value assigned to them from the domain with no conflicts of constraints. An example of a CSP is sudoku. In sudoku we have a grid of 9x9, thus we have 81 different variables, we have some pre-filled values, for example, if we have sudoku with 11 filled values, then the set of variables contains 70 elements for each infilled box, the set of domain will be all values each box can take, which is a set consisting of the numbers 1,2,3,4,5,6,7,8,9. Finally, the constraint is that no element can be rotated in its row and column. Using a CSP algorithm like the Arc Consistency algorithm #3 or AC3, we can solve this problem. Another example of a CSP is the fa Australia map coloring problem, where we have to color the 7 regions of Australia, thus the variables are the set of all 7 regions, we can only choose from 3 colors, red, green, and blue, thus the set of the domain contains these 3 colors. Finally, the constraint is that no two neighboring regions can have the same color.","title":"Question 3"},{"location":"Question_3/#3-part-1-combinational-logic-design-multiplexersdemultiplexers-encodersdecoders-comparators-parity-generatorscheckers-arithmetical-logical-units-part-2-present-the-general-problem-solving-methods-and-compare-them-with-the-methods-for-solving-constraint-satisfaction-problems","text":"","title":"3. PART 1 Combinational logic design. Multiplexers/Demultiplexers. Encoders/Decoders. Comparators. Parity generators/checkers. Arithmetical logical units. PART 2 Present the general problem solving methods and compare them with the methods for solving constraint satisfaction problems."},{"location":"Question_3/#part-1","text":"Combinational Logic is a type of digital logic that is implemented by Boolean circuits, where the output is a pure function of the present input only. This contrasts with sequential logic, in which the output depends not only on the present input but also on the history of the input, thus sequential logic has memory while combinational logic does not. Combinational logic is used in computer circuits to perform Boolean algebra on input signals and stored data. Computers contain lots of elements that are made using combinational logic like multiplexers, demultiplexers, encoders, decoders, comparators, arithmetic logic units, etc. These circuits are made up of basic NAND, NOR, or NOT gates that are combined or connected to produce more complicated circuits. We have three main ways of specifying the function of a combinational logic circuit: Boolean Algebra \u2013 This is the algebraic expression showing the operation of the logic circuit for each input variable either True or False. Truth Table \u2013 A truth table defines all outputs of a logic gate for given input combinations. Logic Diagram \u2013 This is a graphical representation of the logic circuit that shows the wiring and connections of each logic gate. Multiplexers or MUX are combinational logic circuits that take in several analogs or digital input signals and output only one. The selection is managed by a separate set of digital inputs known as select lines. It can have a maximum of 2n data inputs, n selection lines, and a single output line. One of these data inputs is connected to the output line. In a 4x1 multiplexer, we have 4 data inputs, 2 selection lines, and 1 output. We can construct a truth table of the selection line values, we will have 4 such combinations for both values of each selection line, these 4 combinations will each refer to one of the 4 data inputs. MUX is used in communication systems and computer memory. Demultiplexers or demux are combinational logic circuits that can distribute multiple outputs from a single input. A demux has one input, n number of output lines, m number of control lines and it should conform to n = 2m. In a 1 to 4 demux, we have 1 input, 4 outputs, and 2 control bits. As we have 4 different Boolean combinations for the control bits, these will dictate which of the 4 outputs is used. Demux is used in the arithmetic logic unit and serial to parallel converters. Encoders are combinational logic circuits that convert binary information in the form of 2n input lines into n output lines. For example, we have octal to binary encoder. This takes in 8 input lines and generates 3 output lines, where n = 3. We have a special encoder called the priority encoder which compresses the multiple binary input into a small number of outputs. Encoders are used to translate decimal values to binary for operations like addition, subtraction. Priority encoders are used to detect interrupts in microprocessors applications. Decoders are combinational logic circuits that do the opposite job of encoders. It converts n lines of input into 2n lines of output. For example, we have a 3 to 8 decoder where n = 3. Decoders are used in code conversions, in high-performance memory systems, and for data distribution. Digital comparators are electronic devices that take two numbers as input in binary form and determine whether one number is greater than, less than, or equal to the other number. Comparators are used in microprocessors and microcontrollers. These are made up of AND, NOR, and NOT gates. A 1-bit digital comparator is the simplest digital comparator, it can compare 2 inputs of 1 bit each. We can formulate a truth table that compares these 2 bits. Parity generators are used for error detection during data transmission. When data is transmitted, there may be noise that can change a 1 to 0 or a 0 to a 1. The parity bit is added to the data to make the 1s either even or odd. At the receiving end, the number of 1s is counted and if it doesn\u2019t match with the transmitted data, then it means there is an error in the data. A parity generator is a combinational logic circuit that generates the parity bit in the transmitter. On the receiving end, we have a parity checker which checks the parity. A combination of parity generators and parity checkers is used in digital systems to detect single-bit errors in transmitted data. In even parity, the parity bit will make the total number of 1s an even number, and for odd parity, it will do the same, but the sum will be odd. These errors can be detected and corrected using exclusive OR gates. In a combinational circuit that accepts n-1 bit data, it generates a parity bit which is added to the bitstream. In an even parity bit scheme, the parity bit is 0, if there are an even number of 1s and 1 if there are odd number of 1s. For example: if we want to transmit a 3-bit message with an even parity bit, we have 4 bits in total, A, B, C, and P is the parity bit. We can formulate a truth table with 8 different combinations of A, B, C, for half of the P will be 0 and for the other half, it will be 1. Arithmetic Logic Unit or ALU is a combinational logic circuit that performs arithmetic and bitwise operations on integer binary numbers. This contrasts with the floating-point unit which operates on floating-point numbers or decimals. The inputs to an ALU are the data to be operated on called the operands and a code indicating the operation to be performed. The output is the result of the performed operation on the operands. Many ALU designs, also have status inputs or outputs which convey information about a previous operation or the current operation between ALU and an external status register. Basic ALU has three parallel data buses consisting of two input operands and result output. The opcode is a parallel bus that conveys the ALU an operation selection code.","title":"Part 1"},{"location":"Question_3/#part-2","text":"Problem-solving is the method to reach the desired goal or find a solution to a given situation. In computer science, problem-solving refers to artificial intelligence techniques using efficient algorithms, heuristics, etc. to find solutions. Some general problem-solving methods are: Heuristics \u2013 This type of method understands the problem and finds a solution based on experimental and trial and error methods. However, these heuristics do not often find the best optimal solution to a specific problem, instead, these offer quick and efficient solutions to attain immediate goals. Since this method is fast and efficient but has lower inaccuracy, it can be combined with optimization algorithms to increase the accuracy of this method. An example of such a problem is the traveling salesman problem where we have a list of cities and their distances, the user must find the optimal route for the salesman to visit every city and return to the starting city. Using a greedy algorithm, we pick the next best step on every current city. Searching Algorithms \u2013 Searching is a very common method of solving a problem. Rational agents use these searching algorithms to find optimal solutions. These agents are goal-based and use atomic representation. These algorithms base the quality of the solution by keeping in mind optimality, time complexity, and space complexity. There are two main types of search algorithms: a. Informed Search \u2013 These algorithms use basic domain knowledge and understand the available information. They use this as a guideline for optimal solutions. These are more efficient solutions than uninformed searches. Some examples are Greedy search and A* search. A greedy algorithm for example finds the optimal choice at each stage, intending to find the global optimal eventually, if we are at a node with 3 possible paths, this algorithm will choose the node with the lowest cost without caring about the overall system. An example is Dijkstra\u2019s algorithm which is a pathfinding algorithm. b. Uninformed Search \u2013 These algorithms do not have the basic domain knowledge. It contains information regarding traversing a tree and identifying lead and goal nodes. The search goes through every node till reaching the desired destination. Some examples are Breadth-first search, Depth-first search, Uniform cost search, etc. In depth-first search, we traverse a given node till its deepest node, then we go back and on to the next node, we do not go to the next node until we have traversed all child nodes of the node we are on. In breadth-first search, we traverse all nodes on a given level before moving to a deeper level. Constraint Satisfaction Problems or CSP can be used to solve a variety of problems more efficiently. A CSP consists of three components, which are variables that are denoted with a V, domains of those variables which are denoted with a D, and constraints that are denoted with a C. In general, problems we try to represent them as states, we move ahead to different states and try to solve those problems, in a CSP we use these 3 components, V, D, and C. V is the set of variables, this is a finite set, for example, we might have variables like V1, V2, all the way till Vn. D is the set of domains; these are also finite and there is one domain for each variable. An example of a set of domains can be D1, D2, all the way till Dn. The domain of a variable is all set of values that the variable can take, this can be a real number, imaginary number, a color, or anything. C is the set of constraints that specify the allowable combination of values. C is represented as Ci which is a tuple of two elements, scope, and relation. Ci = (scope, relation). The scope is the variables we are currently dealing with, and the relationship defines the relationship between the variables in the scope. For this example, let us assume the score is binary, which means the scope contains two variables, let\u2019s say V1 and V2. Now the relationship can be any relationship between these two variables in the scope, let\u2019s say V1 cannot be equal to V2. This relationship is our constraints applied to these two values, so in this example, V1 and V2 can take values from their domain where they do not equal each other, if the domain is a set of all integers from and including 1 to 3, then one valid case could be that V1=1 and V2=2, and an invalid case could be that V1=1 and V2=1. In a constraints satisfaction problem, we have multiple such relationships between many combinations of variables. A CSP is considered solved when all variables have a value assigned to them from the domain with no conflicts of constraints. An example of a CSP is sudoku. In sudoku we have a grid of 9x9, thus we have 81 different variables, we have some pre-filled values, for example, if we have sudoku with 11 filled values, then the set of variables contains 70 elements for each infilled box, the set of domain will be all values each box can take, which is a set consisting of the numbers 1,2,3,4,5,6,7,8,9. Finally, the constraint is that no element can be rotated in its row and column. Using a CSP algorithm like the Arc Consistency algorithm #3 or AC3, we can solve this problem. Another example of a CSP is the fa Australia map coloring problem, where we have to color the 7 regions of Australia, thus the variables are the set of all 7 regions, we can only choose from 3 colors, red, green, and blue, thus the set of the domain contains these 3 colors. Finally, the constraint is that no two neighboring regions can have the same color.","title":"Part 2"},{"location":"Question_4/","text":"4. PART 1 The SSH protocol, key generation, configuration of user settings PART 2 The principles of control, feedback control and open loop control. Set point control and reference signal tracking, the role of negative feedback. Requirements for control systems. Part 1 SSH protocol or the secure shell protocol is a method for secure remote login from one computer to another. It has a strong authentication system, it maintains the integrity of the data being sent by running on top of the TCP protocol and always listens to the TCP/IP port number 22 by default, and it is secure since it encrypts the data using encryption algorithms like Advanced Encryption Standard AES or Data Encryption Standard DES. It is more secure than other remote login protocols like telnet and it is better at exchanging data than insecure file sharing protocols like File Transfer Protocol FTP. SSH can be used via the command line to connect and issue commands to the remote computer. SSH first establishes a connection between the client and the server in three steps: Verification of server \u2013 The client initiates an SSH connection with the server, the client will authenticate the public key of the server. Generation of a session key \u2013 Once the server is verified, both client and server will negotiate a session key using a Diffie-Hellman algorithm. The generated key is a shared symmetric key that will be used for the encryption and decryption of the data. Authentication of the client \u2013 The client will send an ID for the public-private key pair. The server will check its authorized_keys file and see if the client's ID exists here. If it is found then the server will generate a random number and encrypt it using the public key, this message is sent to the client. If the client has the correct private key, it can decrypt the message and get the random number. This random number is used with the shared session key to create an MD5 hash. This MD5 hash is sent to the server, the server can use the shared session key and the original random number to calculate its MD5 hash. If the MD5 hash generated by the client matches the one generated by the server, the client has been authenticated and then the connection is completed. Instead of MD5, we can also use SHA-2 or secure hash algorithm 2. SSH is used widely for things like managing routers, server hardware, virtualization platform, remote operating systems, etc. Key generation is the process of generating keys in cryptography. A key is used to encrypt and decrypt whatever data is being encrypted/decrypted. A device or program used to generate keys is called a key generator or keygen. Modern cryptographic systems include symmetric key algorithms like Data Encryption Standard or DES and Advanced Encryption Standard or AES and public key algorithms like RSA. Symmetric key algorithms use a single shared key; keeping data secret requires keeping this key secret. Public key algorithms use a public key and a private key. The public key is made available to anyone by the means of a digital certificate. A sender encrypts the data with the receiver\u2019s public key and only the holder of this private key can then decrypt the data. The SSH protocol uses a combination of the two key algorithms since public key algorithms tend to be much slower than the symmetric key algorithm. One party receives the other\u2019s public key and encrypts a small piece of data, then the remainder of the connection uses a typically faster symmetric key algorithm for encryption. On a Linux machine, we can simply use the ssh-keygen command to generate a public/private authentication key pair. Authentication keys allow a user to connect to a remote system without supplying a password. Keys must be generated for each user separately. Use the -t option to specify the type of keys like RSA or DSA. The user can also specify a passphrase to encrypt the private key. The shh-key command will generate two keys in the hidden directory .ssh in the home directory of the user. These two files are id_rsa and id_rsa.pub. the id_rsa.pub is the public key, this should be placed in the remote systems authorized_keys directory in its .ssh hidden folder on its home directory, this directory must also have root permissions granted. Now whoever has the corresponding private key can login to this remote computer using ssh protocol. A user is an entity in Linux that can manipulate files and perform other operations. Each user is assigned a unique ID called the UID or user identifier. When the system is created ID 0 is assigned to the root user and IDs 1 to 999 are assigned to the system users and thus the IDs of local users begin from 1000. On Linux systems, user settings configuration can be done using the command line. A new user can be added using the adduser command followed by a name that has not already been taken. This command will create a new account with a new UID, the user\u2019s information is stored in the /etc/passwd and /etc/shadow files, and create a home directory for this user. The user can set a password for this account using the passwd followed by the name. A user account can be removed using the deluser command. The user configuration file which is the /etc/passwd file can be accessed using a text editor like nano since the data is stored in plaintext. The usermod command with various arguments can be used to make changes like changing the ID of the user, modifying the group ID of the user, changing the user login name, and changing the home directory of the user. The root user has access to all files on the system whereas the normal users have limited access. Part 2 Control systems manage commands, direct, or regulates the behavior of other devices or systems using control loops. A control loop is the fundamental building block of industrial control systems. Control loops are systems applied by design engineers to maintain process variables PVs at the desired value or setpoint SP. Control loops are important for maintaining the stability of a system and for consistently producing the desired outcome of a process. A basic example of a control loop is a temperature control loop. These work to maintain the temperatures in our homes and offices. The steps are as follows: The process to be controlled is established. In the case of the temperature control loop, this refers to the temperature of a substance that is being heated. Sensors measure the process value PV. In the case of the temperature control loop, the current value of the temperature, the sensor is usually a thermostat in this example. Sensors feed the measured value of the process value to the controller. In this example the temperature controller. This initiates the control process to achieve the set point or desired temperature in this example. The final control element receives the manipulated values from the controller. In this example, the temperature is increased or decreased. The core components of a control loop are: Sensors and transducers \u2013 Sensors are the initial measurement devices in a control loop. They convert the process variable PV into corresponding analog or digital signals which are read by the controllers. Transducers are advanced sensors that further transform the given values through signal conditioning. Controllers \u2013 The controller is the device that interprets the measurements fed by the sensor and determines the control action to take based on a comparison of that to the setpoint SP. PID or proportional integral derivative controllers are the most effective and stable controllers. Final control elements and actuators \u2013 Final control elements are those that receive the control action signals from the controllers. They adjust the process variable PV at the desired parameter. Actuators are an important component within final control elements. These have a direct influence on the control process. There are two types of control loops, open-loop control, and closed-loop or feedback control loops. An open-loop control system acts completely based on input; the output does not affect the control action. A closed-loop control system looks at the current output and alters it to the desired condition; also known as a feedback system, the control action in these systems is based on the output. Open-loop control is used when low cost is a priority as open control is inexpensive. They are also great when the output rarely changes if at all for example in cooling pumps. They can be used when there is no possibility of quantitative measurement or when the process is erratic, for example, a process with an erratic sensor. Closed-loop or feedback control loops are used when the measurement is feasible, and the process has a degree of predictability; that is when we have a known estimated response to the input control. We also use closed-loop control when the output varies from the desired outcome. Before working with closed-loop control, all parts must be in proper working condition and order and there must be no erratic sensors. This makes closed-loop controls more expensive than open-loop control. Setpoint in control systems is the target value that an automatic control system, for example, a PID controller will aim to reach. For example, a boiler control system will have a temperature setpoint which is set using a thermostat. This is the temperature the control system aims to attain, and the entire system will modulate the actuators to achieve it. The reference signal is the signal which is external to the control loop, this serves as the reference or the comparison to the controller variable. Reference signal tracking involves keeping track of this value and constantly updating the system to match it. Feedback loops come in two different kinds: positive and negative. Negative feedback loops are more common and work to keep a system stabilized or at equilibrium. In negative feedback, the effective input is the difference between the reference input and the feedback signal but in positive feedback, the effective input is a summation of the reference input and the feedback signal. This is why the stability of a system increases in negative feedback, but it decreases in positive feedback. Thus, the accuracy of a system also increases in negative feedback. An electronic amplifier is an example of a negative feedback system. We prefer negative feedback to positive feedback since positive feedback tends to lead to instability due to exponential growth or chaotic behavior, on the other hand, negative feedback promotes stability. Requirements of a good control system: Sensitivity \u2013 The rate of change of a control system with the change in its surroundings is called sensitivity. A good control system should be sensitive to its input only and should not be sensitive to the surrounding parameters. Accuracy \u2013 The tolerance of errors of an instrument is known as accuracy. We can improve the accuracy by using feedback elements like adding an error detector circuit in the control system to increase the accuracy. Stability \u2013 If the input of a system is zero then the output should also be a zero value. If the input changes, the output also changes as per the system function, this is a stable system. Noise \u2013 Undesired signal input due to external sources is known as noise. A good control system should have a high noise tolerance value, so it can reduce the noise level. System performance decreases as noise value increases. Speed \u2013 In control systems, the time taken by the output to be stable is known as speed. High-speed systems are considered good control systems. Bandwidth \u2013 Bandwidth is the range of frequencies of a system. Bandwidth is decided by the operating frequencies. A system having a high bandwidth is considered a good control system. Oscillation \u2013 Oscillation means the fluctuations of the output of a system. These oscillations affect the stability and a higher number of these fluctuations in a system will decrease the stability of a system.","title":"Question 4"},{"location":"Question_4/#4-part-1-the-ssh-protocol-key-generation-configuration-of-user-settings-part-2-the-principles-of-control-feedback-control-and-open-loop-control-set-point-control-and-reference-signal-tracking-the-role-of-negative-feedback-requirements-for-control-systems","text":"","title":"4. PART 1 The SSH protocol, key generation, configuration of user settings PART 2 The principles of control, feedback control and open loop control. Set point control and reference signal tracking, the role of negative feedback. Requirements for control systems."},{"location":"Question_4/#part-1","text":"SSH protocol or the secure shell protocol is a method for secure remote login from one computer to another. It has a strong authentication system, it maintains the integrity of the data being sent by running on top of the TCP protocol and always listens to the TCP/IP port number 22 by default, and it is secure since it encrypts the data using encryption algorithms like Advanced Encryption Standard AES or Data Encryption Standard DES. It is more secure than other remote login protocols like telnet and it is better at exchanging data than insecure file sharing protocols like File Transfer Protocol FTP. SSH can be used via the command line to connect and issue commands to the remote computer. SSH first establishes a connection between the client and the server in three steps: Verification of server \u2013 The client initiates an SSH connection with the server, the client will authenticate the public key of the server. Generation of a session key \u2013 Once the server is verified, both client and server will negotiate a session key using a Diffie-Hellman algorithm. The generated key is a shared symmetric key that will be used for the encryption and decryption of the data. Authentication of the client \u2013 The client will send an ID for the public-private key pair. The server will check its authorized_keys file and see if the client's ID exists here. If it is found then the server will generate a random number and encrypt it using the public key, this message is sent to the client. If the client has the correct private key, it can decrypt the message and get the random number. This random number is used with the shared session key to create an MD5 hash. This MD5 hash is sent to the server, the server can use the shared session key and the original random number to calculate its MD5 hash. If the MD5 hash generated by the client matches the one generated by the server, the client has been authenticated and then the connection is completed. Instead of MD5, we can also use SHA-2 or secure hash algorithm 2. SSH is used widely for things like managing routers, server hardware, virtualization platform, remote operating systems, etc. Key generation is the process of generating keys in cryptography. A key is used to encrypt and decrypt whatever data is being encrypted/decrypted. A device or program used to generate keys is called a key generator or keygen. Modern cryptographic systems include symmetric key algorithms like Data Encryption Standard or DES and Advanced Encryption Standard or AES and public key algorithms like RSA. Symmetric key algorithms use a single shared key; keeping data secret requires keeping this key secret. Public key algorithms use a public key and a private key. The public key is made available to anyone by the means of a digital certificate. A sender encrypts the data with the receiver\u2019s public key and only the holder of this private key can then decrypt the data. The SSH protocol uses a combination of the two key algorithms since public key algorithms tend to be much slower than the symmetric key algorithm. One party receives the other\u2019s public key and encrypts a small piece of data, then the remainder of the connection uses a typically faster symmetric key algorithm for encryption. On a Linux machine, we can simply use the ssh-keygen command to generate a public/private authentication key pair. Authentication keys allow a user to connect to a remote system without supplying a password. Keys must be generated for each user separately. Use the -t option to specify the type of keys like RSA or DSA. The user can also specify a passphrase to encrypt the private key. The shh-key command will generate two keys in the hidden directory .ssh in the home directory of the user. These two files are id_rsa and id_rsa.pub. the id_rsa.pub is the public key, this should be placed in the remote systems authorized_keys directory in its .ssh hidden folder on its home directory, this directory must also have root permissions granted. Now whoever has the corresponding private key can login to this remote computer using ssh protocol. A user is an entity in Linux that can manipulate files and perform other operations. Each user is assigned a unique ID called the UID or user identifier. When the system is created ID 0 is assigned to the root user and IDs 1 to 999 are assigned to the system users and thus the IDs of local users begin from 1000. On Linux systems, user settings configuration can be done using the command line. A new user can be added using the adduser command followed by a name that has not already been taken. This command will create a new account with a new UID, the user\u2019s information is stored in the /etc/passwd and /etc/shadow files, and create a home directory for this user. The user can set a password for this account using the passwd followed by the name. A user account can be removed using the deluser command. The user configuration file which is the /etc/passwd file can be accessed using a text editor like nano since the data is stored in plaintext. The usermod command with various arguments can be used to make changes like changing the ID of the user, modifying the group ID of the user, changing the user login name, and changing the home directory of the user. The root user has access to all files on the system whereas the normal users have limited access.","title":"Part 1"},{"location":"Question_4/#part-2","text":"Control systems manage commands, direct, or regulates the behavior of other devices or systems using control loops. A control loop is the fundamental building block of industrial control systems. Control loops are systems applied by design engineers to maintain process variables PVs at the desired value or setpoint SP. Control loops are important for maintaining the stability of a system and for consistently producing the desired outcome of a process. A basic example of a control loop is a temperature control loop. These work to maintain the temperatures in our homes and offices. The steps are as follows: The process to be controlled is established. In the case of the temperature control loop, this refers to the temperature of a substance that is being heated. Sensors measure the process value PV. In the case of the temperature control loop, the current value of the temperature, the sensor is usually a thermostat in this example. Sensors feed the measured value of the process value to the controller. In this example the temperature controller. This initiates the control process to achieve the set point or desired temperature in this example. The final control element receives the manipulated values from the controller. In this example, the temperature is increased or decreased. The core components of a control loop are: Sensors and transducers \u2013 Sensors are the initial measurement devices in a control loop. They convert the process variable PV into corresponding analog or digital signals which are read by the controllers. Transducers are advanced sensors that further transform the given values through signal conditioning. Controllers \u2013 The controller is the device that interprets the measurements fed by the sensor and determines the control action to take based on a comparison of that to the setpoint SP. PID or proportional integral derivative controllers are the most effective and stable controllers. Final control elements and actuators \u2013 Final control elements are those that receive the control action signals from the controllers. They adjust the process variable PV at the desired parameter. Actuators are an important component within final control elements. These have a direct influence on the control process. There are two types of control loops, open-loop control, and closed-loop or feedback control loops. An open-loop control system acts completely based on input; the output does not affect the control action. A closed-loop control system looks at the current output and alters it to the desired condition; also known as a feedback system, the control action in these systems is based on the output. Open-loop control is used when low cost is a priority as open control is inexpensive. They are also great when the output rarely changes if at all for example in cooling pumps. They can be used when there is no possibility of quantitative measurement or when the process is erratic, for example, a process with an erratic sensor. Closed-loop or feedback control loops are used when the measurement is feasible, and the process has a degree of predictability; that is when we have a known estimated response to the input control. We also use closed-loop control when the output varies from the desired outcome. Before working with closed-loop control, all parts must be in proper working condition and order and there must be no erratic sensors. This makes closed-loop controls more expensive than open-loop control. Setpoint in control systems is the target value that an automatic control system, for example, a PID controller will aim to reach. For example, a boiler control system will have a temperature setpoint which is set using a thermostat. This is the temperature the control system aims to attain, and the entire system will modulate the actuators to achieve it. The reference signal is the signal which is external to the control loop, this serves as the reference or the comparison to the controller variable. Reference signal tracking involves keeping track of this value and constantly updating the system to match it. Feedback loops come in two different kinds: positive and negative. Negative feedback loops are more common and work to keep a system stabilized or at equilibrium. In negative feedback, the effective input is the difference between the reference input and the feedback signal but in positive feedback, the effective input is a summation of the reference input and the feedback signal. This is why the stability of a system increases in negative feedback, but it decreases in positive feedback. Thus, the accuracy of a system also increases in negative feedback. An electronic amplifier is an example of a negative feedback system. We prefer negative feedback to positive feedback since positive feedback tends to lead to instability due to exponential growth or chaotic behavior, on the other hand, negative feedback promotes stability. Requirements of a good control system: Sensitivity \u2013 The rate of change of a control system with the change in its surroundings is called sensitivity. A good control system should be sensitive to its input only and should not be sensitive to the surrounding parameters. Accuracy \u2013 The tolerance of errors of an instrument is known as accuracy. We can improve the accuracy by using feedback elements like adding an error detector circuit in the control system to increase the accuracy. Stability \u2013 If the input of a system is zero then the output should also be a zero value. If the input changes, the output also changes as per the system function, this is a stable system. Noise \u2013 Undesired signal input due to external sources is known as noise. A good control system should have a high noise tolerance value, so it can reduce the noise level. System performance decreases as noise value increases. Speed \u2013 In control systems, the time taken by the output to be stable is known as speed. High-speed systems are considered good control systems. Bandwidth \u2013 Bandwidth is the range of frequencies of a system. Bandwidth is decided by the operating frequencies. A system having a high bandwidth is considered a good control system. Oscillation \u2013 Oscillation means the fluctuations of the output of a system. These oscillations affect the stability and a higher number of these fluctuations in a system will decrease the stability of a system.","title":"Part 2"},{"location":"Question_5/","text":"5. PART 1 Present the adversarial searches and the conditions necessary for the existence of a winning strategy. PART 2 MOS transistor: large signal model and characteristics. The MOS transistor as a switch. CMOS inverter, basic logic gates. The operational amplifier. Negative feedback. Basic applications. Part 1 Adversarial search is a search where we examine a problem that arises when we try to plan, and other agents are planning against us. This type of search strategy is not associated with a single agent, instead, there is more than one agent which is searching for the solution in the same search space. This situation usually occurs in game playing. An environment with more than one agent is called a multi-agent environment, in this, each agent is an opponent of the other agent and playing against each other. Each agent has to consider the action of other agents and the effect of that action on their performance. Games are modeled as a search problem and heuristic evaluation function; these are the two main factors that help to model and solve games in artificial intelligence. The types of games in AI: Perfect information \u2013 In this type of game, the agents have all the information or in other words can see the entire game board. They can see their opponent agent\u2019s moves and current positions. Examples are chess and checkers. Imperfect information \u2013 In this type of game the agents do not have all the information about the game, they might have partial information, but never the entire information. Examples are tic tac toe and battleship. Deterministic Games \u2013 These types of games follow a strict pattern and set of rules for the games. There is no randomness associated with them. Examples are chess and tic tac toe. Non-deterministic games \u2013 These types of games have various unpredictable events and have a factor of chance or luck. This luck factor is introduced by dice or cards. Examples are monopoly and poker. Zero-Sum games are adversarial search that involves pure competition. In these games, each agent\u2019s gain or loss is perfectly balanced by the losses and gains of the competing agent. One player of the game tries to maximize one single value, while the other player tries to minimize it. A problem can be formalized with the following elements: Initial state \u2013 This specifies how the game is set up at the start. Players \u2013 This specifies which player has moved in the state space Actions \u2013 This returns the set of legal moves in the state space Results \u2013 This is the transition model, which specifies the result of moves in the state space. Terminal-test \u2013 This is set to true if the game is over, else this is false in all other cases. The state where the game ends is called the terminal state. Utility function \u2013 This gives a numeric value for the terminal states. This can be won, lose, drawn, or any numerical value. The Minimax algorithm is a very famous example of an adversarial search algorithm used to solve two player games in which we are competing against one opponent agent. Usually, both agents taking part in such games have complete information, thus they are perfect information games. Minimax algorithm is a backtracking algorithm, which means we begin at the root node and traverse deep to the terminal nodes, here we calculate the values and propagate back to the root node and then choose the value or path to take. In such games, we deal with the concept of the best move or optimal move. Here both competing agents will make moves that are optimal towards making them the winner, this results in the optimal moves for each side being the worst-case scenario for the opponent agent. The reason this algorithm is called the Minimax algorithm is since we are dealing with two agents, the first one is MAX and the second one is MIN. In a Minimax algorithm, we form a binary game tree, each node will have 2 child nodes and this tree will increase based on the complexity, usually, the root node is where the MAX agent will have its chance, on the next level will be MIN agent and this way we keep on alternating one by one. The objective of MAX is to maximize its utility, and MIN will try to minimize its utility. MAX will try to make the best move from the point of view of the MAX agent, while MIN will try to make the worst movie from the point of view of the MAX agent, which in turn will be the best move from its point of view. We start with MAX at the root node, we can take one of two paths, once the path is chosen, now is the turn for MIN, it will have two paths as well. The leaf nodes or the terminal nodes have utility values, the negative values indicate that the MIN agent has won, and the positive values indicate that the MAX agent has won. Using backtracking and keeping in mind the chance of either MIN or MAX agent, we can determine the utility value for each node. The time complexity of this algorithm is O(bd) where b is the branching factor, or the number of possible choices and d is the depth. This is fine for games with a smaller number of choices, but the time complexity goes exponentially up for games like chess where there are lots of choices, this level of the game tree is not feasible. To improve on the Minimax algorithm, we can use Alpha-beta pruning. Alpha-beta pruning can reduce the time complexity of the Minimax algorithm by cutting off or pruning some nodes, thus we do not traverse every single branch to find the optimal route for the MAX agent. This is done by pruning off the parts when we already found a better path. To find a path to prune, if a certain path\u2019s parent node or any other node further up is a better choice, then we do not need to traverse that path at all. To calculate this we use two values, alpha, and beta. Alpha is the best choice or highest value we have found so far at any point along the path for MATH, alpha acts as a lower bound for all MAX nodes, thus only MAX nodes update the value of alpha. Initially, we take the value of alpha to be negative infinity. Beta is the best choice or lowest value we have found so far at any point along the path for MIN, it acts as an upper bound for all MIN values, thus only MIN nodes update the value of beta. Initially, we take the value of beta to be positive infinity. Since MINIMAX follows a depth-first search, we will start at the deepest left side node of the tree with values for alpha being negative infinity and beta being positive infinity. Using the same strategy as MINIMAX, we will find the values of alpha and beta, and using the construct that at a MAX node, the value must be equal to or greater than alpha, and at a MIN node, the value must be less than or equal to the beta, we will find the node values. But this way when we start to traverse a new node, we will first see if it follows the alpha-beta construct, if it does not then we can prune that branch since there is no point in visiting that branch. Alpha-beta pruning gives us the same steps as MINIMAX's final answer but uses less time to do so. Part 2 A MOS transistor is also called a MOSFET transistor is the metal oxide semiconductor field-effect transistor. This transistor has four terminals, gate, substrate, drain, and source. The device is formed using the MOS structure in which we have three layers stacked on top of each other, the top layer is the metal layer, the second layer is the silicon oxide layer, and the last layer is the substrate layer. The top metal layer is connected to the gate terminal and has a gate voltage. The substrate layer is connected to the substrate terminal and has a substrate voltage. Concerning channel, we have two types of MOS transistors: Enhancement type \u2013 this has no conducting channel region between the source and drain when the gate voltage is zero. Depletion type \u2013 this has a conducting channel region between the source and drain when a gate bias voltage is 0. MOSFETS vary the voltage on the gate by changing the resistance on the source and drain terminals. An example is an NMOS transistor in which we have a P-type substrate on which we create two heavily doped n-type regions, one of them is the source and the other is the drain. The oxide layer is deposited on top of the source and drain and then comes the metal layer on top of that. When a positive voltage is applied to the gate, this causes the depletion region to connect both the source and drain causing a field effect. At this point the electricity is not flowing, before the gate voltage has reached the threshold voltage, this is called saturation. When the gate voltage surpasses the threshold voltage the current flows between the source and drain. A large-signal model is an analysis method for transistors where we have nonlinear elements. Under large-signal conditions, AC signals have a high enough magnitude that nonlinear effects must be considered. In a large signal model, the large signal affects the operating point. The non-linear elements can be limited by power supply values to avoid variation in operating points. A small-signal model ignores these variations. MOS transistors can be used as a switch to turn on and off. Billions of these switches are used in modern microprocessors. MOS transistors are good for switching since they are good for high power applications since they can switch faster, this lets them use smaller inductors and supplies, this, in turn, increases the efficiency of MOS transistors as a switch. When the gate voltage of the MOS transistor is less than the threshold voltage, the circuit is considered open and has no current flow, this is the OFF state. When the gate voltage of the MOS transistor is more than the threshold voltage, the circuit is closed, the current value is more than 0, this is the ON state. CMOS inverter or simply CMOS is a complementary metal-oxide semiconductor that acts as a NOT gate or an inverter. A high voltage or a logic 1 will result in a logic 0 output and a low voltage or a logic 0 will result in a logic 1 output. A CMOS inverter is created with one PMOS or P-type channel MOS and one NMOS or N-type channel MOS. A PMOS has a P-type source and drain on an N-type substrate, and we have the output connected near the ground. PMOS gives a low output for high input and high output for low input. An NMOS has an N-type source and drains on a P-type substrate, and we have the output connected near the gate voltage. NMOS also gives a low output for high input and high output for low input, thus both PMOS and NMOS act as inverters. To create a CMOS inverter, we connect the PMOS and NMOS outputs with PMOS connected to the gate voltage and the NMOS connected to the ground. When the input is a logic 0, the NMOS circuit is open and the PMOS circuit is shorted, which means the PMOS is active, and we get a logic 1. When the input logic is 1, the PMOS circuit is open and the NMOS circuit is shorted, here the output connects to the ground and thus we get a logic 0. We use a CMOS inverter since it consumes less power when switching at high frequencies. We can create a CMOS NAND gate using two PMOS and two NMOS. We connect the two PMOS in parallel and the two NMOS in series. The output is joined for all, the PMOS circuit connects to the gate voltage and the NMOS circuit connects to the ground. We have two inputs A and B. A is given to one PMOS and one NMOS and B is also given to one PMOS and one NMOS. In this circuit, when we apply logic 1 on both A and B, we get a logic 0 as output, and we get logic 1 in any other case. We can also create an AND gate by connecting the output of the CMOS NAND gate to a CMOS inverter. We can create a CMOS NOR gate using two PMOS and two NMOS. We connect the two PMOS in series and the two NMOS in parallel. The output is joined for all, the PMOS circuit connects to the gate voltage and the NMOS circuit connects to the ground. We have two inputs A and B. A is given to one PMOS and one NMOS and B is also given to one PMOS and one NMOS. In this circuit, when we apply a logic 0 on both A and B, then we get a logic 1 as output, and we get a logic 0 in any other case. We can also create an OR gate by connecting the output of the CMOS NOR gate to a CMOS inverter. The operational amplifier is a type of amplifier, amplifiers are devices that take in a signal and produce the same signal with a larger amplitude. Op-amps have a positive input called the non-inverting input and a negative input called the inverting input, the difference between these two inputs is the final input, thus this is called differential input. The output is the product of the differential inputs and the high gain of this device. Op-amps have a very high gain like 105 which makes the circuit design difficult because of high gain sensitivity. Negative feedback is when we connect the output back to the inverting input, this makes it possible to set again and cut off frequency to the desired value, which improves the stability and reduced variation. Op-amps are used for AC and DC signal amplification, voltage regulators, and as filters.","title":"Question 5"},{"location":"Question_5/#5-part-1-present-the-adversarial-searches-and-the-conditions-necessary-for-the-existence-of-a-winning-strategy-part-2-mos-transistor-large-signal-model-and-characteristics-the-mos-transistor-as-a-switch-cmos-inverter-basic-logic-gates-the-operational-amplifier-negative-feedback-basic-applications","text":"","title":"5. PART 1 Present the adversarial searches and the conditions necessary for the existence of a winning strategy. PART 2 MOS transistor: large signal model and characteristics. The MOS transistor as a switch. CMOS inverter, basic logic gates. The operational amplifier. Negative feedback. Basic applications."},{"location":"Question_5/#part-1","text":"Adversarial search is a search where we examine a problem that arises when we try to plan, and other agents are planning against us. This type of search strategy is not associated with a single agent, instead, there is more than one agent which is searching for the solution in the same search space. This situation usually occurs in game playing. An environment with more than one agent is called a multi-agent environment, in this, each agent is an opponent of the other agent and playing against each other. Each agent has to consider the action of other agents and the effect of that action on their performance. Games are modeled as a search problem and heuristic evaluation function; these are the two main factors that help to model and solve games in artificial intelligence. The types of games in AI: Perfect information \u2013 In this type of game, the agents have all the information or in other words can see the entire game board. They can see their opponent agent\u2019s moves and current positions. Examples are chess and checkers. Imperfect information \u2013 In this type of game the agents do not have all the information about the game, they might have partial information, but never the entire information. Examples are tic tac toe and battleship. Deterministic Games \u2013 These types of games follow a strict pattern and set of rules for the games. There is no randomness associated with them. Examples are chess and tic tac toe. Non-deterministic games \u2013 These types of games have various unpredictable events and have a factor of chance or luck. This luck factor is introduced by dice or cards. Examples are monopoly and poker. Zero-Sum games are adversarial search that involves pure competition. In these games, each agent\u2019s gain or loss is perfectly balanced by the losses and gains of the competing agent. One player of the game tries to maximize one single value, while the other player tries to minimize it. A problem can be formalized with the following elements: Initial state \u2013 This specifies how the game is set up at the start. Players \u2013 This specifies which player has moved in the state space Actions \u2013 This returns the set of legal moves in the state space Results \u2013 This is the transition model, which specifies the result of moves in the state space. Terminal-test \u2013 This is set to true if the game is over, else this is false in all other cases. The state where the game ends is called the terminal state. Utility function \u2013 This gives a numeric value for the terminal states. This can be won, lose, drawn, or any numerical value. The Minimax algorithm is a very famous example of an adversarial search algorithm used to solve two player games in which we are competing against one opponent agent. Usually, both agents taking part in such games have complete information, thus they are perfect information games. Minimax algorithm is a backtracking algorithm, which means we begin at the root node and traverse deep to the terminal nodes, here we calculate the values and propagate back to the root node and then choose the value or path to take. In such games, we deal with the concept of the best move or optimal move. Here both competing agents will make moves that are optimal towards making them the winner, this results in the optimal moves for each side being the worst-case scenario for the opponent agent. The reason this algorithm is called the Minimax algorithm is since we are dealing with two agents, the first one is MAX and the second one is MIN. In a Minimax algorithm, we form a binary game tree, each node will have 2 child nodes and this tree will increase based on the complexity, usually, the root node is where the MAX agent will have its chance, on the next level will be MIN agent and this way we keep on alternating one by one. The objective of MAX is to maximize its utility, and MIN will try to minimize its utility. MAX will try to make the best move from the point of view of the MAX agent, while MIN will try to make the worst movie from the point of view of the MAX agent, which in turn will be the best move from its point of view. We start with MAX at the root node, we can take one of two paths, once the path is chosen, now is the turn for MIN, it will have two paths as well. The leaf nodes or the terminal nodes have utility values, the negative values indicate that the MIN agent has won, and the positive values indicate that the MAX agent has won. Using backtracking and keeping in mind the chance of either MIN or MAX agent, we can determine the utility value for each node. The time complexity of this algorithm is O(bd) where b is the branching factor, or the number of possible choices and d is the depth. This is fine for games with a smaller number of choices, but the time complexity goes exponentially up for games like chess where there are lots of choices, this level of the game tree is not feasible. To improve on the Minimax algorithm, we can use Alpha-beta pruning. Alpha-beta pruning can reduce the time complexity of the Minimax algorithm by cutting off or pruning some nodes, thus we do not traverse every single branch to find the optimal route for the MAX agent. This is done by pruning off the parts when we already found a better path. To find a path to prune, if a certain path\u2019s parent node or any other node further up is a better choice, then we do not need to traverse that path at all. To calculate this we use two values, alpha, and beta. Alpha is the best choice or highest value we have found so far at any point along the path for MATH, alpha acts as a lower bound for all MAX nodes, thus only MAX nodes update the value of alpha. Initially, we take the value of alpha to be negative infinity. Beta is the best choice or lowest value we have found so far at any point along the path for MIN, it acts as an upper bound for all MIN values, thus only MIN nodes update the value of beta. Initially, we take the value of beta to be positive infinity. Since MINIMAX follows a depth-first search, we will start at the deepest left side node of the tree with values for alpha being negative infinity and beta being positive infinity. Using the same strategy as MINIMAX, we will find the values of alpha and beta, and using the construct that at a MAX node, the value must be equal to or greater than alpha, and at a MIN node, the value must be less than or equal to the beta, we will find the node values. But this way when we start to traverse a new node, we will first see if it follows the alpha-beta construct, if it does not then we can prune that branch since there is no point in visiting that branch. Alpha-beta pruning gives us the same steps as MINIMAX's final answer but uses less time to do so.","title":"Part 1"},{"location":"Question_5/#part-2","text":"A MOS transistor is also called a MOSFET transistor is the metal oxide semiconductor field-effect transistor. This transistor has four terminals, gate, substrate, drain, and source. The device is formed using the MOS structure in which we have three layers stacked on top of each other, the top layer is the metal layer, the second layer is the silicon oxide layer, and the last layer is the substrate layer. The top metal layer is connected to the gate terminal and has a gate voltage. The substrate layer is connected to the substrate terminal and has a substrate voltage. Concerning channel, we have two types of MOS transistors: Enhancement type \u2013 this has no conducting channel region between the source and drain when the gate voltage is zero. Depletion type \u2013 this has a conducting channel region between the source and drain when a gate bias voltage is 0. MOSFETS vary the voltage on the gate by changing the resistance on the source and drain terminals. An example is an NMOS transistor in which we have a P-type substrate on which we create two heavily doped n-type regions, one of them is the source and the other is the drain. The oxide layer is deposited on top of the source and drain and then comes the metal layer on top of that. When a positive voltage is applied to the gate, this causes the depletion region to connect both the source and drain causing a field effect. At this point the electricity is not flowing, before the gate voltage has reached the threshold voltage, this is called saturation. When the gate voltage surpasses the threshold voltage the current flows between the source and drain. A large-signal model is an analysis method for transistors where we have nonlinear elements. Under large-signal conditions, AC signals have a high enough magnitude that nonlinear effects must be considered. In a large signal model, the large signal affects the operating point. The non-linear elements can be limited by power supply values to avoid variation in operating points. A small-signal model ignores these variations. MOS transistors can be used as a switch to turn on and off. Billions of these switches are used in modern microprocessors. MOS transistors are good for switching since they are good for high power applications since they can switch faster, this lets them use smaller inductors and supplies, this, in turn, increases the efficiency of MOS transistors as a switch. When the gate voltage of the MOS transistor is less than the threshold voltage, the circuit is considered open and has no current flow, this is the OFF state. When the gate voltage of the MOS transistor is more than the threshold voltage, the circuit is closed, the current value is more than 0, this is the ON state. CMOS inverter or simply CMOS is a complementary metal-oxide semiconductor that acts as a NOT gate or an inverter. A high voltage or a logic 1 will result in a logic 0 output and a low voltage or a logic 0 will result in a logic 1 output. A CMOS inverter is created with one PMOS or P-type channel MOS and one NMOS or N-type channel MOS. A PMOS has a P-type source and drain on an N-type substrate, and we have the output connected near the ground. PMOS gives a low output for high input and high output for low input. An NMOS has an N-type source and drains on a P-type substrate, and we have the output connected near the gate voltage. NMOS also gives a low output for high input and high output for low input, thus both PMOS and NMOS act as inverters. To create a CMOS inverter, we connect the PMOS and NMOS outputs with PMOS connected to the gate voltage and the NMOS connected to the ground. When the input is a logic 0, the NMOS circuit is open and the PMOS circuit is shorted, which means the PMOS is active, and we get a logic 1. When the input logic is 1, the PMOS circuit is open and the NMOS circuit is shorted, here the output connects to the ground and thus we get a logic 0. We use a CMOS inverter since it consumes less power when switching at high frequencies. We can create a CMOS NAND gate using two PMOS and two NMOS. We connect the two PMOS in parallel and the two NMOS in series. The output is joined for all, the PMOS circuit connects to the gate voltage and the NMOS circuit connects to the ground. We have two inputs A and B. A is given to one PMOS and one NMOS and B is also given to one PMOS and one NMOS. In this circuit, when we apply logic 1 on both A and B, we get a logic 0 as output, and we get logic 1 in any other case. We can also create an AND gate by connecting the output of the CMOS NAND gate to a CMOS inverter. We can create a CMOS NOR gate using two PMOS and two NMOS. We connect the two PMOS in series and the two NMOS in parallel. The output is joined for all, the PMOS circuit connects to the gate voltage and the NMOS circuit connects to the ground. We have two inputs A and B. A is given to one PMOS and one NMOS and B is also given to one PMOS and one NMOS. In this circuit, when we apply a logic 0 on both A and B, then we get a logic 1 as output, and we get a logic 0 in any other case. We can also create an OR gate by connecting the output of the CMOS NOR gate to a CMOS inverter. The operational amplifier is a type of amplifier, amplifiers are devices that take in a signal and produce the same signal with a larger amplitude. Op-amps have a positive input called the non-inverting input and a negative input called the inverting input, the difference between these two inputs is the final input, thus this is called differential input. The output is the product of the differential inputs and the high gain of this device. Op-amps have a very high gain like 105 which makes the circuit design difficult because of high gain sensitivity. Negative feedback is when we connect the output back to the inverting input, this makes it possible to set again and cut off frequency to the desired value, which improves the stability and reduced variation. Op-amps are used for AC and DC signal amplification, voltage regulators, and as filters.","title":"Part 2"},{"location":"Question_6/","text":"6. PART 1 Sequential logical: Latches and Flip-Flops. Counters. Shift registers. Memories. PART 2 New elements of HTML5. New features of CSS3. Control structures in web scripts. Sensor through a web page. Providing remote management systems through a web page. Part 1 Sequential logic is a form of binary circuit design that has one or more inputs and one or more outputs, whose states are dependent in part on the previous states. Sequential logic circuits have some form of inherent memory built-in. Sequential logic circuits remember the conditions and stay fixed in their current state until the next clock signal changes one of the states. Latches are basic storage elements that operate with signal levels. Latches are level-sensitive devices and are useful for the design of asynchronous sequential circuits. There are two basic latches called the SR latch and the D latch. SR latch is a circuit with two cross-coupled NOR gates or two cross-coupled NAND gates with two inputs labeled S for set and R for a reset with two outputs Q and Q prime. This latch has two useful states. When the output Q is 1 and Q prime is 0, the latch is said to be inset state. When Q is 0 and Q prime is 1, then it is in a reset state. Normally the outputs Q and Q prime complement each other. An SR Nor latch has these states: When S is 0 and R is 1, then the output Q is 0 and Q prime is 1. This is the reset condition. Now if R goes back to 0, the reset state remains, now S and R are 0 but the previous output has been stored as memory. When S is 1 and R is 0, then Q prime is 0 and Q is 1. This is the set state. Now if S goes back to 0, the circuit remains in the set state. When S and R are both 1, then Q and Q prime become 0, this violates that both outputs must complement each other, this condition is avoided by making sure that 1s are not applied to both inputs simultaneously, this is the invalid state. To fix the drawback of the invalid state where both inputs are 1, we use the D latch, which has a single input D by sending the same signal to both S and R but placing an inverter in front of either one of them. This ensures that the input to one is always the opposite of the input to the other. Flip-Flops are just edge-triggered latches, it only changes state when a control signal goes from high to low or low to high. This makes using flip-flops with clock signals possible. We have SR flip-flops and D flip-flops which are the same as the latches. We have other flip-flops like JK flip-flop and T flip-flop. Counters are devices that store the number of times a particular event or process has occurred, often in relationship to a clock. A 4-bit ripple counter will count from 0000 or 0 to 1111 or 15. This can be made by chaining 4 T flip-flops together. Each clock pulse causes a change that ripples through the chain of flip flops with a delay. We also have synchronous counters which use the same clock signal on all flip-flops, this uses more circuitry but has no delay. There are also ring counters which are composed of flip-flops connected into a shift register, with the output of the last flip-flop fed to the input of the first, making a circular or ring structure. There are two types of ring counters, a straight ring counter connects the output of the last shift register to the first shift register input, this circulates a single one or zero bit around the ring. A twisted ring counter connects the complement of the output of the last shift register to the input of the first register and circulates a stream of ones followed by zeros around the ring. Flip-flops can be used to store a single bit of binary data. To store multiple bits of data, we need multiple flip-flops, N flip-flops connected to store n bits of data are called registers. A shift register is a type of digital circuit using a cascade of flip-flops where the output of one flip-flop is connected to the input of the next. They share a single clock signal which causes the data stores in the system to shift from one location to the next. By connecting the last flip-flop back to the first, the data can cycle within the shifters for extended periods and in this form, they are used as a form of computer memory. The registers which shift the bits to the left are called shift left registers and the registers which will shift the bits to the right are called shift right registers. We have 4 basic types of shift registers: Serial in serial out \u2013 these allow serial input and produce serial output. Serial in parallel out \u2013 these allow serial input and produce a parallel output. Parallel in serial out \u2013 these allow parallel input and produce a serial output. Parallel in parallel out \u2013 these allow parallel input and produce a parallel output. System memory is like a human brain. It is used to store data and instructions. Computer memory is the storage space in computers where data to be processed and instructions required for processing are stored. The memory is divided into a large number of small parts. Each part is called a cell and each location or cell has a unique address that varies from zero to memory size minus one. Memory is primarily of two types, internal memory like cache memory and primary memory, and external memory like magnetic disk, optical disk, or flash storage. Part 2 HTML 5 introduced the following new elements: Article \u2013 represents an independent piece of content of a document, such as a block entry or newspaper article. Aside \u2013 Represents a piece of content that is only slightly related to the rest of the page. Audio \u2013 defines an audio file. Canvas \u2013 This is used for rendering dynamic bitmap graphics on the fly, such as graphs or games. Command \u2013 Represents a command the user can invoke. Datalist \u2013 Together with the new list attribute for input, can be used to make combo boxes. Details \u2013 Represents additional information or controls which the user can obtain on demand. Embed \u2013 Defines external interactive content or plugin. Figure \u2013 Represents a piece of self-contained flow content, typically referenced as a single unit from the main flow of the document. Footer \u2013 Represents a footer for a section and can contain information about the author, copyright information, etc. Header \u2013 Represents a group of introductory or navigational aids. Group \u2013 represents the header of a section. Keygen \u2013 Represents a control for key pair generation. Mark \u2013 Represents a run of text in one document marked or highlighted for reference purposes. Meter \u2013 Represents a measurement such as a disk usage. Nav \u2013 Represents a section of the document intended for navigation. Output \u2013 Represents some type of output, such as from a calculation done through scripting. Progress \u2013 Represents a completion of a task, such as downloading or when performing a series of expensive operations. Ruby \u2013 Together with rt and RP allows for marking up ruby annotations. Section \u2013 Represents a generic document or application section. Time \u2013 Represents a date or time. Video \u2013 Defines a video file. Wbr \u2013 Represents a line break opportunity. New features of CSS3 are as follows: Advanced animations \u2013 We can utilize both transition and animation when it is required to change a component starting with one state moving onto the next. With transitions, a user can make float or mouse down effects or trigger the animation by changing the style of a component with JavaScript. Multiple backgrounds and gradient \u2013 With multiple backgrounds, creators can stack various pictures as backgrounds of a component. Each picture or layer can be moved and animated with ease. CSS3 also allows for gradients as backgrounds. Multiple column layouts \u2013 This feature enables web designers to display their content in multiple sections with alternatives like column-width, column-gap, etc. Opacity \u2013 This property can make components more transparent. The opacity ranges from 0 which is transparent to 1 which is opaque. Rounded corner \u2013 This feature is very famous among social media giants. Rounded corners can make a site look tidier. Selectors \u2013 these are patterns or elements and other terms that tell the browser which HTML elements should be selected to have the CSS property values inside the rule applied to them. The elements selected by the selector are called the subject of the selector. Control structures are programming constructs that determine which statements or procedures are executed at a given point in a program, either based on the evaluation of one or more variables or in response to some external input. In the absence of control structures, program statements will execute sequentially or in the order in which they appear in the code. There are two basic kinds of control structures \u2013 conditional and iterative. A conditional control structure typically defines a sequence of one or more program statements that will be executed if a particular condition is met. An iterative control structure loops through or iterates a sequence of program statements repeatedly until some predetermined exit condition is met. Program loops can be for example a counting loop that iterates a fixed number of times and then exits the loop. The number of iterations is determined by the value of a counter variable. In web scripts we have some basic types of control structures: If-else \u2013 This evaluates a condition as a true or false value, if the value is true, the block following if statement is executed, if this is false then the block following else is executed. We can only execute one of the two blocks for a given expression. Switch \u2013 The switch statement allows us to make multiple if-else constructs more elegantly. We have a statement that might have multiple outputs, for each discrete output we have a different switch case. We also have a default case for when none of the given cases are met. For a given expression only one case can be executed at a time. For loop \u2013 This control structure has a counter variable and will run a block of code for a specified number of times, specified by the counter variable. Once this number has been reached, the program exits the loop. For in \u2013 This control structure iterates through the enumerable properties of a JavaScript object. While loop \u2013 This loop executes a block of code over and over until a certain condition is met. This condition is defined outside the loop and can be changed within the loop. Once this condition is met, the loop exits. Try catch \u2013 The try-catch block is used to handle exceptions. The code is first sent to the try block, if it doesn\u2019t throw an error, the next catch block is ignored, and program flow continues. If the try block throws an error, the control is transferred to the catch block which will execute the given exception handler. Sensor data is used by many web apps to enable immersive gaming, fitness tracking, and augmented reality applications. The Generic Sensor API is a set of interfaces that expose sensor devices to the web platform. The API has a base sensor interface and a set of sensor classes built on top. There are sensors like accelerometer, gyroscope, gravity sensor for motion, and ambient light sensors for the environment. Remote Management is managing a computer or a network from a remote location. It involves installing software and managing all activities on the systems/network, workstations, servers, or endpoints of a client, from a remote location. Remote administration refers to any method of controlling a computer from a remote location. Software that allows remote administration is becoming increasingly common and is often used when it is difficult or impractical to be physically near a system to use it.","title":"Question 6"},{"location":"Question_6/#6-part-1-sequential-logical-latches-and-flip-flops-counters-shift-registers-memories-part-2-new-elements-of-html5-new-features-of-css3-control-structures-in-web-scripts-sensor-through-a-web-page-providing-remote-management-systems-through-a-web-page","text":"","title":"6. PART 1 Sequential logical: Latches and Flip-Flops. Counters. Shift registers. Memories. PART 2 New elements of HTML5. New features of CSS3. Control structures in web scripts. Sensor through a web page. Providing remote management systems through a web page."},{"location":"Question_6/#part-1","text":"Sequential logic is a form of binary circuit design that has one or more inputs and one or more outputs, whose states are dependent in part on the previous states. Sequential logic circuits have some form of inherent memory built-in. Sequential logic circuits remember the conditions and stay fixed in their current state until the next clock signal changes one of the states. Latches are basic storage elements that operate with signal levels. Latches are level-sensitive devices and are useful for the design of asynchronous sequential circuits. There are two basic latches called the SR latch and the D latch. SR latch is a circuit with two cross-coupled NOR gates or two cross-coupled NAND gates with two inputs labeled S for set and R for a reset with two outputs Q and Q prime. This latch has two useful states. When the output Q is 1 and Q prime is 0, the latch is said to be inset state. When Q is 0 and Q prime is 1, then it is in a reset state. Normally the outputs Q and Q prime complement each other. An SR Nor latch has these states: When S is 0 and R is 1, then the output Q is 0 and Q prime is 1. This is the reset condition. Now if R goes back to 0, the reset state remains, now S and R are 0 but the previous output has been stored as memory. When S is 1 and R is 0, then Q prime is 0 and Q is 1. This is the set state. Now if S goes back to 0, the circuit remains in the set state. When S and R are both 1, then Q and Q prime become 0, this violates that both outputs must complement each other, this condition is avoided by making sure that 1s are not applied to both inputs simultaneously, this is the invalid state. To fix the drawback of the invalid state where both inputs are 1, we use the D latch, which has a single input D by sending the same signal to both S and R but placing an inverter in front of either one of them. This ensures that the input to one is always the opposite of the input to the other. Flip-Flops are just edge-triggered latches, it only changes state when a control signal goes from high to low or low to high. This makes using flip-flops with clock signals possible. We have SR flip-flops and D flip-flops which are the same as the latches. We have other flip-flops like JK flip-flop and T flip-flop. Counters are devices that store the number of times a particular event or process has occurred, often in relationship to a clock. A 4-bit ripple counter will count from 0000 or 0 to 1111 or 15. This can be made by chaining 4 T flip-flops together. Each clock pulse causes a change that ripples through the chain of flip flops with a delay. We also have synchronous counters which use the same clock signal on all flip-flops, this uses more circuitry but has no delay. There are also ring counters which are composed of flip-flops connected into a shift register, with the output of the last flip-flop fed to the input of the first, making a circular or ring structure. There are two types of ring counters, a straight ring counter connects the output of the last shift register to the first shift register input, this circulates a single one or zero bit around the ring. A twisted ring counter connects the complement of the output of the last shift register to the input of the first register and circulates a stream of ones followed by zeros around the ring. Flip-flops can be used to store a single bit of binary data. To store multiple bits of data, we need multiple flip-flops, N flip-flops connected to store n bits of data are called registers. A shift register is a type of digital circuit using a cascade of flip-flops where the output of one flip-flop is connected to the input of the next. They share a single clock signal which causes the data stores in the system to shift from one location to the next. By connecting the last flip-flop back to the first, the data can cycle within the shifters for extended periods and in this form, they are used as a form of computer memory. The registers which shift the bits to the left are called shift left registers and the registers which will shift the bits to the right are called shift right registers. We have 4 basic types of shift registers: Serial in serial out \u2013 these allow serial input and produce serial output. Serial in parallel out \u2013 these allow serial input and produce a parallel output. Parallel in serial out \u2013 these allow parallel input and produce a serial output. Parallel in parallel out \u2013 these allow parallel input and produce a parallel output. System memory is like a human brain. It is used to store data and instructions. Computer memory is the storage space in computers where data to be processed and instructions required for processing are stored. The memory is divided into a large number of small parts. Each part is called a cell and each location or cell has a unique address that varies from zero to memory size minus one. Memory is primarily of two types, internal memory like cache memory and primary memory, and external memory like magnetic disk, optical disk, or flash storage.","title":"Part 1"},{"location":"Question_6/#part-2","text":"HTML 5 introduced the following new elements: Article \u2013 represents an independent piece of content of a document, such as a block entry or newspaper article. Aside \u2013 Represents a piece of content that is only slightly related to the rest of the page. Audio \u2013 defines an audio file. Canvas \u2013 This is used for rendering dynamic bitmap graphics on the fly, such as graphs or games. Command \u2013 Represents a command the user can invoke. Datalist \u2013 Together with the new list attribute for input, can be used to make combo boxes. Details \u2013 Represents additional information or controls which the user can obtain on demand. Embed \u2013 Defines external interactive content or plugin. Figure \u2013 Represents a piece of self-contained flow content, typically referenced as a single unit from the main flow of the document. Footer \u2013 Represents a footer for a section and can contain information about the author, copyright information, etc. Header \u2013 Represents a group of introductory or navigational aids. Group \u2013 represents the header of a section. Keygen \u2013 Represents a control for key pair generation. Mark \u2013 Represents a run of text in one document marked or highlighted for reference purposes. Meter \u2013 Represents a measurement such as a disk usage. Nav \u2013 Represents a section of the document intended for navigation. Output \u2013 Represents some type of output, such as from a calculation done through scripting. Progress \u2013 Represents a completion of a task, such as downloading or when performing a series of expensive operations. Ruby \u2013 Together with rt and RP allows for marking up ruby annotations. Section \u2013 Represents a generic document or application section. Time \u2013 Represents a date or time. Video \u2013 Defines a video file. Wbr \u2013 Represents a line break opportunity. New features of CSS3 are as follows: Advanced animations \u2013 We can utilize both transition and animation when it is required to change a component starting with one state moving onto the next. With transitions, a user can make float or mouse down effects or trigger the animation by changing the style of a component with JavaScript. Multiple backgrounds and gradient \u2013 With multiple backgrounds, creators can stack various pictures as backgrounds of a component. Each picture or layer can be moved and animated with ease. CSS3 also allows for gradients as backgrounds. Multiple column layouts \u2013 This feature enables web designers to display their content in multiple sections with alternatives like column-width, column-gap, etc. Opacity \u2013 This property can make components more transparent. The opacity ranges from 0 which is transparent to 1 which is opaque. Rounded corner \u2013 This feature is very famous among social media giants. Rounded corners can make a site look tidier. Selectors \u2013 these are patterns or elements and other terms that tell the browser which HTML elements should be selected to have the CSS property values inside the rule applied to them. The elements selected by the selector are called the subject of the selector. Control structures are programming constructs that determine which statements or procedures are executed at a given point in a program, either based on the evaluation of one or more variables or in response to some external input. In the absence of control structures, program statements will execute sequentially or in the order in which they appear in the code. There are two basic kinds of control structures \u2013 conditional and iterative. A conditional control structure typically defines a sequence of one or more program statements that will be executed if a particular condition is met. An iterative control structure loops through or iterates a sequence of program statements repeatedly until some predetermined exit condition is met. Program loops can be for example a counting loop that iterates a fixed number of times and then exits the loop. The number of iterations is determined by the value of a counter variable. In web scripts we have some basic types of control structures: If-else \u2013 This evaluates a condition as a true or false value, if the value is true, the block following if statement is executed, if this is false then the block following else is executed. We can only execute one of the two blocks for a given expression. Switch \u2013 The switch statement allows us to make multiple if-else constructs more elegantly. We have a statement that might have multiple outputs, for each discrete output we have a different switch case. We also have a default case for when none of the given cases are met. For a given expression only one case can be executed at a time. For loop \u2013 This control structure has a counter variable and will run a block of code for a specified number of times, specified by the counter variable. Once this number has been reached, the program exits the loop. For in \u2013 This control structure iterates through the enumerable properties of a JavaScript object. While loop \u2013 This loop executes a block of code over and over until a certain condition is met. This condition is defined outside the loop and can be changed within the loop. Once this condition is met, the loop exits. Try catch \u2013 The try-catch block is used to handle exceptions. The code is first sent to the try block, if it doesn\u2019t throw an error, the next catch block is ignored, and program flow continues. If the try block throws an error, the control is transferred to the catch block which will execute the given exception handler. Sensor data is used by many web apps to enable immersive gaming, fitness tracking, and augmented reality applications. The Generic Sensor API is a set of interfaces that expose sensor devices to the web platform. The API has a base sensor interface and a set of sensor classes built on top. There are sensors like accelerometer, gyroscope, gravity sensor for motion, and ambient light sensors for the environment. Remote Management is managing a computer or a network from a remote location. It involves installing software and managing all activities on the systems/network, workstations, servers, or endpoints of a client, from a remote location. Remote administration refers to any method of controlling a computer from a remote location. Software that allows remote administration is becoming increasingly common and is often used when it is difficult or impractical to be physically near a system to use it.","title":"Part 2"},{"location":"Question_7/","text":"7. PART 1 Provide the necessary steps and technologies for developing a sample software product on a choosen platform. Describe the benefits and difficulties of the platform, the implementation steps, and the most widely used current technologies. PART 2 Implementation of control structures in assembly (control program flow, branching, looping) Part 1 Software is planned, created, tested, and deployed, this cycle is called a Software Development Life Cycle or SDLC. Various SDLC methodologies were popular in a certain time frame and were slowly replaced by the next one. Waterfall Model: The waterfall model is broken down into sequential phases. In this model, each phase must be completed before the next phase can begin and there is no overlapping in the phases. The outcome of one phase acts as the input for the next phase sequentially. This model has 6 phases: Requirement gathering and analysis System Design Implementation Integration and Testing Deployment of system Maintenance Advantages: This model is simple and easy to understand and easy to manage due to its rigidity. All phases are processed one at a time which means it has very well-defined stages with deliverables that can be easily assessed. Disadvantages: There is no working software until the end of the life cycle. This introduces lots of risk and uncertainty. This model can get overwhelming for complex projects or long-term projects. This type of model is also not suited for projects which have a risk of changing their requirements. It is difficult to measure progress within the stages. Iterative Model: In this model, we start with a simple implementation of a small set of software requirements and iteratively enhance versions until the complete system is implemented and ready to be deployed. We do not require a full specification of requirements to start. On each iteration, the project is reviewed, and modifications are made on the next iteration. Each iteration flows through the design and development, testing, and implementation phases. Advantages: A working software can be developed early in the life cycle. Results are obtained early, and progress is easy to measure. This type of model is cheaper to change the requirements for. The testing phase is simpler as it is only concerned with the changes in the current iteration. It is easier to analyze risk since risks can be identified on each iteration. Disadvantages: This type of model requires more resources, even though the cost to change the requirements is less, more resources are required to execute the many iterations. This is not suitable for smaller projects. The end of the project is not clearly defined which is a risk. Spiral Model: This model is a combination of iterative models and takes aspects of the sequential flow like in the waterfall model. This model has four phases, a software project is repeatedly passed through these phases in iterations called spirals. These phases are: Identification \u2013 This involves gathering the business requirements in the initial spiral. In subsequent spirals, the subsystem requirements are identified in each iteration. Design \u2013 This phase starts with conceptual design in the initial spiral. In subsequent spirals, it involves architecture design, logic design, product, and final design. Constructor builds \u2013 This is where the actual software development happens. In the initial spiral, the software is simply a proof of concept or POC. In subsequent spirals, it is developed further. Evaluation and risk analysis \u2013 This phase includes identifying, estimating, and monitoring risks such as schedule slippage or cost overrun. The customer provides feedback on every iteration of this phase. Advantages: It\u2019s easy to change the requirements. This model allows for quick iterations of software early in the life cycle. Development can be divided into smaller parts and riskier parts can be developed in later spirals. Disadvantages: Management for such a model is more complex and the end of the project is not clearly defined. This is not suitable for small-scale projects. Since there are many intermediate stages, it requires extensive documentation. Agile Model: This model breaks the product into small incremental builds. These builds are provided in iteration where each iteration typically lasts from one to three weeks. Every iteration includes cross functioning teams working on areas like: Planning Requirement analysis Design Coding Unit testing Acceptance testing At the end of the iteration, a working product is displayed to the customer. In Agile, tasks are divided into time boxes to deliver specific features for a release. Each build is incremental in terms of features and the final build contains all features required by the customer. This is the most popular model used in SDLC today. Agile has 4 main principles: Individuals and interactions \u2013 Self-organization and motivation are important as well as co-location and pair programming. Working software \u2013 There must be a demo working software that best communicates to the customer to give updates and understand new requirements instead of simply documentation. Customer collaboration \u2013 Continuous customer interaction is very important since all the requirements are not established at the very start, these evolve with the input of the customer. Responding to change \u2013 Agile development is focused on quick responses to change and continuous development. Advantages: It is a very realistic approach and promotes teamwork and cross-training. Functionality can be developed rapidly and demonstrated. This works for both fixed and changing requirements. There is a working software early in the life cycle. This model is easy to manage and gives flexibility to developers. Disadvantages: This model requires a plan, an agile leader, and an agile project manager. Depends heavily on customer interaction, which means if the customer is not clear then the project can become slow or hard to materialize. Transfer of technology to new team members might be difficult due to a lack of extensive documentation. There are some famous agile frameworks like SCRUM where a product owner will dictate some requirements which are put in the product backlog which is managed by a scrum master. Some sprints can last from one or two weeks with daily scrum meetings to give small updates and talk about roadblocks. At the end of a sprint, a version is delivered which is showcased to the customer. We also have Kanban which is a scheduling system like Jira which includes 4 main columns which are to-do, in progress, testing, and done. Tickets are created and tasks are moved from left to right as they are done. This is completely transparent to the team and the customer. Part 2 Control structures are a way to specify the flow of control in programs. They analyze and choose the direction in which a program flows based on certain parameters or conditions. The three basic control structures are Sequence, which is the default control structure where instructions are executed one after the other. The next basic type is conditional control structures, which allow the program to follow many options of paths depending on a given condition. The last type of control structure is iterative where instructions are executed in the body of a loop. Assembly language low-level control structures make extensive use of labels. These control structures usually transfer control between two points in the program. We specify the destination of such a transfer using a statement label, a label consists of a valid identifier and a colon. The three basic types of control structures are present in the assembly and are: Sequential \u2013 In assembly, these are basic arithmetic, logical, and bit operations where data is moved and copied. An example would be \u2018MOV EAX, EBX\u2019 where we move the 4 bytes in memory at address ebx into eax. Conditional or branching \u2013 In assembly these structures consist of direct and indirect jumps. Here we choose between two or more alternative paths, in assembly, this is done by using two types of instructions: a compare instruction like CMP which compares the two values, this is simply a subtract instruction internally. This is followed by a jump statement which will go to the instruction label which satisfies the given condition. There are two types of jumps, conditional jump instruction like jump if equal JE will go to the label if the two values are equal, which means the output of the compare instruction must be zero. The other type of jump instruction is an unconditional jump. This is performed by the JMP instruction. This type of jump instruction is used to jump on a particular location unconditionally, that is there is no need to satisfy any condition for the jump to take place. Some examples of conditional jump instructions are JNE or jump if not equal, JG or jump if greater, JGE, jump if equal or greater, JL or jump if lower, JLE or jump if lower. An unconditional jump example could be JMP followed by the label which is a direct jump or It can be followed by a register or a memory address. An example of conditional jump instruction can be that we have two registers AH and CH and we have moved a value into them. Next, we can compare these two with the CMP instruction followed by AH and then CH. We can then define some conditional jump instructions like JE followed by a label like L1, this instruction will be executed if the value in AH and CH are equal, we can also have an instruction like JL followed by a label like L2, we will jump to L2 if AH is less than CH. Both L1 and L2 will point to code blocks where we can execute whatever logic we need to. Iterative or looping \u2013 In assembly, these are looping structures like WHILE, DO WHILE, and FOR. They can be implemented using the JMP instruction. These loops must have an exit or break statement to avoid infinite loops. Infinite loops are usually a programmer\u2019s error, but event loops and task schedulers are examples of an infinite loop. Processors also have a newer LOOP instruction to execute loops more conveniently. An example of a loop that will execute a block for a fixed number of times with JMP instruction could be if we move the value 5 to the register AL. Then we will define a label for example L1, inside this code block we will decrease the value of AL by one using the DEC instruction on each loop iteration. Next, we can use a conditional jump statement like JNZ followed by L1, this code will run 5 times, when AL reached 0, we will not go back into the loop but exit it. We can also use the LOOP instruction to create a loop. An example could be if we move the value 5 in the ECX register and define a loop L1. This loop can simply have the instruction LOOP followed by L1 or the label we want to iterate over. The LOOP instruction decrements the value of ECX and compares it with zero, if the value in ECX is equal to zero, the program jumps to the L1 label, otherwise, it exits the loop.","title":"Question 7"},{"location":"Question_7/#7-part-1-provide-the-necessary-steps-and-technologies-for-developing-a-sample-software-product-on-a-choosen-platform-describe-the-benefits-and-difficulties-of-the-platform-the-implementation-steps-and-the-most-widely-used-current-technologies-part-2-implementation-of-control-structures-in-assembly-control-program-flow-branching-looping","text":"","title":"7. PART 1 Provide the necessary steps and technologies for developing a sample software product on a choosen platform. Describe the benefits and difficulties of the platform, the implementation steps, and the most widely used current technologies. PART 2 Implementation of control structures in assembly (control program flow, branching, looping)"},{"location":"Question_7/#part-1","text":"Software is planned, created, tested, and deployed, this cycle is called a Software Development Life Cycle or SDLC. Various SDLC methodologies were popular in a certain time frame and were slowly replaced by the next one. Waterfall Model: The waterfall model is broken down into sequential phases. In this model, each phase must be completed before the next phase can begin and there is no overlapping in the phases. The outcome of one phase acts as the input for the next phase sequentially. This model has 6 phases: Requirement gathering and analysis System Design Implementation Integration and Testing Deployment of system Maintenance Advantages: This model is simple and easy to understand and easy to manage due to its rigidity. All phases are processed one at a time which means it has very well-defined stages with deliverables that can be easily assessed. Disadvantages: There is no working software until the end of the life cycle. This introduces lots of risk and uncertainty. This model can get overwhelming for complex projects or long-term projects. This type of model is also not suited for projects which have a risk of changing their requirements. It is difficult to measure progress within the stages. Iterative Model: In this model, we start with a simple implementation of a small set of software requirements and iteratively enhance versions until the complete system is implemented and ready to be deployed. We do not require a full specification of requirements to start. On each iteration, the project is reviewed, and modifications are made on the next iteration. Each iteration flows through the design and development, testing, and implementation phases. Advantages: A working software can be developed early in the life cycle. Results are obtained early, and progress is easy to measure. This type of model is cheaper to change the requirements for. The testing phase is simpler as it is only concerned with the changes in the current iteration. It is easier to analyze risk since risks can be identified on each iteration. Disadvantages: This type of model requires more resources, even though the cost to change the requirements is less, more resources are required to execute the many iterations. This is not suitable for smaller projects. The end of the project is not clearly defined which is a risk. Spiral Model: This model is a combination of iterative models and takes aspects of the sequential flow like in the waterfall model. This model has four phases, a software project is repeatedly passed through these phases in iterations called spirals. These phases are: Identification \u2013 This involves gathering the business requirements in the initial spiral. In subsequent spirals, the subsystem requirements are identified in each iteration. Design \u2013 This phase starts with conceptual design in the initial spiral. In subsequent spirals, it involves architecture design, logic design, product, and final design. Constructor builds \u2013 This is where the actual software development happens. In the initial spiral, the software is simply a proof of concept or POC. In subsequent spirals, it is developed further. Evaluation and risk analysis \u2013 This phase includes identifying, estimating, and monitoring risks such as schedule slippage or cost overrun. The customer provides feedback on every iteration of this phase. Advantages: It\u2019s easy to change the requirements. This model allows for quick iterations of software early in the life cycle. Development can be divided into smaller parts and riskier parts can be developed in later spirals. Disadvantages: Management for such a model is more complex and the end of the project is not clearly defined. This is not suitable for small-scale projects. Since there are many intermediate stages, it requires extensive documentation. Agile Model: This model breaks the product into small incremental builds. These builds are provided in iteration where each iteration typically lasts from one to three weeks. Every iteration includes cross functioning teams working on areas like: Planning Requirement analysis Design Coding Unit testing Acceptance testing At the end of the iteration, a working product is displayed to the customer. In Agile, tasks are divided into time boxes to deliver specific features for a release. Each build is incremental in terms of features and the final build contains all features required by the customer. This is the most popular model used in SDLC today. Agile has 4 main principles: Individuals and interactions \u2013 Self-organization and motivation are important as well as co-location and pair programming. Working software \u2013 There must be a demo working software that best communicates to the customer to give updates and understand new requirements instead of simply documentation. Customer collaboration \u2013 Continuous customer interaction is very important since all the requirements are not established at the very start, these evolve with the input of the customer. Responding to change \u2013 Agile development is focused on quick responses to change and continuous development. Advantages: It is a very realistic approach and promotes teamwork and cross-training. Functionality can be developed rapidly and demonstrated. This works for both fixed and changing requirements. There is a working software early in the life cycle. This model is easy to manage and gives flexibility to developers. Disadvantages: This model requires a plan, an agile leader, and an agile project manager. Depends heavily on customer interaction, which means if the customer is not clear then the project can become slow or hard to materialize. Transfer of technology to new team members might be difficult due to a lack of extensive documentation. There are some famous agile frameworks like SCRUM where a product owner will dictate some requirements which are put in the product backlog which is managed by a scrum master. Some sprints can last from one or two weeks with daily scrum meetings to give small updates and talk about roadblocks. At the end of a sprint, a version is delivered which is showcased to the customer. We also have Kanban which is a scheduling system like Jira which includes 4 main columns which are to-do, in progress, testing, and done. Tickets are created and tasks are moved from left to right as they are done. This is completely transparent to the team and the customer.","title":"Part 1"},{"location":"Question_7/#part-2","text":"Control structures are a way to specify the flow of control in programs. They analyze and choose the direction in which a program flows based on certain parameters or conditions. The three basic control structures are Sequence, which is the default control structure where instructions are executed one after the other. The next basic type is conditional control structures, which allow the program to follow many options of paths depending on a given condition. The last type of control structure is iterative where instructions are executed in the body of a loop. Assembly language low-level control structures make extensive use of labels. These control structures usually transfer control between two points in the program. We specify the destination of such a transfer using a statement label, a label consists of a valid identifier and a colon. The three basic types of control structures are present in the assembly and are: Sequential \u2013 In assembly, these are basic arithmetic, logical, and bit operations where data is moved and copied. An example would be \u2018MOV EAX, EBX\u2019 where we move the 4 bytes in memory at address ebx into eax. Conditional or branching \u2013 In assembly these structures consist of direct and indirect jumps. Here we choose between two or more alternative paths, in assembly, this is done by using two types of instructions: a compare instruction like CMP which compares the two values, this is simply a subtract instruction internally. This is followed by a jump statement which will go to the instruction label which satisfies the given condition. There are two types of jumps, conditional jump instruction like jump if equal JE will go to the label if the two values are equal, which means the output of the compare instruction must be zero. The other type of jump instruction is an unconditional jump. This is performed by the JMP instruction. This type of jump instruction is used to jump on a particular location unconditionally, that is there is no need to satisfy any condition for the jump to take place. Some examples of conditional jump instructions are JNE or jump if not equal, JG or jump if greater, JGE, jump if equal or greater, JL or jump if lower, JLE or jump if lower. An unconditional jump example could be JMP followed by the label which is a direct jump or It can be followed by a register or a memory address. An example of conditional jump instruction can be that we have two registers AH and CH and we have moved a value into them. Next, we can compare these two with the CMP instruction followed by AH and then CH. We can then define some conditional jump instructions like JE followed by a label like L1, this instruction will be executed if the value in AH and CH are equal, we can also have an instruction like JL followed by a label like L2, we will jump to L2 if AH is less than CH. Both L1 and L2 will point to code blocks where we can execute whatever logic we need to. Iterative or looping \u2013 In assembly, these are looping structures like WHILE, DO WHILE, and FOR. They can be implemented using the JMP instruction. These loops must have an exit or break statement to avoid infinite loops. Infinite loops are usually a programmer\u2019s error, but event loops and task schedulers are examples of an infinite loop. Processors also have a newer LOOP instruction to execute loops more conveniently. An example of a loop that will execute a block for a fixed number of times with JMP instruction could be if we move the value 5 to the register AL. Then we will define a label for example L1, inside this code block we will decrease the value of AL by one using the DEC instruction on each loop iteration. Next, we can use a conditional jump statement like JNZ followed by L1, this code will run 5 times, when AL reached 0, we will not go back into the loop but exit it. We can also use the LOOP instruction to create a loop. An example could be if we move the value 5 in the ECX register and define a loop L1. This loop can simply have the instruction LOOP followed by L1 or the label we want to iterate over. The LOOP instruction decrements the value of ECX and compares it with zero, if the value in ECX is equal to zero, the program jumps to the L1 label, otherwise, it exits the loop.","title":"Part 2"},{"location":"Question_8/","text":"8. PART 1 Concept, typical applications and requirements of embedded systems. Real-time and reactive systems. Embedded systems architecture. Hardware and software layers. Embedded software: system software layer and application software layer. PART 2 Functions and services of the MRTG and Nagios network management systems. Part 1 An embedded system is any computer system contained within a product or embedded into another product that is not described as a computer. These are different from general-purpose computers, instead, they are custom made for a given purpose, they run software on dedicated hardware which might be terrible as a general-purpose computer but will excel in the task it was made for. Embedded systems have three main components, hardware, software, and firmware. The requirements of embedded systems change as per the application, for example, an embedded system in a fire alarm might only have a very low power single microprocessor and some sensors and actuators, it must operate in a wide range of temperature and humidity values and must consume as little energy as possible since it is highly likely that it will be working off a battery. Some other applications of embedded systems are digital watches, washing machines, cameras, automobiles, etc. Real-time systems are computer systems that monitor, respond to, or control an external environment. The environment is connected to the computer system through sensors, actuators, and other I/O interfaces. This computer system must meet various timing and other constraints that are imposed on it by the real-time behavior of the external world to which it is interfaced. These systems are also called reactive systems because their primary purpose is to respond to or react to signals from their environment. These types of systems are usually a component of a larger system thus they are embedded systems. Embedded systems come in two broad architecture types, Von Neumann architecture and Harvard architecture. The von Neumann architecture was proposed by Hungarian computer scientists John Von Neumann. In this architecture, one data bus exists for both instruction and data. Because of this, the CPU does one operation at a time, it can either fetch an instruction from memory or perform a read/write operation on the data. A fetch and data operation cannot occur simultaneously. The processor takes two clock cycles to execute, it will fetch a code in a separate cycle and read/write in a separate cycle. This architecture is simple and less time-consuming. The Harvard architecture on the other hand has separate storage and signal buses for instruction and data. Using separate internal buses, we can access the program instructions and data. This allows for fetch and reads/write operations to occur at the same time using different buses. The CPU can use a single clock cycle, this makes the design more complex and is more time-consuming. The hardware in an embedded system is based around a microprocessor or microcontroller. The embedded system hardware also contains other elements like memory, I/O devices, expandable interfaces like a display or camera, chips for wireless communication protocols. The embedded system software is written to perform a particular function. It is typically written in a high-level format and then compiled down to provide code that can be lodged within a non-volatile memory within the hardware. For educational embedded system devices like raspberry-pi, the language to write software is typically python. For a wide range of embedded devices, we use embedded C++ or C. For mission-critical applications of embedded systems like the anti-lock brake system, the code could be written directly in assembly language to get the fastest response from the system. The system software layer is the software that is designed to provide a platform for other software. System software includes operating systems like Windows or macOS, game engines like Unity and Unreal Engine, and software as a service application like Amazon Web Services and Microsoft Azure. For embedded systems like raspberry pi, the system software of choice is Raspberry Pi OS which is a Debian-based operating system or Unix-like system. This operating system was specially written for the ARM-based CISC processors which run these single board computers, that is why they are extremely optimized. This system software is very elaborate compared to a lot of other embedded systems, it offers a fully functioning graphical user interface, very close to macOS or windows in appearance. Users can install packages or pieces of fully functioning software using the APT or advanced package tool. On the other hand, we have the very simple embedded system Arduino, which uses the system software called Xinu and includes very basic features. Another tool called Protothreads can be used to execute linear code in C, and this can be used without any underlying operating system. Application software on the other hand is software that performs very specific tasks for the end-user. The user will directly interact with a piece of this software, unlike system software that will run process abstract from the end-user, of course, the user can alter these system software processes but, in most cases, it will result in misbehavior of the system or even complete shutdown. Embedded application software is very specialized to the application the embedded system will be embedded in. The important thing about embedded application software compared to application software found on a general-purpose computer is that non or not all functions of embedded software are initiated or controlled via a human interface, but through machine interfaces instead. An example of embedded software can be for controlling lights in homes, this can run on a simple 8bit microcontroller with just a few kilobytes of memory on very little energy, compared to 64-bit modern processors using several gigabytes of memory to function in the case of general-purpose computers. Part 2 MRTG or multi-router traffic Grapher is a free software for monitoring and measuring the traffic load on network links. It allows the user to see traffic load on a network over time in a graphical form. It was originally built to monitor router traffic, but it has evolved as a tool to create graphs and statistics for almost anything. This software is written in Perl and runs on all major operating systems. MRTG uses the simple network management protocol or SNMP to send requests with two object identifiers OIDs to a device. The device, which must be SNMP-enabled, will have a management information base to look up the OID specified. After collecting the information, it will send back the saw data encapsulated in an SNMP protocol. MRTG records this data in a log on the client along with previously recorded data for the device. The software then creates an HTML document from the logs, containing a list of graphs detailing traffic for the selected devices in the server. MRTG can also be configured to run a script of command and parse its output for counter values. The MRTG website contains a large library of external scripts to enable monitoring of SQL database statistics, firewall rules, CPU fan RPMs, or other integer value data. MRTG measures two values, input, and output per target. It gets its data via an SNMP agent or through the output of a command line. The frequency of data collection is typically every five minutes, but it can be configured to different time intervals as well. It creates an HTML page per target that features four graphs as GIF or PNG images. Results are plotted vs time into the day, week, month, and year graphs, with the input as a full green area and the output as a blue line. MRTG will automatically scale the Y-axis of the graphs to show the most detail and it will calculate the max, average and current values for both input and output on the HTML page. We can also make it send warning emails if the values are above a certain threshold. The RRDtool or round-robin database tool is an implementation of MRTG which aims to handle time-series data like network bandwidth but also includes other data like temperature and CPU load. A lot of other widely used tools have been made which are based on RRDtool like Munin and Cacti. Nagios is a free and open-source computer software application that monitors systems, networks, and infrastructure. Nagios offers to monitor and alerting services for servers, switches, and applications. It alerts users when things go wrong and alert them a second time when the problem has been resolved. It is continuous monitoring software which is a process to detect, report, and respond to all attacks which occur in the infrastructure. Continuous monitoring starts when the deployment is done on the production servers. There are several benefits to continuous monitoring like detecting all server and network problems, finding the root cause of the failure, helping reduce the maintenance cost, helping in troubleshooting the performance issues, helping updating infrastructure before it gets outdated, and monitoring the complete infrastructure every second. Nagios is a great choice since it can monitor database servers like SQL Server, Oracle, MySQL, Postgres, and others, it gives application-level information and also allows for protocol monitoring like HTTP, FTP SNMP, and SSH protocol monitoring, provides active development, it has an excellent community of open-source developers working hard to make it as bug-free as possible, it runs on virtually all operating systems, and it can ping to see if the host is reachable. Nagios helps in getting rid of periodic testing, it detects split-second failures and reduces maintenance cost without sacrificing performance. It also provides timely notification to the management of control and breakdown. Nagios has a server-agent architecture. The Nagios server is installed on the host and plugins are installed on the remote servers which are to be monitored. Nagios sends a signal through a process scheduler to run the plugins on the local or remote servers. These plugins collect the data like CPU usage and memory usage and send it back to the scheduler, then the process schedules send the notification to the admins and update the Nagios graphical user interface.","title":"Question 8"},{"location":"Question_8/#8-part-1-concept-typical-applications-and-requirements-of-embedded-systems-real-time-and-reactive-systems-embedded-systems-architecture-hardware-and-software-layers-embedded-software-system-software-layer-and-application-software-layer-part-2-functions-and-services-of-the-mrtg-and-nagios-network-management-systems","text":"","title":"8. PART 1 Concept, typical applications and requirements of embedded systems. Real-time and reactive systems. Embedded systems architecture. Hardware and software layers. Embedded software: system software layer and application software layer. PART 2 Functions and services of the MRTG and Nagios network management systems."},{"location":"Question_8/#part-1","text":"An embedded system is any computer system contained within a product or embedded into another product that is not described as a computer. These are different from general-purpose computers, instead, they are custom made for a given purpose, they run software on dedicated hardware which might be terrible as a general-purpose computer but will excel in the task it was made for. Embedded systems have three main components, hardware, software, and firmware. The requirements of embedded systems change as per the application, for example, an embedded system in a fire alarm might only have a very low power single microprocessor and some sensors and actuators, it must operate in a wide range of temperature and humidity values and must consume as little energy as possible since it is highly likely that it will be working off a battery. Some other applications of embedded systems are digital watches, washing machines, cameras, automobiles, etc. Real-time systems are computer systems that monitor, respond to, or control an external environment. The environment is connected to the computer system through sensors, actuators, and other I/O interfaces. This computer system must meet various timing and other constraints that are imposed on it by the real-time behavior of the external world to which it is interfaced. These systems are also called reactive systems because their primary purpose is to respond to or react to signals from their environment. These types of systems are usually a component of a larger system thus they are embedded systems. Embedded systems come in two broad architecture types, Von Neumann architecture and Harvard architecture. The von Neumann architecture was proposed by Hungarian computer scientists John Von Neumann. In this architecture, one data bus exists for both instruction and data. Because of this, the CPU does one operation at a time, it can either fetch an instruction from memory or perform a read/write operation on the data. A fetch and data operation cannot occur simultaneously. The processor takes two clock cycles to execute, it will fetch a code in a separate cycle and read/write in a separate cycle. This architecture is simple and less time-consuming. The Harvard architecture on the other hand has separate storage and signal buses for instruction and data. Using separate internal buses, we can access the program instructions and data. This allows for fetch and reads/write operations to occur at the same time using different buses. The CPU can use a single clock cycle, this makes the design more complex and is more time-consuming. The hardware in an embedded system is based around a microprocessor or microcontroller. The embedded system hardware also contains other elements like memory, I/O devices, expandable interfaces like a display or camera, chips for wireless communication protocols. The embedded system software is written to perform a particular function. It is typically written in a high-level format and then compiled down to provide code that can be lodged within a non-volatile memory within the hardware. For educational embedded system devices like raspberry-pi, the language to write software is typically python. For a wide range of embedded devices, we use embedded C++ or C. For mission-critical applications of embedded systems like the anti-lock brake system, the code could be written directly in assembly language to get the fastest response from the system. The system software layer is the software that is designed to provide a platform for other software. System software includes operating systems like Windows or macOS, game engines like Unity and Unreal Engine, and software as a service application like Amazon Web Services and Microsoft Azure. For embedded systems like raspberry pi, the system software of choice is Raspberry Pi OS which is a Debian-based operating system or Unix-like system. This operating system was specially written for the ARM-based CISC processors which run these single board computers, that is why they are extremely optimized. This system software is very elaborate compared to a lot of other embedded systems, it offers a fully functioning graphical user interface, very close to macOS or windows in appearance. Users can install packages or pieces of fully functioning software using the APT or advanced package tool. On the other hand, we have the very simple embedded system Arduino, which uses the system software called Xinu and includes very basic features. Another tool called Protothreads can be used to execute linear code in C, and this can be used without any underlying operating system. Application software on the other hand is software that performs very specific tasks for the end-user. The user will directly interact with a piece of this software, unlike system software that will run process abstract from the end-user, of course, the user can alter these system software processes but, in most cases, it will result in misbehavior of the system or even complete shutdown. Embedded application software is very specialized to the application the embedded system will be embedded in. The important thing about embedded application software compared to application software found on a general-purpose computer is that non or not all functions of embedded software are initiated or controlled via a human interface, but through machine interfaces instead. An example of embedded software can be for controlling lights in homes, this can run on a simple 8bit microcontroller with just a few kilobytes of memory on very little energy, compared to 64-bit modern processors using several gigabytes of memory to function in the case of general-purpose computers.","title":"Part 1"},{"location":"Question_8/#part-2","text":"MRTG or multi-router traffic Grapher is a free software for monitoring and measuring the traffic load on network links. It allows the user to see traffic load on a network over time in a graphical form. It was originally built to monitor router traffic, but it has evolved as a tool to create graphs and statistics for almost anything. This software is written in Perl and runs on all major operating systems. MRTG uses the simple network management protocol or SNMP to send requests with two object identifiers OIDs to a device. The device, which must be SNMP-enabled, will have a management information base to look up the OID specified. After collecting the information, it will send back the saw data encapsulated in an SNMP protocol. MRTG records this data in a log on the client along with previously recorded data for the device. The software then creates an HTML document from the logs, containing a list of graphs detailing traffic for the selected devices in the server. MRTG can also be configured to run a script of command and parse its output for counter values. The MRTG website contains a large library of external scripts to enable monitoring of SQL database statistics, firewall rules, CPU fan RPMs, or other integer value data. MRTG measures two values, input, and output per target. It gets its data via an SNMP agent or through the output of a command line. The frequency of data collection is typically every five minutes, but it can be configured to different time intervals as well. It creates an HTML page per target that features four graphs as GIF or PNG images. Results are plotted vs time into the day, week, month, and year graphs, with the input as a full green area and the output as a blue line. MRTG will automatically scale the Y-axis of the graphs to show the most detail and it will calculate the max, average and current values for both input and output on the HTML page. We can also make it send warning emails if the values are above a certain threshold. The RRDtool or round-robin database tool is an implementation of MRTG which aims to handle time-series data like network bandwidth but also includes other data like temperature and CPU load. A lot of other widely used tools have been made which are based on RRDtool like Munin and Cacti. Nagios is a free and open-source computer software application that monitors systems, networks, and infrastructure. Nagios offers to monitor and alerting services for servers, switches, and applications. It alerts users when things go wrong and alert them a second time when the problem has been resolved. It is continuous monitoring software which is a process to detect, report, and respond to all attacks which occur in the infrastructure. Continuous monitoring starts when the deployment is done on the production servers. There are several benefits to continuous monitoring like detecting all server and network problems, finding the root cause of the failure, helping reduce the maintenance cost, helping in troubleshooting the performance issues, helping updating infrastructure before it gets outdated, and monitoring the complete infrastructure every second. Nagios is a great choice since it can monitor database servers like SQL Server, Oracle, MySQL, Postgres, and others, it gives application-level information and also allows for protocol monitoring like HTTP, FTP SNMP, and SSH protocol monitoring, provides active development, it has an excellent community of open-source developers working hard to make it as bug-free as possible, it runs on virtually all operating systems, and it can ping to see if the host is reachable. Nagios helps in getting rid of periodic testing, it detects split-second failures and reduces maintenance cost without sacrificing performance. It also provides timely notification to the management of control and breakdown. Nagios has a server-agent architecture. The Nagios server is installed on the host and plugins are installed on the remote servers which are to be monitored. Nagios sends a signal through a process scheduler to run the plugins on the local or remote servers. These plugins collect the data like CPU usage and memory usage and send it back to the scheduler, then the process schedules send the notification to the admins and update the Nagios graphical user interface.","title":"Part 2"},{"location":"Question_9/","text":"9. PART 1 Programmable logic devices. Designing a digital system in hardware description language, and implementing it in FPGA devices. PART 2 Basic concepts of system engineering, different paradigms. Characteristics of the classical methods: waterfall, evolution, incremental, agile methods. Fundamentals and patterns of OOdesign. MVC Part 1 Programmable logic devices or PLD is electronic component used to build reconfigurable digital circuits. Unlike integrated circuits which consist of logic gates and have a fixed function, a PLD has an undefined function at the time of manufacture. Before the PLD can be used in a circuit it must be programmed or reconfigured by using a specialized program. PLDs have programmable logic, compared to fixed logic which has permanent configurations. Fixed logic is great for a final production design, but it cannot be used to experiment or add new features, we could keep creating new fixed logic chips for every new feature to be tested but that is not cost-efficient. PLDs are easy to program, and affordable, new code can be developed on them and tested in live electronic circuits. Every Boolean logic can be decomposed into product-of-sum POS or sum-of-product SOP, PLDs are typically built with an array of AND gates and an array of OR gates to implement the SOP. A simple programming technology is to use fuses, in the original state all fuses are intact, once programmed the fuses are blown along the paths that must be removed to obtain the correct logic. PLDs are broadly classified into simple programmable logic devices and high-capacity programmable logic devices. Some examples of SPLDs are ROM or read-only memory, PLA or programmable logic array, and PAL or programmable array logic. A widely used example of HCPLD is the FPGA or field-programmable gate array. Hardware description language or HDL is a specialized computer language used to describe the structure and behavior of electronic circuits and digital logic circuits. A hardware description language enables a precise, formal description of an electronic circuit that allows for automated analysis and simulation of an electronic circuit. Most hardware description languages look like other software programming languages like C or ALGOL, a textual description consisting of expressions, statements, and control structures. One important difference between most programming languages and HDLs is that HDLs explicitly include the notion of time. HDLs provide flexible modeling capabilities and can express large complex designs with tens of millions of logic gates. Today there are two main HDLs used, VHDL and Verilog. VHDL is the more popular choice of the two, it stands for Very High-Speed Integration Circuit HDL and it is standardized by IEEE. VHDL allows the user to define data types, supports parallel or concurrent procedure calls, it contains a built-in mod operator, but it is more difficult to learn compared to Verilog. Verilog is easier to learn and is used for describing digital systems like a network switch or FPGAs. Verilog supports design at many layers of abstraction: behavioral level, register transfer level, and gate-level. Behavioral level describes a system concurrent algorithm. Every algorithm is sequential, functions and blocks are the main elements. Register-transfer level designs specify the characteristics of a circuit using operations and the transfer of data between the registers. In gate-level design, the characteristics of a system are described by logical links and their timing properties. All signals are discrete signals and can only have definite logical values, logical 1, logical 0, unknown logic value X, and high impedance state Z. The usable operations are predefined logic primitives or basic logic gates. Gate level design is not widely used since it is the lowest level and is inefficient, instead, this code is generated using synthesis tools. We also define the physical wire that represents the physical wire used for connections of gates or modules using the Wire keyword. FPGA stands for field-programmable gate array. FPGAs are essentially a huge array of gates that can be programmed and reconfigured with code. Some modern FPGAs also have more complex blocks like memory controllers, high-speed communications interfaces, PCIe endpoints, etc. but at the core, we have grids of gates that can be programmed. The most widely used FPGAs are manufactured by Xilinx, which is a subsidiary of fixed logic general-purpose processor manufacturer AMD. A basic Xilinx FPGA board can be programmed in the Vivado integrated development environment using the middle layer of abstraction called the register transfer level or RTL which lies between the strictly behavioral and the pure gate-level model. In RTL we can describe a sequence of data flow from a set of registers to the next at each clock cycle. RTL code can be converted to Verilog. Using Verilog, we can define modules where we state the wire and the clock. We begin initializing the clock to 0 and begin defining an endpoint for the clock. We begin the clock and then call whatever code block we want to, for example, we can define another module that will assign a variable to a wire and its complement to another wire. Using Vivido IDE, we can use the simulation model and run to see the output in the simulation in a waveform format. We can also inspect the waveform to make sure our Verilog module is working as expected. Connecting our physical FPGA board to the development computer, we can then run this script on the board and see the output by assigning the values to some LEDs for visual representation, in our case we can program the original value and its complement can be two different LEDs, for our defined clock period, these two LEDs can blink alternatively since they have a NOT gate between them. Part 2 A system is a collection of different elements that interact to produce results that are not obtainable by the elements alone. An automobile is made up of thousands of parts and each part must work with the others if the vehicle is to function as desired. From a functional viewpoint, systems have inputs, processes, and outputs. Inputs are the resources put into a system, processes combine the resources to produce the output which can be a product, service, or enterprise. From a physical viewpoint, the system consists of mechanical, electrical, and software components. These systems are built in a way to provide feedback on if the system is working correctly or not. Systems engineering is when different engineers in different fields come together to work on a larger system, it is an interdisciplinary field. Systems engineering have five core concepts: Value \u2013 Systems provide value when they meet the needs of stakeholders. Context \u2013 The context of the system is important; the engineers need to consider where the products will be used. Trade-offs \u2013 Trade-offs can cost, time and performance and must be evaluated during designing. Abstraction \u2013 Engineers need the ability to abstract a design conception independent of the solution. Interdisciplinarity \u2013 Interdisciplinarity supports the systems approach which means the teams must comprise members for various disciplines to meet every requirement of the stakeholders. In software engineering which is a part of systems engineering where the system is the software product, we have different paradigms: Waterfall \u2013 This is a linear sequential model. In this model, each phase must be completed before the next phase can begin and there is no overlapping in the phases. These phases are requirement analysis, system design, implementation, testing, deployment, and maintenance. This method is not used anymore due to better methods like Agile. Evolution \u2013 This is a software engineering model where the software is developed initially, then it is updated timely for various reasons like to add new features or to remove obsolete functionality. The evolution process includes fundamental activities of change analysis, release planning, system implementation, and releasing a system to customers. Incremental \u2013 This model of software engineering is when the requirements are divided into multiple standalone modules of the software development cycle. Each module goes through the requirements, design, implementation, and testing phases. Every subsequent release of the module adds function to the previous release, this process continues until the complete system is achieved. Agile \u2013 This model focuses on adaptability and customer satisfaction by rapid delivery of working software products. The agile method breaks the product into small incremental builds. These builds are provided in iteration, where each iteration typically lasts from one to three weeks. Every iteration involves cross functioning teams working on areas like planning, requirement analysis, design, coding, unit testing, etc. Customer interaction is the backbone of agile, the product development has complete transparency to the customer with timely working demos instead of lots of documentation. The customer then provides feedback and can change requirements or add to them. This is the most widely used model today and has many useful frameworks like SCRUM and Kanban. Object-oriented programs are made up of objects. An object packages both data and procedures. Procedures are typically called methods. Objects have procedures that can access and modify the data fields themselves. Most popular object-oriented programming languages are class-based like Java and C sharp. Object-oriented programming is based on 4 pillars, which are Abstraction, Polymorphism, Inheritance, and Encapsulation. Abstraction is when we separate the interface of a class from its implementation and focus on the interface, this is like treating the system as a black box and not worrying about how it is working. We only present a clean and easy-to-use interface via the class\u2019s member functions. Polymorphism is when objects of different types can be accessed through the same interface, each type can provide its independent implementation of this interface. Inheritance allows us to create subclasses from parent classes, the subclass will inherit all objects and methods from its parent class and can be overridden to change the functionality as well. Encapsulation in OOP is when we bind the data and functions into a single class, by doing so we hide private details of a class from the outside world and only expose functionality that is important for interfacing with it. Design patterns in OO refer to using patterns that can be followed to easily solve problems in object-oriented programming: Creational \u2013 Creational patterns have to do with the creation of objects. It emphasizes the automatic creation of objects; a function or method can call the code necessary to instantiate a new object on the programmer\u2019s behalf. This allows us to create new objects faster where there might be many defaults, in the case where we might need to change a default, we can do so explicitly for that object. Structural \u2013 Structural design patterns have to do with making larger structures from smaller objects and classes. This allows us to leverage relationships between classes and objects to form larger structures, thus we use inheritance here. Behavioral \u2013 Behavioral patterns are concerned with the communication of different objects with each other, or the interaction between objects and how they affect each other. An example could be a bottle of water object and a human object, the human drinks the bottle of water using the bottle of water\u2019s drink method. This causes the thirst attribute of the human object to become false and also causes the water level attribute of the bottle of water to be lower in value. MVC architecture or Model View Controller architecture pattern turns complex applications development into a much more manageable process. It allows several developers to simultaneously work on the application. MVC has three components, Model is the backend that contains all the data logic, View is the frontend or graphical user interface, Controller is the brains of the applications that control how data is displayed. MVC pattern is widely used for modern web applications since it allows the application to be scalable, maintainable, and easy to expand. MVC pattern helps us break up the frontend and backend code into separate components. This way it is much easier to manage and make changes to either side without them interfering with each other. The model component is where the data is managed, data can come from a database, API, or a JSON object. The view is the user interface, view\u2019s job is to decide what the user will see on the screen and how. The Controller is the brain component, it is responsible to pull, modify and provide data to the user. Essentially the controller is the link between the view and model. Some frameworks using MVC concepts are Django and Ruby on Rails.","title":"Question 9"},{"location":"Question_9/#9-part-1-programmable-logic-devices-designing-a-digital-system-in-hardware-description-language-and-implementing-it-in-fpga-devices-part-2-basic-concepts-of-system-engineering-different-paradigms-characteristics-of-the-classical-methods-waterfall-evolution-incremental-agile-methods-fundamentals-and-patterns-of-oodesign-mvc","text":"","title":"9. PART 1 Programmable logic devices. Designing a digital system in hardware description language, and implementing it in FPGA devices. PART 2 Basic concepts of system engineering, different paradigms. Characteristics of the classical methods: waterfall, evolution, incremental, agile methods. Fundamentals and patterns of OOdesign. MVC"},{"location":"Question_9/#part-1","text":"Programmable logic devices or PLD is electronic component used to build reconfigurable digital circuits. Unlike integrated circuits which consist of logic gates and have a fixed function, a PLD has an undefined function at the time of manufacture. Before the PLD can be used in a circuit it must be programmed or reconfigured by using a specialized program. PLDs have programmable logic, compared to fixed logic which has permanent configurations. Fixed logic is great for a final production design, but it cannot be used to experiment or add new features, we could keep creating new fixed logic chips for every new feature to be tested but that is not cost-efficient. PLDs are easy to program, and affordable, new code can be developed on them and tested in live electronic circuits. Every Boolean logic can be decomposed into product-of-sum POS or sum-of-product SOP, PLDs are typically built with an array of AND gates and an array of OR gates to implement the SOP. A simple programming technology is to use fuses, in the original state all fuses are intact, once programmed the fuses are blown along the paths that must be removed to obtain the correct logic. PLDs are broadly classified into simple programmable logic devices and high-capacity programmable logic devices. Some examples of SPLDs are ROM or read-only memory, PLA or programmable logic array, and PAL or programmable array logic. A widely used example of HCPLD is the FPGA or field-programmable gate array. Hardware description language or HDL is a specialized computer language used to describe the structure and behavior of electronic circuits and digital logic circuits. A hardware description language enables a precise, formal description of an electronic circuit that allows for automated analysis and simulation of an electronic circuit. Most hardware description languages look like other software programming languages like C or ALGOL, a textual description consisting of expressions, statements, and control structures. One important difference between most programming languages and HDLs is that HDLs explicitly include the notion of time. HDLs provide flexible modeling capabilities and can express large complex designs with tens of millions of logic gates. Today there are two main HDLs used, VHDL and Verilog. VHDL is the more popular choice of the two, it stands for Very High-Speed Integration Circuit HDL and it is standardized by IEEE. VHDL allows the user to define data types, supports parallel or concurrent procedure calls, it contains a built-in mod operator, but it is more difficult to learn compared to Verilog. Verilog is easier to learn and is used for describing digital systems like a network switch or FPGAs. Verilog supports design at many layers of abstraction: behavioral level, register transfer level, and gate-level. Behavioral level describes a system concurrent algorithm. Every algorithm is sequential, functions and blocks are the main elements. Register-transfer level designs specify the characteristics of a circuit using operations and the transfer of data between the registers. In gate-level design, the characteristics of a system are described by logical links and their timing properties. All signals are discrete signals and can only have definite logical values, logical 1, logical 0, unknown logic value X, and high impedance state Z. The usable operations are predefined logic primitives or basic logic gates. Gate level design is not widely used since it is the lowest level and is inefficient, instead, this code is generated using synthesis tools. We also define the physical wire that represents the physical wire used for connections of gates or modules using the Wire keyword. FPGA stands for field-programmable gate array. FPGAs are essentially a huge array of gates that can be programmed and reconfigured with code. Some modern FPGAs also have more complex blocks like memory controllers, high-speed communications interfaces, PCIe endpoints, etc. but at the core, we have grids of gates that can be programmed. The most widely used FPGAs are manufactured by Xilinx, which is a subsidiary of fixed logic general-purpose processor manufacturer AMD. A basic Xilinx FPGA board can be programmed in the Vivado integrated development environment using the middle layer of abstraction called the register transfer level or RTL which lies between the strictly behavioral and the pure gate-level model. In RTL we can describe a sequence of data flow from a set of registers to the next at each clock cycle. RTL code can be converted to Verilog. Using Verilog, we can define modules where we state the wire and the clock. We begin initializing the clock to 0 and begin defining an endpoint for the clock. We begin the clock and then call whatever code block we want to, for example, we can define another module that will assign a variable to a wire and its complement to another wire. Using Vivido IDE, we can use the simulation model and run to see the output in the simulation in a waveform format. We can also inspect the waveform to make sure our Verilog module is working as expected. Connecting our physical FPGA board to the development computer, we can then run this script on the board and see the output by assigning the values to some LEDs for visual representation, in our case we can program the original value and its complement can be two different LEDs, for our defined clock period, these two LEDs can blink alternatively since they have a NOT gate between them.","title":"Part 1"},{"location":"Question_9/#part-2","text":"A system is a collection of different elements that interact to produce results that are not obtainable by the elements alone. An automobile is made up of thousands of parts and each part must work with the others if the vehicle is to function as desired. From a functional viewpoint, systems have inputs, processes, and outputs. Inputs are the resources put into a system, processes combine the resources to produce the output which can be a product, service, or enterprise. From a physical viewpoint, the system consists of mechanical, electrical, and software components. These systems are built in a way to provide feedback on if the system is working correctly or not. Systems engineering is when different engineers in different fields come together to work on a larger system, it is an interdisciplinary field. Systems engineering have five core concepts: Value \u2013 Systems provide value when they meet the needs of stakeholders. Context \u2013 The context of the system is important; the engineers need to consider where the products will be used. Trade-offs \u2013 Trade-offs can cost, time and performance and must be evaluated during designing. Abstraction \u2013 Engineers need the ability to abstract a design conception independent of the solution. Interdisciplinarity \u2013 Interdisciplinarity supports the systems approach which means the teams must comprise members for various disciplines to meet every requirement of the stakeholders. In software engineering which is a part of systems engineering where the system is the software product, we have different paradigms: Waterfall \u2013 This is a linear sequential model. In this model, each phase must be completed before the next phase can begin and there is no overlapping in the phases. These phases are requirement analysis, system design, implementation, testing, deployment, and maintenance. This method is not used anymore due to better methods like Agile. Evolution \u2013 This is a software engineering model where the software is developed initially, then it is updated timely for various reasons like to add new features or to remove obsolete functionality. The evolution process includes fundamental activities of change analysis, release planning, system implementation, and releasing a system to customers. Incremental \u2013 This model of software engineering is when the requirements are divided into multiple standalone modules of the software development cycle. Each module goes through the requirements, design, implementation, and testing phases. Every subsequent release of the module adds function to the previous release, this process continues until the complete system is achieved. Agile \u2013 This model focuses on adaptability and customer satisfaction by rapid delivery of working software products. The agile method breaks the product into small incremental builds. These builds are provided in iteration, where each iteration typically lasts from one to three weeks. Every iteration involves cross functioning teams working on areas like planning, requirement analysis, design, coding, unit testing, etc. Customer interaction is the backbone of agile, the product development has complete transparency to the customer with timely working demos instead of lots of documentation. The customer then provides feedback and can change requirements or add to them. This is the most widely used model today and has many useful frameworks like SCRUM and Kanban. Object-oriented programs are made up of objects. An object packages both data and procedures. Procedures are typically called methods. Objects have procedures that can access and modify the data fields themselves. Most popular object-oriented programming languages are class-based like Java and C sharp. Object-oriented programming is based on 4 pillars, which are Abstraction, Polymorphism, Inheritance, and Encapsulation. Abstraction is when we separate the interface of a class from its implementation and focus on the interface, this is like treating the system as a black box and not worrying about how it is working. We only present a clean and easy-to-use interface via the class\u2019s member functions. Polymorphism is when objects of different types can be accessed through the same interface, each type can provide its independent implementation of this interface. Inheritance allows us to create subclasses from parent classes, the subclass will inherit all objects and methods from its parent class and can be overridden to change the functionality as well. Encapsulation in OOP is when we bind the data and functions into a single class, by doing so we hide private details of a class from the outside world and only expose functionality that is important for interfacing with it. Design patterns in OO refer to using patterns that can be followed to easily solve problems in object-oriented programming: Creational \u2013 Creational patterns have to do with the creation of objects. It emphasizes the automatic creation of objects; a function or method can call the code necessary to instantiate a new object on the programmer\u2019s behalf. This allows us to create new objects faster where there might be many defaults, in the case where we might need to change a default, we can do so explicitly for that object. Structural \u2013 Structural design patterns have to do with making larger structures from smaller objects and classes. This allows us to leverage relationships between classes and objects to form larger structures, thus we use inheritance here. Behavioral \u2013 Behavioral patterns are concerned with the communication of different objects with each other, or the interaction between objects and how they affect each other. An example could be a bottle of water object and a human object, the human drinks the bottle of water using the bottle of water\u2019s drink method. This causes the thirst attribute of the human object to become false and also causes the water level attribute of the bottle of water to be lower in value. MVC architecture or Model View Controller architecture pattern turns complex applications development into a much more manageable process. It allows several developers to simultaneously work on the application. MVC has three components, Model is the backend that contains all the data logic, View is the frontend or graphical user interface, Controller is the brains of the applications that control how data is displayed. MVC pattern is widely used for modern web applications since it allows the application to be scalable, maintainable, and easy to expand. MVC pattern helps us break up the frontend and backend code into separate components. This way it is much easier to manage and make changes to either side without them interfering with each other. The model component is where the data is managed, data can come from a database, API, or a JSON object. The view is the user interface, view\u2019s job is to decide what the user will see on the screen and how. The Controller is the brain component, it is responsible to pull, modify and provide data to the user. Essentially the controller is the link between the view and model. Some frameworks using MVC concepts are Django and Ruby on Rails.","title":"Part 2"}]}